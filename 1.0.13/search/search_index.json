{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Beacon API Python Documentation","text":"<p>Beacon API is a Python package that provides an interface to interact with Beacon Data Lakes. It allows users to query and retrieve data from these lakes in a structured manner.</p> <p>Beacon Data Lake</p> <p>To visit the Documentation about the Beacon Data Lake software itself, click here.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>To get started with the <code>beacon_api</code> package, follow these steps:</p>"},{"location":"getting_started/#install-the-package-using-pip","title":"Install the package using pip","text":"<pre><code>pip install beacon_api\n</code></pre>"},{"location":"getting_started/#import-the-package-in-your-python-code","title":"Import the package in your Python code","text":"<pre><code>import beacon_api\n</code></pre> <p>Code Completion</p> <p>The package integrates well with VS Code and PyCharm, providing features like code completion and inline documentation. It will also work in Jupyter Notebooks but may not provide the same level of code completion.</p>"},{"location":"getting_started/#connect-to-a-beacon-data-lake","title":"Connect to a Beacon Data Lake","text":"<pre><code>client = beacon_api.Client(\"https://some-beacon-datalake.com\")\n</code></pre> <p>Passing API Token</p> <p>If your Beacon Data Lake requires authentication, you can pass an API token when creating the client: <pre><code>client = beacon_api.Client(\"https://some-beacon-datalake.com\", jwt_token=\"your_api_token\")\n</code></pre></p>"},{"location":"getting_started/#list-available-tables","title":"List available tables","text":"<p>To view all the available tables/data collections in the connected Beacon Data Lake, you can use the <code>list_tables</code> method:</p> <pre><code>tables = client.list_tables()\nprint(tables)\n</code></pre> <p>Note</p> <p>The <code>list_tables</code> method returns a dictionary where the keys are the table names and the values are <code>Table</code> objects that you can use to interact with each table.</p>"},{"location":"getting_started/#viewing-the-schema-available-columns-of-a-specific-table","title":"Viewing the schema (available columns) of a specific table","text":"<p>To view the schema (available columns) of a specific table, you can use the <code>get_table_schema</code> method:</p> <pre><code>tables = client.list_tables()\nschema = tables['default'].get_table_schema()\nprint(schema)   \n</code></pre> <p>Note</p> <p>Every beacon will have a table named <code>default</code>. This can be used to query the main data collection of the beacon.</p>"},{"location":"getting_started/#create-and-execute-a-query","title":"Create and execute a query","text":"<p>To create and execute a query on a specific table, you can use the <code>query</code> method of the <code>Table</code> object. Here's an example of how to create a simple query that selects specific columns and applies filters:</p> <pre><code>df = (\n    tables['default'] # Select the 'default' table as our data source\n    .query() # Create a new query on the selected table\n    .add_select_column(\"LONGITUDE\") # Select the LONGITUDE column\n    .add_select_column(\"LATITUDE\") # Select the LATITUDE column\n    .add_select_column(\"JULD\")\n    .add_select_column(\"PRES\")\n    .add_select_column(\"TEMP\")\n    .add_select_column(\"PSAL\")\n    .add_select_column(\".featureType\") # Select the .featureType column\n    .add_select_column(\"DATA_TYPE\")\n    .add_range_filter(\"JULD\", \"2020-01-01T00:00:00\", \"2021-01-01T00:00:00\") # Filter for JULD between 2020 and 2021 for the column JULD\n    .add_range_filter(\"PRES\", 0, 10) # Filter for pressure between 0 and 10 dbar for the column PRES\n    .to_pandas_dataframe() # Execute the query and return the results as a Pandas DataFrame\n)\ndf\n</code></pre> <p>Note</p> <p>The <code>to_pandas_dataframe</code> method executes the query and returns the results as a Pandas DataFrame.</p> <p>Chained Methods</p> <p>The query methods can be chained together to build complex queries in a readable manner.</p> <p>For more detailed information on the available methods and options for querying, refer to the Client Reference, Table Reference, and Query Reference sections of the documentation.</p> <p>Alternative Query Creation</p> <p>The query can also be created using the client directly, by default it will use the <code>default</code> table: <pre><code>df = (\n    client.query()\n    .add_select_column(\"LONGITUDE\")\n    .add_select_column(\"LATITUDE\")\n    .add_select_column(\"JULD\")\n    .add_select_column(\"PRES\")\n    .add_select_column(\"TEMP\")\n    .add_select_column(\"PSAL\")\n    .add_select_column(\".featureType\")\n    .add_select_column(\"DATA_TYPE\")\n    .add_range_filter(\"JULD\", \"2020-01-01T00:00:00\", \"2021-01-01T00:00:00\")\n    .add_range_filter(\"PRES\", 0, 10)\n    .to_pandas_dataframe()\n)\ndf\n</code></pre></p>"},{"location":"installation/","title":"Installation","text":"<p>To install the <code>beacon_api</code> package, you can use pip. Run the following command in your terminal:</p> <pre><code>pip install beacon_api\n</code></pre> <p>Upgrade pyarrow if using 19.0.0</p> <p>If you are using pyarrow 19.0.0, please upgrade to at least 19.0.1 due to a known issue. <pre><code>pip install --upgrade pyarrow\n</code></pre></p> <p>Make sure you have Python 3.7 or higher installed on your system. You can check your Python version by running:</p> <pre><code>python --version\n</code></pre> <p>Some features of the <code>beacon_api</code> package may require additional dependencies. You can install these optional dependencies using:</p> <p>GeoPandas support:</p> <pre><code>pip install beacon_api[geopandas]\n</code></pre> <p>Zarr support:</p> <pre><code>pip install beacon_api[zarr]\n</code></pre> <p>Query Profiling support:</p> <pre><code>pip install beacon_api[profiling]\n</code></pre>"},{"location":"examples/wod/","title":"World Ocean Database Example","text":"<p>The World Ocean Database (WOD) is a comprehensive collection of oceanographic data, including temperature, salinity, oxygen, and other parameters. This example demonstrates how to use the <code>beacon_api</code> package to query and retrieve data from a Beacon Data Lake that hosts the WOD dataset. The Beacon Data Lake contains around 20 million netCDF files stored into various Beacon Binary Format files (think of a zip containing multiple netcdf files), which are organized into tables for efficient querying.</p>"},{"location":"examples/wod/#connecting-to-the-beacon-wod-data-lake","title":"Connecting to the Beacon WOD Data Lake","text":"<pre><code>from beacon_api import Client\n\nclient = Client(\"https://beacon-wod.maris.nl\")\ntables = client.list_tables()\nwod_table = tables['default']\n</code></pre>"},{"location":"examples/wod/#viewing-table-schema","title":"Viewing Table Schema","text":"<pre><code>schema = wod_table.get_table_schema()\nprint(schema)\n</code></pre>"},{"location":"examples/wod/#querying-data","title":"Querying Data","text":"<pre><code>df = (\n    wod_table\n    .query()\n    .add_select_column(\"lon\", alias=\"longitude\")\n    .add_select_column(\"lat\", alias=\"latitude\")\n    .add_select_column(\"z\", alias=\"depth\")\n    .add_select_column(\"time\")\n    .add_select_column(\"Temperature\")\n    .add_select_column(\"Salinity\")\n    .add_range_filter(\"time\", \"2020-01-01T00:00:00\", \"2021-01-01T00:00:00\")\n    .to_pandas_dataframe()\n)\nprint(df)\n</code></pre>"},{"location":"reference/client/","title":"Client Reference","text":""},{"location":"reference/client/#beacon_api.client.Client","title":"<code>Client</code>","text":"Source code in <code>beacon_api/client.py</code> <pre><code>class Client:\n    def __init__(self, url: str, proxy_headers: dict[str,str] | None = None, jwt_token: str | None = None, basic_auth: tuple[str, str] | None = None):\n        if proxy_headers is None:\n            proxy_headers = {}\n        # Set JSON headers\n        proxy_headers['Content-Type'] = 'application/json'\n        proxy_headers['Accept'] = 'application/json'\n        if jwt_token:\n            proxy_headers['Authorization'] = f'Bearer {jwt_token}'\n\n        if basic_auth:\n            if not isinstance(basic_auth, tuple) or len(basic_auth) != 2:\n                raise ValueError(\"Basic auth must be a tuple of (username, password)\")\n            proxy_headers['Authorization'] = f'Basic {requests.auth._basic_auth_str(*basic_auth)}' # type: ignore\n\n        self.session = BaseBeaconSession(url)\n        self.session.headers.update(proxy_headers)\n\n        if self.check_status():\n            raise Exception(\"Failed to connect to server\")\n\n    def check_status(self):\n        \"\"\"Check the status of the server\"\"\"\n        response = self.session.get(\"api/health\")\n        if response.status_code != 200:\n            raise Exception(f\"Failed to connect to server: {response.text}\")\n        else:\n            print(\"Connected to: {} server successfully\".format(self.session.base_url))\n\n    def available_columns(self) -&gt; list[str]:\n        \"\"\"Get all the available columns for the default data table\"\"\"\n        response = self.session.get(\"/api/query/available-columns\")\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get columns: {response.text}\")\n        columns = response.json()\n        return columns\n\n    def available_columns_with_data_type(self) -&gt; dict[str, type]:\n        tables = self.list_tables()\n        if 'default' not in tables:\n            raise Exception(\"No default table found\")\n        table = tables['default']\n        return table.get_table_schema()\n\n    def list_tables(self) -&gt; dict[str,DataTable]:\n        \"\"\"Get all the tables\"\"\"\n        response = self.session.get(\"/api/tables\")\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get tables: {response.text}\")\n        tables = response.json()\n\n        data_tables = {}\n        for table in tables:\n            data_tables[table] = DataTable(\n                http_session=self.session,\n                table_name=table,\n            )\n\n        return data_tables\n\n    def list_datasets(self, pattern: str | None = None, limit : int | None = None, offset: int | None = None) -&gt; dict[str, Dataset]:\n        \"\"\"Get all the datasets\"\"\"\n        response = self.session.get(\"/api/datasets\", params={\n            \"pattern\": pattern,\n            \"limit\": limit,\n            \"offset\": offset\n        })\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get datasets: {response.text}\")\n        datasets = response.json()\n        dataset_objects = {}\n        for dataset in datasets:\n            dataset_objects[dataset] = Dataset(\n                http_session=self.session,\n                file_path=dataset\n            )\n        return dataset_objects\n\n    def query(self) -&gt; Query:\n        \"\"\"Create a new query object. \n        This is the starting point for building a query.\n        The query can then be built using the methods on the Query object.\n        You can also create a query from a specific table from the list_tables() method.\n\n        To materialize and run the query, use the .to_dataframe() or .to_csv() methods on the Query object.\n        Returns:\n            Query: A new query object.\n        \"\"\"\n        return Query(http_session=self.session, from_table=\"default\")\n\n    def subset(self, longitude_column: str, latitude_column: str, time_column: str, depth_column: str, columns: list[str],\n                         bbox: Optional[tuple[float, float, float, float]] = None,\n                         depth_range: Optional[tuple[float, float]] = None,\n                         time_range: Optional[tuple[datetime.datetime, datetime.datetime]] = None) -&gt; Query:\n        \"\"\"\n        Create a query to subset the default collection based on the provided parameters.\n\n        Args:\n            longitude_column: Name of the column containing longitude values.\n            latitude_column: Name of the column containing latitude values.\n            time_column: Name of the column containing time values.\n            depth_column: Name of the column containing depth values.\n            columns: List of additional columns to include in the query.\n            bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).\n            depth_range: Optional range for depth defined as (min_depth, max_depth).\n            time_range: Optional range for time defined as (start_time, end_time).\n        Returns\n            A Query object that can be executed to retrieve the subset of data.\n        \"\"\"\n        table = self.list_tables()['default']\n        return table.subset(\n            longitude_column=longitude_column,\n            latitude_column=latitude_column,\n            time_column=time_column,\n            depth_column=depth_column,\n            columns=columns,\n            bbox=bbox,\n            depth_range=depth_range,\n            time_range=time_range\n        )\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.available_columns","title":"<code>available_columns()</code>","text":"<p>Get all the available columns for the default data table</p> Source code in <code>beacon_api/client.py</code> <pre><code>def available_columns(self) -&gt; list[str]:\n    \"\"\"Get all the available columns for the default data table\"\"\"\n    response = self.session.get(\"/api/query/available-columns\")\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get columns: {response.text}\")\n    columns = response.json()\n    return columns\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.check_status","title":"<code>check_status()</code>","text":"<p>Check the status of the server</p> Source code in <code>beacon_api/client.py</code> <pre><code>def check_status(self):\n    \"\"\"Check the status of the server\"\"\"\n    response = self.session.get(\"api/health\")\n    if response.status_code != 200:\n        raise Exception(f\"Failed to connect to server: {response.text}\")\n    else:\n        print(\"Connected to: {} server successfully\".format(self.session.base_url))\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.list_datasets","title":"<code>list_datasets(pattern=None, limit=None, offset=None)</code>","text":"<p>Get all the datasets</p> Source code in <code>beacon_api/client.py</code> <pre><code>def list_datasets(self, pattern: str | None = None, limit : int | None = None, offset: int | None = None) -&gt; dict[str, Dataset]:\n    \"\"\"Get all the datasets\"\"\"\n    response = self.session.get(\"/api/datasets\", params={\n        \"pattern\": pattern,\n        \"limit\": limit,\n        \"offset\": offset\n    })\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get datasets: {response.text}\")\n    datasets = response.json()\n    dataset_objects = {}\n    for dataset in datasets:\n        dataset_objects[dataset] = Dataset(\n            http_session=self.session,\n            file_path=dataset\n        )\n    return dataset_objects\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.list_tables","title":"<code>list_tables()</code>","text":"<p>Get all the tables</p> Source code in <code>beacon_api/client.py</code> <pre><code>def list_tables(self) -&gt; dict[str,DataTable]:\n    \"\"\"Get all the tables\"\"\"\n    response = self.session.get(\"/api/tables\")\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get tables: {response.text}\")\n    tables = response.json()\n\n    data_tables = {}\n    for table in tables:\n        data_tables[table] = DataTable(\n            http_session=self.session,\n            table_name=table,\n        )\n\n    return data_tables\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.query","title":"<code>query()</code>","text":"<p>Create a new query object.  This is the starting point for building a query. The query can then be built using the methods on the Query object. You can also create a query from a specific table from the list_tables() method.</p> <p>To materialize and run the query, use the .to_dataframe() or .to_csv() methods on the Query object. Returns:     Query: A new query object.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def query(self) -&gt; Query:\n    \"\"\"Create a new query object. \n    This is the starting point for building a query.\n    The query can then be built using the methods on the Query object.\n    You can also create a query from a specific table from the list_tables() method.\n\n    To materialize and run the query, use the .to_dataframe() or .to_csv() methods on the Query object.\n    Returns:\n        Query: A new query object.\n    \"\"\"\n    return Query(http_session=self.session, from_table=\"default\")\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.subset","title":"<code>subset(longitude_column, latitude_column, time_column, depth_column, columns, bbox=None, depth_range=None, time_range=None)</code>","text":"<p>Create a query to subset the default collection based on the provided parameters.</p> <p>Args:     longitude_column: Name of the column containing longitude values.     latitude_column: Name of the column containing latitude values.     time_column: Name of the column containing time values.     depth_column: Name of the column containing depth values.     columns: List of additional columns to include in the query.     bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).     depth_range: Optional range for depth defined as (min_depth, max_depth).     time_range: Optional range for time defined as (start_time, end_time). Returns     A Query object that can be executed to retrieve the subset of data.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def subset(self, longitude_column: str, latitude_column: str, time_column: str, depth_column: str, columns: list[str],\n                     bbox: Optional[tuple[float, float, float, float]] = None,\n                     depth_range: Optional[tuple[float, float]] = None,\n                     time_range: Optional[tuple[datetime.datetime, datetime.datetime]] = None) -&gt; Query:\n    \"\"\"\n    Create a query to subset the default collection based on the provided parameters.\n\n    Args:\n        longitude_column: Name of the column containing longitude values.\n        latitude_column: Name of the column containing latitude values.\n        time_column: Name of the column containing time values.\n        depth_column: Name of the column containing depth values.\n        columns: List of additional columns to include in the query.\n        bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).\n        depth_range: Optional range for depth defined as (min_depth, max_depth).\n        time_range: Optional range for time defined as (start_time, end_time).\n    Returns\n        A Query object that can be executed to retrieve the subset of data.\n    \"\"\"\n    table = self.list_tables()['default']\n    return table.subset(\n        longitude_column=longitude_column,\n        latitude_column=latitude_column,\n        time_column=time_column,\n        depth_column=depth_column,\n        columns=columns,\n        bbox=bbox,\n        depth_range=depth_range,\n        time_range=time_range\n    )\n</code></pre>"},{"location":"reference/functions/","title":"Functions Reference","text":"Source code in <code>beacon_api/query.py</code> <pre><code>class Functions:\n    @staticmethod\n    def concat(args: List[Union[str, Select]], alias: str) -&gt; SelectFunction:\n        \"\"\"\n        Constructs a CONCAT function, concatenating the selected columns or arguments.\n        Args:\n            args (list[str  |  Select]): List of column names (str) or Select objects to concatenate.\n            alias (str): Alias name for the resulting select expression.\n        \"\"\"\n\n        select_args = []\n        for arg in args:\n            if isinstance(arg, str):\n                select_args.append(SelectColumn(column=arg))\n            elif isinstance(arg, Select):\n                select_args.append(arg)\n        return SelectFunction(\"concat\", args=select_args, alias=alias)\n\n    @staticmethod\n    def coalesce(args: List[Union[str, Select]], alias: str) -&gt; SelectFunction:\n        \"\"\"\n        Constructs a COALESCE function, returning the first non-null value from the selected columns or arguments.\n        Args:\n            args (list[str  |  Select]): List of column names (str) or Select objects to coalesce.\n            alias (str): Alias name for the resulting select expression.\n\n        Returns:\n            SelectFunction: SelectFunction representing the COALESCE operation.\n        \"\"\"\n        select_args = []\n        for arg in args:\n            if isinstance(arg, str):\n                select_args.append(SelectColumn(column=arg))\n            elif isinstance(arg, Select):\n                select_args.append(arg)\n        return SelectFunction(\"coalesce\", args=select_args, alias=alias)\n\n    @staticmethod\n    def try_cast_to_type(arg: Union[str, Select], to_type: DTypeLike, alias: str) -&gt; SelectFunction:\n            \"\"\"\n            Attempts to cast the input column or argument to the specified data type.\n            Args:\n                arg: Column name (str) or Select object to cast.\n                to_type: Target data type (compatible with numpy dtype). Eg. np.int64, np.float64, np.datetime64, np.str_\n                alias: Alias name for the resulting select expression.\n            Returns:\n                SelectFunction representing the cast operation.\n            \"\"\"\n            dtype = np.dtype(to_type)  # normalize everything into a np.dtype\n            arrow_type = None\n            if np.issubdtype(dtype, np.integer):\n                print(\"This is an integer dtype:\", dtype)\n                arrow_type = \"Int64\"\n            elif np.issubdtype(dtype, np.floating):\n                arrow_type = \"Float64\"\n            elif np.issubdtype(dtype, np.datetime64):\n                arrow_type = 'Timestamp(Nanosecond, None)'\n            elif np.issubdtype(dtype, np.str_):\n                arrow_type = 'Utf8'\n            else:\n                raise ValueError(f\"Unsupported type for cast_to_type: {to_type}\")\n\n            if isinstance(arg, str):\n                arg = SelectColumn(column=arg)\n                return SelectFunction(\"try_arrow_cast\", args=[arg, SelectLiteral(value=arrow_type)], alias=alias)\n            elif isinstance(arg, Select):\n                return SelectFunction(\"try_arrow_cast\", args=[arg, SelectLiteral(value=arrow_type)], alias=alias)\n\n    @staticmethod\n    def cast_byte_to_char(arg: Union[str, Select], alias: str) -&gt; SelectFunction:\n        \"\"\"Maps byte values to char.\n\n        Args:\n            arg (str | Select): column name (str) or Select object containing the byte value.\n            alias (str): Alias name for the resulting select expression/column.\n\n        Returns:\n            SelectFunction: SelectFunction representing the cast operation.\n        \"\"\"\n        if isinstance(arg, str):\n            arg = SelectColumn(column=arg)\n        return SelectFunction(\"cast_int8_as_char\", args=[arg], alias=alias)\n\n    @staticmethod\n    def map_wod_quality_flag_to_sdn_scheme(arg: Union[str, Select], alias: str) -&gt; SelectFunction:\n        \"\"\"Maps WOD quality flags to the SDN scheme.\n\n        Args:\n            arg (str | Select): column name (str) or Select object containing the WOD quality flag.\n            alias (str): Alias name for the resulting select expression/column.\n\n        Returns:\n            SelectFunction: SelectFunction representing the mapping operation.\n        \"\"\"\n        if isinstance(arg, str):\n            arg = SelectColumn(column=arg)\n        return SelectFunction(\"map_wod_quality_flag\", args=[arg], alias=alias)\n\n    @staticmethod\n    def map_pressure_to_depth(arg: Union[str, Select], latitude_column: Union[str, Select], alias: str) -&gt; SelectFunction:\n        \"\"\"Maps pressure values to depth based on latitude using teos-10.\n\n        Args:\n            arg (str | Select): column name (str) or Select object containing the pressure value.\n            latitude_column (str | Select): column name (str) or Select object containing the latitude value.\n            alias (str): Alias name for the resulting select expression/column.\n\n        Returns:\n            SelectFunction: SelectFunction representing the pressure-to-depth mapping operation.\n        \"\"\"\n        if isinstance(arg, str):\n            arg = SelectColumn(column=arg)\n        if isinstance(latitude_column, str):\n            latitude_column = SelectColumn(column=latitude_column)\n        return SelectFunction(\"pressure_to_depth_teos_10\", args=[arg, latitude_column], alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.cast_byte_to_char","title":"<code>cast_byte_to_char(arg, alias)</code>  <code>staticmethod</code>","text":"<p>Maps byte values to char.</p> <p>Args:     arg (str | Select): column name (str) or Select object containing the byte value.     alias (str): Alias name for the resulting select expression/column.</p> <p>Returns:     SelectFunction: SelectFunction representing the cast operation.</p> Source code in <code>beacon_api/query.py</code> <pre><code>@staticmethod\ndef cast_byte_to_char(arg: Union[str, Select], alias: str) -&gt; SelectFunction:\n    \"\"\"Maps byte values to char.\n\n    Args:\n        arg (str | Select): column name (str) or Select object containing the byte value.\n        alias (str): Alias name for the resulting select expression/column.\n\n    Returns:\n        SelectFunction: SelectFunction representing the cast operation.\n    \"\"\"\n    if isinstance(arg, str):\n        arg = SelectColumn(column=arg)\n    return SelectFunction(\"cast_int8_as_char\", args=[arg], alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.coalesce","title":"<code>coalesce(args, alias)</code>  <code>staticmethod</code>","text":"<p>Constructs a COALESCE function, returning the first non-null value from the selected columns or arguments. Args:     args (list[str  |  Select]): List of column names (str) or Select objects to coalesce.     alias (str): Alias name for the resulting select expression.</p> <p>Returns:     SelectFunction: SelectFunction representing the COALESCE operation.</p> Source code in <code>beacon_api/query.py</code> <pre><code>@staticmethod\ndef coalesce(args: List[Union[str, Select]], alias: str) -&gt; SelectFunction:\n    \"\"\"\n    Constructs a COALESCE function, returning the first non-null value from the selected columns or arguments.\n    Args:\n        args (list[str  |  Select]): List of column names (str) or Select objects to coalesce.\n        alias (str): Alias name for the resulting select expression.\n\n    Returns:\n        SelectFunction: SelectFunction representing the COALESCE operation.\n    \"\"\"\n    select_args = []\n    for arg in args:\n        if isinstance(arg, str):\n            select_args.append(SelectColumn(column=arg))\n        elif isinstance(arg, Select):\n            select_args.append(arg)\n    return SelectFunction(\"coalesce\", args=select_args, alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.concat","title":"<code>concat(args, alias)</code>  <code>staticmethod</code>","text":"<p>Constructs a CONCAT function, concatenating the selected columns or arguments. Args:     args (list[str  |  Select]): List of column names (str) or Select objects to concatenate.     alias (str): Alias name for the resulting select expression.</p> Source code in <code>beacon_api/query.py</code> <pre><code>@staticmethod\ndef concat(args: List[Union[str, Select]], alias: str) -&gt; SelectFunction:\n    \"\"\"\n    Constructs a CONCAT function, concatenating the selected columns or arguments.\n    Args:\n        args (list[str  |  Select]): List of column names (str) or Select objects to concatenate.\n        alias (str): Alias name for the resulting select expression.\n    \"\"\"\n\n    select_args = []\n    for arg in args:\n        if isinstance(arg, str):\n            select_args.append(SelectColumn(column=arg))\n        elif isinstance(arg, Select):\n            select_args.append(arg)\n    return SelectFunction(\"concat\", args=select_args, alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.map_pressure_to_depth","title":"<code>map_pressure_to_depth(arg, latitude_column, alias)</code>  <code>staticmethod</code>","text":"<p>Maps pressure values to depth based on latitude using teos-10.</p> <p>Args:     arg (str | Select): column name (str) or Select object containing the pressure value.     latitude_column (str | Select): column name (str) or Select object containing the latitude value.     alias (str): Alias name for the resulting select expression/column.</p> <p>Returns:     SelectFunction: SelectFunction representing the pressure-to-depth mapping operation.</p> Source code in <code>beacon_api/query.py</code> <pre><code>@staticmethod\ndef map_pressure_to_depth(arg: Union[str, Select], latitude_column: Union[str, Select], alias: str) -&gt; SelectFunction:\n    \"\"\"Maps pressure values to depth based on latitude using teos-10.\n\n    Args:\n        arg (str | Select): column name (str) or Select object containing the pressure value.\n        latitude_column (str | Select): column name (str) or Select object containing the latitude value.\n        alias (str): Alias name for the resulting select expression/column.\n\n    Returns:\n        SelectFunction: SelectFunction representing the pressure-to-depth mapping operation.\n    \"\"\"\n    if isinstance(arg, str):\n        arg = SelectColumn(column=arg)\n    if isinstance(latitude_column, str):\n        latitude_column = SelectColumn(column=latitude_column)\n    return SelectFunction(\"pressure_to_depth_teos_10\", args=[arg, latitude_column], alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.map_wod_quality_flag_to_sdn_scheme","title":"<code>map_wod_quality_flag_to_sdn_scheme(arg, alias)</code>  <code>staticmethod</code>","text":"<p>Maps WOD quality flags to the SDN scheme.</p> <p>Args:     arg (str | Select): column name (str) or Select object containing the WOD quality flag.     alias (str): Alias name for the resulting select expression/column.</p> <p>Returns:     SelectFunction: SelectFunction representing the mapping operation.</p> Source code in <code>beacon_api/query.py</code> <pre><code>@staticmethod\ndef map_wod_quality_flag_to_sdn_scheme(arg: Union[str, Select], alias: str) -&gt; SelectFunction:\n    \"\"\"Maps WOD quality flags to the SDN scheme.\n\n    Args:\n        arg (str | Select): column name (str) or Select object containing the WOD quality flag.\n        alias (str): Alias name for the resulting select expression/column.\n\n    Returns:\n        SelectFunction: SelectFunction representing the mapping operation.\n    \"\"\"\n    if isinstance(arg, str):\n        arg = SelectColumn(column=arg)\n    return SelectFunction(\"map_wod_quality_flag\", args=[arg], alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.try_cast_to_type","title":"<code>try_cast_to_type(arg, to_type, alias)</code>  <code>staticmethod</code>","text":"<p>Attempts to cast the input column or argument to the specified data type. Args:     arg: Column name (str) or Select object to cast.     to_type: Target data type (compatible with numpy dtype). Eg. np.int64, np.float64, np.datetime64, np.str_     alias: Alias name for the resulting select expression. Returns:     SelectFunction representing the cast operation.</p> Source code in <code>beacon_api/query.py</code> <pre><code>@staticmethod\ndef try_cast_to_type(arg: Union[str, Select], to_type: DTypeLike, alias: str) -&gt; SelectFunction:\n        \"\"\"\n        Attempts to cast the input column or argument to the specified data type.\n        Args:\n            arg: Column name (str) or Select object to cast.\n            to_type: Target data type (compatible with numpy dtype). Eg. np.int64, np.float64, np.datetime64, np.str_\n            alias: Alias name for the resulting select expression.\n        Returns:\n            SelectFunction representing the cast operation.\n        \"\"\"\n        dtype = np.dtype(to_type)  # normalize everything into a np.dtype\n        arrow_type = None\n        if np.issubdtype(dtype, np.integer):\n            print(\"This is an integer dtype:\", dtype)\n            arrow_type = \"Int64\"\n        elif np.issubdtype(dtype, np.floating):\n            arrow_type = \"Float64\"\n        elif np.issubdtype(dtype, np.datetime64):\n            arrow_type = 'Timestamp(Nanosecond, None)'\n        elif np.issubdtype(dtype, np.str_):\n            arrow_type = 'Utf8'\n        else:\n            raise ValueError(f\"Unsupported type for cast_to_type: {to_type}\")\n\n        if isinstance(arg, str):\n            arg = SelectColumn(column=arg)\n            return SelectFunction(\"try_arrow_cast\", args=[arg, SelectLiteral(value=arrow_type)], alias=alias)\n        elif isinstance(arg, Select):\n            return SelectFunction(\"try_arrow_cast\", args=[arg, SelectLiteral(value=arrow_type)], alias=alias)\n</code></pre>"},{"location":"reference/query/","title":"Query Reference","text":"Source code in <code>beacon_api/query.py</code> <pre><code>class Query:\n    def __init__(self, http_session: BaseBeaconSession, from_table: Optional[str] = None, from_file_path: Optional[str] = None):\n        \"\"\"\n        A class to build and run Beacon JSON Queries. Best to construct this object using the Client object or Table object.\n        \"\"\"\n        self.http_session = http_session\n        self.from_table = from_table\n        self.from_file_path = from_file_path\n\n    def select(self, selects: List[Select]) -&gt; Self:\n        self.selects = selects\n        return self\n\n    def add_select(self, select: Select) -&gt; Self:\n        if not hasattr(self, \"selects\"):\n            self.selects = []\n        self.selects.append(select)\n        return self\n\n    def add_selects(self, selects: List[Select]) -&gt; Self:\n        \"\"\"Adds multiple select statements to the query.\n\n        Args:\n            selects (list[Select]): The select statements to add.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if not hasattr(self, \"selects\"):\n            self.selects = []\n        self.selects.extend(selects)\n        return self\n\n    def add_select_column(self, column: str, alias: Optional[str] = None) -&gt; Self:\n        \"\"\"Adds a select column to the query.\n\n        Args:\n            column (str): The name of the column to select.\n            alias (str | None, optional): An optional alias for the column. Defaults to None.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if not hasattr(self, \"selects\"):\n            self.selects = []\n        self.selects.append(SelectColumn(column=column, alias=alias))\n        return self\n\n    def add_select_columns(self, columns: List[Tuple[str, Optional[str]]]) -&gt; Self:\n        \"\"\"Adds multiple select columns to the query.\n\n        Args:\n            columns (List[Tuple[str, Optional[str]]]): A list of tuples containing column names and their aliases.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if not hasattr(self, \"selects\"):\n            self.selects = []\n        for column, alias in columns:\n            self.selects.append(SelectColumn(column=column, alias=alias))\n        return self\n\n    def add_select_coalesced(self, mergeable_columns: List[str], alias: str) -&gt; Self:\n        \"\"\"Adds a coalesced select to the query.\n\n        Args:\n            mergeable_columns (list[str]): The columns to merge.\n            alias (str): The alias for the merged column.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if not hasattr(self, \"selects\"):\n            self.selects = []\n\n        function_call = SelectFunction(\"coalesce\", args=[SelectColumn(column=col) for col in mergeable_columns], alias=alias)\n        self.selects.append(function_call)\n        return self\n\n    def filter(self, filters: List[Filter]) -&gt; Self:\n        \"\"\"Adds filters to the query.\n\n        Args:\n            filters (list[Filter]): The filters to add.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.filters = filters\n        return self\n\n    def add_filter(self, filter: Filter) -&gt; Self:\n        \"\"\"Adds a filter to the query.\n\n        Args:\n            filter (Filter): The filter to add.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if not hasattr(self, \"filters\"):\n            self.filters = []\n        self.filters.append(filter)\n        return self\n\n    def add_bbox_filter(\n        self,\n        longitude_column: str,\n        latitude_column: str,\n        bbox: Tuple[float, float, float, float],\n    ) -&gt; Self:\n        \"\"\"Adds a bounding box filter to the query.\n\n        Args:\n            longitude_column (str): The name of the column for longitude.\n            latitude_column (str): The name of the column for latitude.\n            bbox (tuple[float, float, float, float]): The bounding box coordinates (min_lon, max_lon, min_lat, max_lat).\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if not hasattr(self, \"filters\"):\n            self.filters = []\n        self.filters.append(\n            AndFilter(\n                filters=[\n                    RangeFilter(column=longitude_column, gt_eq=bbox[0]),\n                    RangeFilter(column=longitude_column, lt_eq=bbox[2]),\n                    RangeFilter(column=latitude_column, gt_eq=bbox[1]),\n                    RangeFilter(column=latitude_column, lt_eq=bbox[3]),\n                ]\n            )\n        )\n        return self\n\n    def add_polygon_filter(self, longitude_column: str, latitude_column: str, polygon: List[Tuple[float, float]]) -&gt; Self:\n        \"\"\"Adds a POLYGON filter to the query.\n\n        Args:\n            longitude_column (str): The name of the column for longitude.\n            latitude_column (str): The name of the column for latitude.\n            polygon (list[tuple[float, float]]): A list of (longitude, latitude) tuples defining the polygon.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if not hasattr(self, \"filters\"):\n            self.filters = []\n        self.filters.append(PolygonFilter(longitude_column=longitude_column, latitude_column=latitude_column, polygon=polygon))\n        return self\n\n    def add_range_filter(\n        self,\n        column: str,\n        gt_eq: Union[str, int, float, datetime, None] = None,\n        lt_eq: Union[str, int, float, datetime, None] = None,\n    ) -&gt; Self:\n        \"\"\"Adds a RANGE filter to the query.\n\n        Args:\n            column (str): The name of the column to filter.\n            gt_eq (str | int | float | datetime | None, optional): The lower bound for the range filter. Defaults to None.\n            lt_eq (str | int | float | datetime | None, optional): The upper bound for the range filter. Defaults to None.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if not hasattr(self, \"filters\"):\n            self.filters = []\n        self.filters.append(RangeFilter(column=column, gt_eq=gt_eq, lt_eq=lt_eq))\n        return self\n\n    def add_equals_filter(\n        self, column: str, eq: Union[str, int, float, bool, datetime]\n    ) -&gt; Self:\n        \"\"\"Adds an EQUALS filter to the query.\n\n        Args:\n            column (str): The name of the column to filter.\n            eq (str | int | float | bool | datetime): The value to compare against.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if not hasattr(self, \"filters\"):\n            self.filters = []\n        self.filters.append(EqualsFilter(column=column, eq=eq))\n        return self\n\n    def add_not_equals_filter(\n        self, column: str, neq: Union[str, int, float, bool, datetime]\n    ) -&gt; Self:\n        \"\"\"Adds a NOT EQUALS filter to the query.\n\n        Args:\n            column (str): The name of the column to filter.\n            neq (str | int | float | bool | datetime): The value to compare against.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n\n        if not hasattr(self, \"filters\"):\n            self.filters = []\n        self.filters.append(NotEqualsFilter(column=column, neq=neq))\n        return self\n\n    def add_is_null_filter(self, column: str) -&gt; Self:\n        \"\"\"Adds an IS NULL filter to the query.\n\n        Args:\n            column (str): The name of the column to filter.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if not hasattr(self, \"filters\"):\n            self.filters = []\n        self.filters.append(FilterIsNull(column=column))\n        return self\n\n    def add_is_not_null_filter(self, column: str) -&gt; Self:\n        \"\"\"Adds an IS NOT NULL filter to the query.\n\n        Args:\n            column (str): The name of the column to filter.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if not hasattr(self, \"filters\"):\n            self.filters = []\n        self.filters.append(IsNotNullFilter(column=column))\n        return self\n\n    def set_output(self, output: Output) -&gt; Self:\n        \"\"\"Sets the output format for the query.\n\n        Args:\n            output (Output): The output format to use.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.output = output\n        return self\n\n    def compile_query(self) -&gt; str:\n        \"\"\"Compiles the query into a Beacon JSON Query.\n\n        Raises:\n            ValueError: If the query is invalid.\n            ValueError: If the query is invalid.\n            TypeError: If the query is invalid.\n\n        Returns:\n            str: The compiled query as a JSON string.\n        \"\"\"\n        # Check if from_table is set\n        from_ = None\n        if not self.from_table and not self.from_file_path:\n            from_ = \"default\"\n        elif self.from_table and self.from_file_path:\n            raise ValueError(\"Cannot set both from_table and from_file_path\")\n        elif self.from_file_path:\n            from_ = self.from_file_path\n        else:\n            from_ = self.from_table\n\n        # Check if output is set\n        if not hasattr(self, \"output\"):\n            raise ValueError(\"Output must be set before compiling the query\")\n\n        # Check if selects are set\n        if not hasattr(self, \"selects\"):\n            raise ValueError(\"Selects must be set before compiling the query\")\n\n        query = {\n            \"from\": from_,\n            \"select\": (\n                [s.to_dict() for s in self.selects] if hasattr(self, \"selects\") else []\n            ),\n            \"filters\": (\n                [f.to_dict() for f in self.filters] if hasattr(self, \"filters\") else []\n            ),\n            \"output\": self.output.to_dict() if hasattr(self, \"output\") else {},\n        }\n\n        # Convert datetime objects to ISO format strings\n        # This is necessary for JSON serialization\n        def datetime_converter(o):\n            if isinstance(o, datetime):\n                return o.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n            raise TypeError(f\"Type {type(o)} not serializable\")\n\n        return json.dumps(query, default=datetime_converter)\n\n    def run(self) -&gt; Response:\n        \"\"\"Run the query and return the response\"\"\"\n        query = self.compile_query()\n        print(f\"Running query: {query}\")\n        response = self.http_session.post(\"/api/query\", data=query)\n        if response.status_code != 200:\n            raise Exception(f\"Query failed: {response.text}\")\n        if len(response.content) == 0:\n            raise Exception(\"Query returned no content\")\n        return response\n\n    def explain(self) -&gt; dict:\n        \"\"\"Get the query plan\"\"\"\n        query = self.compile_query()\n        response = self.http_session.post(\"/api/explain-query\", data=query)\n        if response.status_code != 200:\n            raise Exception(f\"Explain query failed: {response.text}\")\n        return response.json()\n\n    def explain_visualize(self):\n        \"\"\"Visualize the query plan using networkx and matplotlib\"\"\"\n\n        try: \n            import networkx as nx\n            import matplotlib.pyplot as plt\n        except ImportError as e:\n            raise ImportError(\n                \"This function requires `networkx` and `matplotlib`. Install with `pip install beacon-api[profiling]`.\"\n            ) from e\n\n        plan_json = self.explain()\n        # Extract the root plan node\n        root_plan = plan_json[0][\"Plan\"]\n\n        # === Step 2: Build a directed graph ===\n        G = nx.DiGraph()\n\n        def make_label(node):\n            \"\"\"Build a multi\u2010line label from whichever fields are present.\"\"\"\n            parts = [node.get(\"Node Type\", \"&lt;unknown&gt;\")]\n            for field in (\n                \"File Type\",\n                \"Options\",\n                \"Condition\",\n                \"Output URL\",\n                \"Expressions\",\n                \"Output\",\n                \"Filter\",\n            ):\n                if field in node and node[field]:\n                    parts.append(f\"{field}: {node[field]}\")\n            return \"\\n\".join(parts)\n\n        def add_nodes(node, parent_id=None):\n            nid = id(node)\n            G.add_node(nid, label=make_label(node))\n            if parent_id is not None:\n                G.add_edge(parent_id, nid)\n            for child in node.get(\"Plans\", []):\n                add_nodes(child, nid)\n\n        add_nodes(root_plan)\n\n        try:\n            pos = nx.nx_agraph.graphviz_layout(G, prog=\"dot\")\n        except Exception:\n            pos = nx.spring_layout(G)\n\n        plt.figure(figsize=(8, 6))\n        labels = nx.get_node_attributes(G, \"label\")\n        nx.draw(G, pos, labels=labels, with_labels=True, node_size=2000, font_size=8)\n        plt.title(\"Beacon Query Plan Visualization\")\n        plt.tight_layout()\n        plt.show()\n\n    def to_netcdf(self, filename: str, build_nc_local: bool = True):\n        \"\"\"Export the query result to a NetCDF file\n        Args:\n            filename (str): The name of the output NetCDF file.\n            build_nc_local (bool): \n                If True, build the NetCDF file locally using pandas and xarray. (This is likely faster in most cases.)\n                If False, use the server to build the NetCDF file.\n        \"\"\"\n        # If build_nc_local is True, we will build the NetCDF file locally\n        if build_nc_local:\n            df = self.to_pandas_dataframe()\n            xdf = df.to_xarray()\n            xdf.to_netcdf(filename, mode=\"w\")\n        # If build_nc_local is False, we will use the server to build the NetCDF\n        else:\n            self.set_output(NetCDF())\n            response = self.run()\n            with open(filename, \"wb\") as f:\n                # Write the content of the response to a file\n                for chunk in response.iter_content(chunk_size=1024 * 1024):\n                    if chunk:  # skip keep-alive chunks\n                        f.write(chunk)\n\n    def to_arrow(self, filename: str):\n        \"\"\"\n        Converts the query result to Apache Arrow format and writes it to a file.\n\n        Args:\n            filename (str): The path to the file where the Arrow-formatted data will be saved.\n\n        Returns:\n            None\n\n        Side Effects:\n            Writes the Arrow-formatted response content to the specified file.\n        \"\"\"\n        self.set_output(Arrow())\n        response = self.run()\n\n        with open(filename, \"wb\") as f:\n            # Write the content of the response to a file\n            for chunk in response.iter_content(chunk_size=1024 * 1024):\n                if chunk:  # skip keep-alive chunks\n                    f.write(chunk)\n\n    def to_parquet(self, filename: str, streaming_chunk_size: int = 1024 * 1024):\n        \"\"\"\n        Exports the query results to a Parquet file.\n\n        This method sets the output format to Parquet, executes the query, and writes the resulting data to the specified file.\n\n        Args:\n            filename (str): The path to the file where the Parquet data will be saved.\n\n        Returns:\n            None\n        \"\"\"\n        self.set_output(Parquet())\n        response = self.run()\n\n        with open(filename, \"wb\") as f:\n            # Write the content of the response to a file\n            for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n                if chunk:  # skip keep-alive chunks\n                    f.write(chunk)\n\n    def to_geoparquet(self, filename: str, longitude_column: str, latitude_column: str, streaming_chunk_size: int = 1024 * 1024):\n        \"\"\"\n        Exports the query results to a GeoParquet file.\n\n        Args:\n            filename (str): The path to the file where the GeoParquet data will be saved.\n            longitude_column (str): The name of the column representing longitude.\n            latitude_column (str): The name of the column representing latitude.\n        \"\"\"\n        self.set_output(GeoParquet(longitude_column=longitude_column, latitude_column=latitude_column))\n        response = self.run()\n\n        with open(filename, \"wb\") as f:\n            # Write the content of the response to a file\n            for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n                if chunk:  # skip keep-alive chunks\n                    f.write(chunk)\n\n    def to_csv(self, filename: str, streaming_chunk_size: int = 1024 * 1024):\n        \"\"\"Exports the query results to a CSV file.\n\n        Args:\n            filename (str): The path to the file where the CSV data will be saved.\n        \"\"\"\n        self.set_output(CSV())\n        response = self.run()\n\n        with open(filename, \"wb\") as f:\n            # Write the content of the response to a file\n            for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n                if chunk:  # skip keep-alive chunks\n                    f.write(chunk)\n\n    def to_zarr(self, filename: str):\n        \"\"\"Exports the query results to a Zarr file.\n\n        Args:\n            filename (str): The path to the file where the Zarr data will be saved.\n        \"\"\"\n\n        try:\n            import zarr # just to check if zarr is installed\n        except ImportError as e:\n            raise ImportError(\n                \"This function requires `zarr`. Install with `pip install beacon-api[zarr]`.\"\n            ) from e\n\n        # Read to pandas dataframe first\n        df = self.to_pandas_dataframe()\n        # Convert to Zarr format\n        xdf = df.to_xarray()\n        xdf.to_zarr(filename, mode=\"w\")\n\n    def to_pandas_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Converts the query results to a pandas DataFrame.\n\n        Returns:\n            pd.DataFrame: The query results as a pandas DataFrame.\n        \"\"\"\n        self.set_output(Parquet())\n        response = self.run()\n        bytes_io = BytesIO(response.content)\n\n        df = pd.read_parquet(bytes_io)\n        return df\n\n    def to_geo_pandas_dataframe(self, longitude_column: str, latitude_column: str, crs: str = \"EPSG:4326\") -&gt; gpd.GeoDataFrame:\n        \"\"\"Converts the query results to a GeoPandas GeoDataFrame.\n\n        Args:\n            longitude_column (str): The name of the column representing longitude.\n            latitude_column (str): The name of the column representing latitude.\n            crs (str, optional): The coordinate reference system to use. Defaults to \"EPSG:4326\".\n\n        Returns:\n            gpd.GeoDataFrame: The query results as a GeoPandas GeoDataFrame.\n        \"\"\"\n\n        try:\n            import geopandas as gpd\n        except ImportError as e:\n            raise ImportError(\n                \"This function requires `geopandas`. Install with `pip install beacon-api[geopandas]`.\"\n            ) from e\n\n        self.set_output(GeoParquet(longitude_column=longitude_column, latitude_column=latitude_column))\n        response = self.run()\n        bytes_io = BytesIO(response.content)\n        # Read into parquet arrow table \n        table = pq.read_table(bytes_io)\n\n        gdf = gpd.GeoDataFrame.from_arrow(table)\n        gdf.set_crs(crs, inplace=True)\n        return gdf\n\n    def to_odv(self, odv_output: Odv, filename: str):\n        \"\"\"Exports the query results to an ODV file.\n\n        Args:\n            odv_output (Odv): The ODV output format to use.\n            filename (str): The path to the file where the ODV data will be saved.\n        \"\"\"\n        self.set_output(odv_output)\n        response = self.run()\n        with open(filename, \"wb\") as f:\n            # Write the content of the response to a file\n            f.write(response.content)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.__init__","title":"<code>__init__(http_session, from_table=None, from_file_path=None)</code>","text":"<p>A class to build and run Beacon JSON Queries. Best to construct this object using the Client object or Table object.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def __init__(self, http_session: BaseBeaconSession, from_table: Optional[str] = None, from_file_path: Optional[str] = None):\n    \"\"\"\n    A class to build and run Beacon JSON Queries. Best to construct this object using the Client object or Table object.\n    \"\"\"\n    self.http_session = http_session\n    self.from_table = from_table\n    self.from_file_path = from_file_path\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_bbox_filter","title":"<code>add_bbox_filter(longitude_column, latitude_column, bbox)</code>","text":"<p>Adds a bounding box filter to the query.</p> <p>Args:     longitude_column (str): The name of the column for longitude.     latitude_column (str): The name of the column for latitude.     bbox (tuple[float, float, float, float]): The bounding box coordinates (min_lon, max_lon, min_lat, max_lat).</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_bbox_filter(\n    self,\n    longitude_column: str,\n    latitude_column: str,\n    bbox: Tuple[float, float, float, float],\n) -&gt; Self:\n    \"\"\"Adds a bounding box filter to the query.\n\n    Args:\n        longitude_column (str): The name of the column for longitude.\n        latitude_column (str): The name of the column for latitude.\n        bbox (tuple[float, float, float, float]): The bounding box coordinates (min_lon, max_lon, min_lat, max_lat).\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if not hasattr(self, \"filters\"):\n        self.filters = []\n    self.filters.append(\n        AndFilter(\n            filters=[\n                RangeFilter(column=longitude_column, gt_eq=bbox[0]),\n                RangeFilter(column=longitude_column, lt_eq=bbox[2]),\n                RangeFilter(column=latitude_column, gt_eq=bbox[1]),\n                RangeFilter(column=latitude_column, lt_eq=bbox[3]),\n            ]\n        )\n    )\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_equals_filter","title":"<code>add_equals_filter(column, eq)</code>","text":"<p>Adds an EQUALS filter to the query.</p> <p>Args:     column (str): The name of the column to filter.     eq (str | int | float | bool | datetime): The value to compare against.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_equals_filter(\n    self, column: str, eq: Union[str, int, float, bool, datetime]\n) -&gt; Self:\n    \"\"\"Adds an EQUALS filter to the query.\n\n    Args:\n        column (str): The name of the column to filter.\n        eq (str | int | float | bool | datetime): The value to compare against.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if not hasattr(self, \"filters\"):\n        self.filters = []\n    self.filters.append(EqualsFilter(column=column, eq=eq))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_filter","title":"<code>add_filter(filter)</code>","text":"<p>Adds a filter to the query.</p> <p>Args:     filter (Filter): The filter to add.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_filter(self, filter: Filter) -&gt; Self:\n    \"\"\"Adds a filter to the query.\n\n    Args:\n        filter (Filter): The filter to add.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if not hasattr(self, \"filters\"):\n        self.filters = []\n    self.filters.append(filter)\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_is_not_null_filter","title":"<code>add_is_not_null_filter(column)</code>","text":"<p>Adds an IS NOT NULL filter to the query.</p> <p>Args:     column (str): The name of the column to filter.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_is_not_null_filter(self, column: str) -&gt; Self:\n    \"\"\"Adds an IS NOT NULL filter to the query.\n\n    Args:\n        column (str): The name of the column to filter.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if not hasattr(self, \"filters\"):\n        self.filters = []\n    self.filters.append(IsNotNullFilter(column=column))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_is_null_filter","title":"<code>add_is_null_filter(column)</code>","text":"<p>Adds an IS NULL filter to the query.</p> <p>Args:     column (str): The name of the column to filter.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_is_null_filter(self, column: str) -&gt; Self:\n    \"\"\"Adds an IS NULL filter to the query.\n\n    Args:\n        column (str): The name of the column to filter.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if not hasattr(self, \"filters\"):\n        self.filters = []\n    self.filters.append(FilterIsNull(column=column))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_not_equals_filter","title":"<code>add_not_equals_filter(column, neq)</code>","text":"<p>Adds a NOT EQUALS filter to the query.</p> <p>Args:     column (str): The name of the column to filter.     neq (str | int | float | bool | datetime): The value to compare against.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_not_equals_filter(\n    self, column: str, neq: Union[str, int, float, bool, datetime]\n) -&gt; Self:\n    \"\"\"Adds a NOT EQUALS filter to the query.\n\n    Args:\n        column (str): The name of the column to filter.\n        neq (str | int | float | bool | datetime): The value to compare against.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n\n    if not hasattr(self, \"filters\"):\n        self.filters = []\n    self.filters.append(NotEqualsFilter(column=column, neq=neq))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_polygon_filter","title":"<code>add_polygon_filter(longitude_column, latitude_column, polygon)</code>","text":"<p>Adds a POLYGON filter to the query.</p> <p>Args:     longitude_column (str): The name of the column for longitude.     latitude_column (str): The name of the column for latitude.     polygon (list[tuple[float, float]]): A list of (longitude, latitude) tuples defining the polygon.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_polygon_filter(self, longitude_column: str, latitude_column: str, polygon: List[Tuple[float, float]]) -&gt; Self:\n    \"\"\"Adds a POLYGON filter to the query.\n\n    Args:\n        longitude_column (str): The name of the column for longitude.\n        latitude_column (str): The name of the column for latitude.\n        polygon (list[tuple[float, float]]): A list of (longitude, latitude) tuples defining the polygon.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if not hasattr(self, \"filters\"):\n        self.filters = []\n    self.filters.append(PolygonFilter(longitude_column=longitude_column, latitude_column=latitude_column, polygon=polygon))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_range_filter","title":"<code>add_range_filter(column, gt_eq=None, lt_eq=None)</code>","text":"<p>Adds a RANGE filter to the query.</p> <p>Args:     column (str): The name of the column to filter.     gt_eq (str | int | float | datetime | None, optional): The lower bound for the range filter. Defaults to None.     lt_eq (str | int | float | datetime | None, optional): The upper bound for the range filter. Defaults to None.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_range_filter(\n    self,\n    column: str,\n    gt_eq: Union[str, int, float, datetime, None] = None,\n    lt_eq: Union[str, int, float, datetime, None] = None,\n) -&gt; Self:\n    \"\"\"Adds a RANGE filter to the query.\n\n    Args:\n        column (str): The name of the column to filter.\n        gt_eq (str | int | float | datetime | None, optional): The lower bound for the range filter. Defaults to None.\n        lt_eq (str | int | float | datetime | None, optional): The upper bound for the range filter. Defaults to None.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if not hasattr(self, \"filters\"):\n        self.filters = []\n    self.filters.append(RangeFilter(column=column, gt_eq=gt_eq, lt_eq=lt_eq))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_select_coalesced","title":"<code>add_select_coalesced(mergeable_columns, alias)</code>","text":"<p>Adds a coalesced select to the query.</p> <p>Args:     mergeable_columns (list[str]): The columns to merge.     alias (str): The alias for the merged column.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_select_coalesced(self, mergeable_columns: List[str], alias: str) -&gt; Self:\n    \"\"\"Adds a coalesced select to the query.\n\n    Args:\n        mergeable_columns (list[str]): The columns to merge.\n        alias (str): The alias for the merged column.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if not hasattr(self, \"selects\"):\n        self.selects = []\n\n    function_call = SelectFunction(\"coalesce\", args=[SelectColumn(column=col) for col in mergeable_columns], alias=alias)\n    self.selects.append(function_call)\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_select_column","title":"<code>add_select_column(column, alias=None)</code>","text":"<p>Adds a select column to the query.</p> <p>Args:     column (str): The name of the column to select.     alias (str | None, optional): An optional alias for the column. Defaults to None.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_select_column(self, column: str, alias: Optional[str] = None) -&gt; Self:\n    \"\"\"Adds a select column to the query.\n\n    Args:\n        column (str): The name of the column to select.\n        alias (str | None, optional): An optional alias for the column. Defaults to None.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if not hasattr(self, \"selects\"):\n        self.selects = []\n    self.selects.append(SelectColumn(column=column, alias=alias))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_select_columns","title":"<code>add_select_columns(columns)</code>","text":"<p>Adds multiple select columns to the query.</p> <p>Args:     columns (List[Tuple[str, Optional[str]]]): A list of tuples containing column names and their aliases.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_select_columns(self, columns: List[Tuple[str, Optional[str]]]) -&gt; Self:\n    \"\"\"Adds multiple select columns to the query.\n\n    Args:\n        columns (List[Tuple[str, Optional[str]]]): A list of tuples containing column names and their aliases.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if not hasattr(self, \"selects\"):\n        self.selects = []\n    for column, alias in columns:\n        self.selects.append(SelectColumn(column=column, alias=alias))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.add_selects","title":"<code>add_selects(selects)</code>","text":"<p>Adds multiple select statements to the query.</p> <p>Args:     selects (list[Select]): The select statements to add.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def add_selects(self, selects: List[Select]) -&gt; Self:\n    \"\"\"Adds multiple select statements to the query.\n\n    Args:\n        selects (list[Select]): The select statements to add.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if not hasattr(self, \"selects\"):\n        self.selects = []\n    self.selects.extend(selects)\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.compile_query","title":"<code>compile_query()</code>","text":"<p>Compiles the query into a Beacon JSON Query.</p> <p>Raises:     ValueError: If the query is invalid.     ValueError: If the query is invalid.     TypeError: If the query is invalid.</p> <p>Returns:     str: The compiled query as a JSON string.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def compile_query(self) -&gt; str:\n    \"\"\"Compiles the query into a Beacon JSON Query.\n\n    Raises:\n        ValueError: If the query is invalid.\n        ValueError: If the query is invalid.\n        TypeError: If the query is invalid.\n\n    Returns:\n        str: The compiled query as a JSON string.\n    \"\"\"\n    # Check if from_table is set\n    from_ = None\n    if not self.from_table and not self.from_file_path:\n        from_ = \"default\"\n    elif self.from_table and self.from_file_path:\n        raise ValueError(\"Cannot set both from_table and from_file_path\")\n    elif self.from_file_path:\n        from_ = self.from_file_path\n    else:\n        from_ = self.from_table\n\n    # Check if output is set\n    if not hasattr(self, \"output\"):\n        raise ValueError(\"Output must be set before compiling the query\")\n\n    # Check if selects are set\n    if not hasattr(self, \"selects\"):\n        raise ValueError(\"Selects must be set before compiling the query\")\n\n    query = {\n        \"from\": from_,\n        \"select\": (\n            [s.to_dict() for s in self.selects] if hasattr(self, \"selects\") else []\n        ),\n        \"filters\": (\n            [f.to_dict() for f in self.filters] if hasattr(self, \"filters\") else []\n        ),\n        \"output\": self.output.to_dict() if hasattr(self, \"output\") else {},\n    }\n\n    # Convert datetime objects to ISO format strings\n    # This is necessary for JSON serialization\n    def datetime_converter(o):\n        if isinstance(o, datetime):\n            return o.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n        raise TypeError(f\"Type {type(o)} not serializable\")\n\n    return json.dumps(query, default=datetime_converter)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.explain","title":"<code>explain()</code>","text":"<p>Get the query plan</p> Source code in <code>beacon_api/query.py</code> <pre><code>def explain(self) -&gt; dict:\n    \"\"\"Get the query plan\"\"\"\n    query = self.compile_query()\n    response = self.http_session.post(\"/api/explain-query\", data=query)\n    if response.status_code != 200:\n        raise Exception(f\"Explain query failed: {response.text}\")\n    return response.json()\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.explain_visualize","title":"<code>explain_visualize()</code>","text":"<p>Visualize the query plan using networkx and matplotlib</p> Source code in <code>beacon_api/query.py</code> <pre><code>def explain_visualize(self):\n    \"\"\"Visualize the query plan using networkx and matplotlib\"\"\"\n\n    try: \n        import networkx as nx\n        import matplotlib.pyplot as plt\n    except ImportError as e:\n        raise ImportError(\n            \"This function requires `networkx` and `matplotlib`. Install with `pip install beacon-api[profiling]`.\"\n        ) from e\n\n    plan_json = self.explain()\n    # Extract the root plan node\n    root_plan = plan_json[0][\"Plan\"]\n\n    # === Step 2: Build a directed graph ===\n    G = nx.DiGraph()\n\n    def make_label(node):\n        \"\"\"Build a multi\u2010line label from whichever fields are present.\"\"\"\n        parts = [node.get(\"Node Type\", \"&lt;unknown&gt;\")]\n        for field in (\n            \"File Type\",\n            \"Options\",\n            \"Condition\",\n            \"Output URL\",\n            \"Expressions\",\n            \"Output\",\n            \"Filter\",\n        ):\n            if field in node and node[field]:\n                parts.append(f\"{field}: {node[field]}\")\n        return \"\\n\".join(parts)\n\n    def add_nodes(node, parent_id=None):\n        nid = id(node)\n        G.add_node(nid, label=make_label(node))\n        if parent_id is not None:\n            G.add_edge(parent_id, nid)\n        for child in node.get(\"Plans\", []):\n            add_nodes(child, nid)\n\n    add_nodes(root_plan)\n\n    try:\n        pos = nx.nx_agraph.graphviz_layout(G, prog=\"dot\")\n    except Exception:\n        pos = nx.spring_layout(G)\n\n    plt.figure(figsize=(8, 6))\n    labels = nx.get_node_attributes(G, \"label\")\n    nx.draw(G, pos, labels=labels, with_labels=True, node_size=2000, font_size=8)\n    plt.title(\"Beacon Query Plan Visualization\")\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.filter","title":"<code>filter(filters)</code>","text":"<p>Adds filters to the query.</p> <p>Args:     filters (list[Filter]): The filters to add.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def filter(self, filters: List[Filter]) -&gt; Self:\n    \"\"\"Adds filters to the query.\n\n    Args:\n        filters (list[Filter]): The filters to add.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.filters = filters\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.run","title":"<code>run()</code>","text":"<p>Run the query and return the response</p> Source code in <code>beacon_api/query.py</code> <pre><code>def run(self) -&gt; Response:\n    \"\"\"Run the query and return the response\"\"\"\n    query = self.compile_query()\n    print(f\"Running query: {query}\")\n    response = self.http_session.post(\"/api/query\", data=query)\n    if response.status_code != 200:\n        raise Exception(f\"Query failed: {response.text}\")\n    if len(response.content) == 0:\n        raise Exception(\"Query returned no content\")\n    return response\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.set_output","title":"<code>set_output(output)</code>","text":"<p>Sets the output format for the query.</p> <p>Args:     output (Output): The output format to use.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def set_output(self, output: Output) -&gt; Self:\n    \"\"\"Sets the output format for the query.\n\n    Args:\n        output (Output): The output format to use.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.output = output\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.to_arrow","title":"<code>to_arrow(filename)</code>","text":"<p>Converts the query result to Apache Arrow format and writes it to a file.</p> <p>Args:     filename (str): The path to the file where the Arrow-formatted data will be saved.</p> <p>Returns:     None</p> <p>Side Effects:     Writes the Arrow-formatted response content to the specified file.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def to_arrow(self, filename: str):\n    \"\"\"\n    Converts the query result to Apache Arrow format and writes it to a file.\n\n    Args:\n        filename (str): The path to the file where the Arrow-formatted data will be saved.\n\n    Returns:\n        None\n\n    Side Effects:\n        Writes the Arrow-formatted response content to the specified file.\n    \"\"\"\n    self.set_output(Arrow())\n    response = self.run()\n\n    with open(filename, \"wb\") as f:\n        # Write the content of the response to a file\n        for chunk in response.iter_content(chunk_size=1024 * 1024):\n            if chunk:  # skip keep-alive chunks\n                f.write(chunk)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.to_csv","title":"<code>to_csv(filename, streaming_chunk_size=1024 * 1024)</code>","text":"<p>Exports the query results to a CSV file.</p> <p>Args:     filename (str): The path to the file where the CSV data will be saved.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def to_csv(self, filename: str, streaming_chunk_size: int = 1024 * 1024):\n    \"\"\"Exports the query results to a CSV file.\n\n    Args:\n        filename (str): The path to the file where the CSV data will be saved.\n    \"\"\"\n    self.set_output(CSV())\n    response = self.run()\n\n    with open(filename, \"wb\") as f:\n        # Write the content of the response to a file\n        for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n            if chunk:  # skip keep-alive chunks\n                f.write(chunk)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.to_geo_pandas_dataframe","title":"<code>to_geo_pandas_dataframe(longitude_column, latitude_column, crs='EPSG:4326')</code>","text":"<p>Converts the query results to a GeoPandas GeoDataFrame.</p> <p>Args:     longitude_column (str): The name of the column representing longitude.     latitude_column (str): The name of the column representing latitude.     crs (str, optional): The coordinate reference system to use. Defaults to \"EPSG:4326\".</p> <p>Returns:     gpd.GeoDataFrame: The query results as a GeoPandas GeoDataFrame.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def to_geo_pandas_dataframe(self, longitude_column: str, latitude_column: str, crs: str = \"EPSG:4326\") -&gt; gpd.GeoDataFrame:\n    \"\"\"Converts the query results to a GeoPandas GeoDataFrame.\n\n    Args:\n        longitude_column (str): The name of the column representing longitude.\n        latitude_column (str): The name of the column representing latitude.\n        crs (str, optional): The coordinate reference system to use. Defaults to \"EPSG:4326\".\n\n    Returns:\n        gpd.GeoDataFrame: The query results as a GeoPandas GeoDataFrame.\n    \"\"\"\n\n    try:\n        import geopandas as gpd\n    except ImportError as e:\n        raise ImportError(\n            \"This function requires `geopandas`. Install with `pip install beacon-api[geopandas]`.\"\n        ) from e\n\n    self.set_output(GeoParquet(longitude_column=longitude_column, latitude_column=latitude_column))\n    response = self.run()\n    bytes_io = BytesIO(response.content)\n    # Read into parquet arrow table \n    table = pq.read_table(bytes_io)\n\n    gdf = gpd.GeoDataFrame.from_arrow(table)\n    gdf.set_crs(crs, inplace=True)\n    return gdf\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.to_geoparquet","title":"<code>to_geoparquet(filename, longitude_column, latitude_column, streaming_chunk_size=1024 * 1024)</code>","text":"<p>Exports the query results to a GeoParquet file.</p> <p>Args:     filename (str): The path to the file where the GeoParquet data will be saved.     longitude_column (str): The name of the column representing longitude.     latitude_column (str): The name of the column representing latitude.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def to_geoparquet(self, filename: str, longitude_column: str, latitude_column: str, streaming_chunk_size: int = 1024 * 1024):\n    \"\"\"\n    Exports the query results to a GeoParquet file.\n\n    Args:\n        filename (str): The path to the file where the GeoParquet data will be saved.\n        longitude_column (str): The name of the column representing longitude.\n        latitude_column (str): The name of the column representing latitude.\n    \"\"\"\n    self.set_output(GeoParquet(longitude_column=longitude_column, latitude_column=latitude_column))\n    response = self.run()\n\n    with open(filename, \"wb\") as f:\n        # Write the content of the response to a file\n        for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n            if chunk:  # skip keep-alive chunks\n                f.write(chunk)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.to_netcdf","title":"<code>to_netcdf(filename, build_nc_local=True)</code>","text":"<p>Export the query result to a NetCDF file Args:     filename (str): The name of the output NetCDF file.     build_nc_local (bool):          If True, build the NetCDF file locally using pandas and xarray. (This is likely faster in most cases.)         If False, use the server to build the NetCDF file.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def to_netcdf(self, filename: str, build_nc_local: bool = True):\n    \"\"\"Export the query result to a NetCDF file\n    Args:\n        filename (str): The name of the output NetCDF file.\n        build_nc_local (bool): \n            If True, build the NetCDF file locally using pandas and xarray. (This is likely faster in most cases.)\n            If False, use the server to build the NetCDF file.\n    \"\"\"\n    # If build_nc_local is True, we will build the NetCDF file locally\n    if build_nc_local:\n        df = self.to_pandas_dataframe()\n        xdf = df.to_xarray()\n        xdf.to_netcdf(filename, mode=\"w\")\n    # If build_nc_local is False, we will use the server to build the NetCDF\n    else:\n        self.set_output(NetCDF())\n        response = self.run()\n        with open(filename, \"wb\") as f:\n            # Write the content of the response to a file\n            for chunk in response.iter_content(chunk_size=1024 * 1024):\n                if chunk:  # skip keep-alive chunks\n                    f.write(chunk)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.to_odv","title":"<code>to_odv(odv_output, filename)</code>","text":"<p>Exports the query results to an ODV file.</p> <p>Args:     odv_output (Odv): The ODV output format to use.     filename (str): The path to the file where the ODV data will be saved.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def to_odv(self, odv_output: Odv, filename: str):\n    \"\"\"Exports the query results to an ODV file.\n\n    Args:\n        odv_output (Odv): The ODV output format to use.\n        filename (str): The path to the file where the ODV data will be saved.\n    \"\"\"\n    self.set_output(odv_output)\n    response = self.run()\n    with open(filename, \"wb\") as f:\n        # Write the content of the response to a file\n        f.write(response.content)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.to_pandas_dataframe","title":"<code>to_pandas_dataframe()</code>","text":"<p>Converts the query results to a pandas DataFrame.</p> <p>Returns:     pd.DataFrame: The query results as a pandas DataFrame.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def to_pandas_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Converts the query results to a pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: The query results as a pandas DataFrame.\n    \"\"\"\n    self.set_output(Parquet())\n    response = self.run()\n    bytes_io = BytesIO(response.content)\n\n    df = pd.read_parquet(bytes_io)\n    return df\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.to_parquet","title":"<code>to_parquet(filename, streaming_chunk_size=1024 * 1024)</code>","text":"<p>Exports the query results to a Parquet file.</p> <p>This method sets the output format to Parquet, executes the query, and writes the resulting data to the specified file.</p> <p>Args:     filename (str): The path to the file where the Parquet data will be saved.</p> <p>Returns:     None</p> Source code in <code>beacon_api/query.py</code> <pre><code>def to_parquet(self, filename: str, streaming_chunk_size: int = 1024 * 1024):\n    \"\"\"\n    Exports the query results to a Parquet file.\n\n    This method sets the output format to Parquet, executes the query, and writes the resulting data to the specified file.\n\n    Args:\n        filename (str): The path to the file where the Parquet data will be saved.\n\n    Returns:\n        None\n    \"\"\"\n    self.set_output(Parquet())\n    response = self.run()\n\n    with open(filename, \"wb\") as f:\n        # Write the content of the response to a file\n        for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n            if chunk:  # skip keep-alive chunks\n                f.write(chunk)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.Query.to_zarr","title":"<code>to_zarr(filename)</code>","text":"<p>Exports the query results to a Zarr file.</p> <p>Args:     filename (str): The path to the file where the Zarr data will be saved.</p> Source code in <code>beacon_api/query.py</code> <pre><code>def to_zarr(self, filename: str):\n    \"\"\"Exports the query results to a Zarr file.\n\n    Args:\n        filename (str): The path to the file where the Zarr data will be saved.\n    \"\"\"\n\n    try:\n        import zarr # just to check if zarr is installed\n    except ImportError as e:\n        raise ImportError(\n            \"This function requires `zarr`. Install with `pip install beacon-api[zarr]`.\"\n        ) from e\n\n    # Read to pandas dataframe first\n    df = self.to_pandas_dataframe()\n    # Convert to Zarr format\n    xdf = df.to_xarray()\n    xdf.to_zarr(filename, mode=\"w\")\n</code></pre>"},{"location":"reference/table/","title":"Table Reference","text":""},{"location":"reference/table/#beacon_api.table.DataTable","title":"<code>DataTable</code>","text":"Source code in <code>beacon_api/table.py</code> <pre><code>class DataTable:\n    # Constructor for DataTable\n    def __init__(self, http_session: BaseBeaconSession, table_name: str):\n        self.http_session = http_session\n        self.table_name = table_name\n\n        # Now query the server for the table type and description\n        # api/table-config?table_name={table_name}\n        response = self.http_session.get(\"/api/table-config\", params={\"table_name\": table_name})\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get table config: {response.text}\")\n        table_config = response.json()\n        self.table_type = table_config.get(\"type\", \"unknown\")\n        self.description = table_config.get(\"description\", None)\n\n    def get_table_description(self) -&gt; str:\n        \"\"\"Get the description of the table\"\"\"\n        return self.description if self.description else \"No description available\"    \n\n    def get_table_schema(self) -&gt; dict[str, type]:\n        \"\"\"Get the schema of the table\"\"\"\n        pa_schema = self.get_table_schema_arrow()\n        if pa_schema:\n            schema_dict = pa_schema\n            return schema_dict\n        else:\n            raise Exception(\"Failed to retrieve table schema\")\n\n    def get_table_schema_arrow(self) -&gt; pa.Schema:\n        \"\"\"Get the schema of the table in Arrow format\"\"\"\n        response = self.http_session.get(\"/api/table-schema\", params={\"table_name\": self.table_name})\n\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get table schema: {response.text}\")\n\n        schema_data = response.json()\n        fields = []\n\n        for field in schema_data['fields']:\n            field_type = field['data_type']\n\n            if isinstance(field_type, str):\n                fields.append(pa.field(field['name'], field_type))\n\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Second\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('s')))\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Millisecond\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('ms')))\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Microsecond\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('us')))\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Nanosecond\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('ns')))\n\n            else:\n                raise Exception(f\"Unsupported data type for field {field['name']}: {field_type}\")\n\n\n        return pa.schema(fields)\n\n    def get_table_type(self) -&gt; dict:\n        \"\"\"Get the type of the table\"\"\"\n        return self.table_type\n\n\n    def subset(self, longitude_column: str, latitude_column: str, time_column: str, depth_column: str, columns: list[str],\n                         bbox: Optional[tuple[float, float, float, float]] = None,\n                         depth_range: Optional[tuple[float, float]] = None,\n                         time_range: Optional[tuple[datetime.datetime, datetime.datetime]] = None) -&gt; Query:\n        \"\"\"\n        Create a query to subset the table based on the provided parameters.\n\n        Args:\n            longitude_column: Name of the column containing longitude values.\n            latitude_column: Name of the column containing latitude values.\n            time_column: Name of the column containing time values.\n            depth_column: Name of the column containing depth values.\n            columns: List of additional columns to include in the query.\n            bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).\n            depth_range: Optional range for depth defined as (min_depth, max_depth).\n            time_range: Optional range for time defined as (start_time, end_time).\n        Returns\n            A Query object that can be executed to retrieve the subset of data.\n        \"\"\"\n        query = self.query()\n        query.add_select_column(longitude_column)\n        query.add_select_column(latitude_column)\n        query.add_select_column(time_column)\n        query.add_select_column(depth_column)\n        for column in columns:\n            query.add_select_column(column)\n        if bbox:\n            query.add_filter(AndFilter([\n                RangeFilter(longitude_column, bbox[0], bbox[2]),\n                RangeFilter(latitude_column, bbox[1], bbox[3])\n            ]))\n        if depth_range:\n            query.add_filter(RangeFilter(depth_column, depth_range[0], depth_range[1]))\n        if time_range:\n            query.add_filter(RangeFilter(time_column, time_range[0].strftime(\"%Y-%m-%dT%H:%M:%S\"), time_range[1].strftime(\"%Y-%m-%dT%H:%M:%S\")))\n        return query\n\n    def query(self) -&gt; Query:\n        \"\"\"Create a new query for the selected table.\n        The query can then be built using the Query methods.\n        Returns:\n            Query: A new query object.\n        \"\"\"\n        return Query(self.http_session, self.table_name)\n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.get_table_description","title":"<code>get_table_description()</code>","text":"<p>Get the description of the table</p> Source code in <code>beacon_api/table.py</code> <pre><code>def get_table_description(self) -&gt; str:\n    \"\"\"Get the description of the table\"\"\"\n    return self.description if self.description else \"No description available\"    \n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.get_table_schema","title":"<code>get_table_schema()</code>","text":"<p>Get the schema of the table</p> Source code in <code>beacon_api/table.py</code> <pre><code>def get_table_schema(self) -&gt; dict[str, type]:\n    \"\"\"Get the schema of the table\"\"\"\n    pa_schema = self.get_table_schema_arrow()\n    if pa_schema:\n        schema_dict = pa_schema\n        return schema_dict\n    else:\n        raise Exception(\"Failed to retrieve table schema\")\n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.get_table_schema_arrow","title":"<code>get_table_schema_arrow()</code>","text":"<p>Get the schema of the table in Arrow format</p> Source code in <code>beacon_api/table.py</code> <pre><code>def get_table_schema_arrow(self) -&gt; pa.Schema:\n    \"\"\"Get the schema of the table in Arrow format\"\"\"\n    response = self.http_session.get(\"/api/table-schema\", params={\"table_name\": self.table_name})\n\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get table schema: {response.text}\")\n\n    schema_data = response.json()\n    fields = []\n\n    for field in schema_data['fields']:\n        field_type = field['data_type']\n\n        if isinstance(field_type, str):\n            fields.append(pa.field(field['name'], field_type))\n\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Second\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('s')))\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Millisecond\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('ms')))\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Microsecond\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('us')))\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Nanosecond\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('ns')))\n\n        else:\n            raise Exception(f\"Unsupported data type for field {field['name']}: {field_type}\")\n\n\n    return pa.schema(fields)\n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.get_table_type","title":"<code>get_table_type()</code>","text":"<p>Get the type of the table</p> Source code in <code>beacon_api/table.py</code> <pre><code>def get_table_type(self) -&gt; dict:\n    \"\"\"Get the type of the table\"\"\"\n    return self.table_type\n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.query","title":"<code>query()</code>","text":"<p>Create a new query for the selected table. The query can then be built using the Query methods. Returns:     Query: A new query object.</p> Source code in <code>beacon_api/table.py</code> <pre><code>def query(self) -&gt; Query:\n    \"\"\"Create a new query for the selected table.\n    The query can then be built using the Query methods.\n    Returns:\n        Query: A new query object.\n    \"\"\"\n    return Query(self.http_session, self.table_name)\n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.subset","title":"<code>subset(longitude_column, latitude_column, time_column, depth_column, columns, bbox=None, depth_range=None, time_range=None)</code>","text":"<p>Create a query to subset the table based on the provided parameters.</p> <p>Args:     longitude_column: Name of the column containing longitude values.     latitude_column: Name of the column containing latitude values.     time_column: Name of the column containing time values.     depth_column: Name of the column containing depth values.     columns: List of additional columns to include in the query.     bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).     depth_range: Optional range for depth defined as (min_depth, max_depth).     time_range: Optional range for time defined as (start_time, end_time). Returns     A Query object that can be executed to retrieve the subset of data.</p> Source code in <code>beacon_api/table.py</code> <pre><code>def subset(self, longitude_column: str, latitude_column: str, time_column: str, depth_column: str, columns: list[str],\n                     bbox: Optional[tuple[float, float, float, float]] = None,\n                     depth_range: Optional[tuple[float, float]] = None,\n                     time_range: Optional[tuple[datetime.datetime, datetime.datetime]] = None) -&gt; Query:\n    \"\"\"\n    Create a query to subset the table based on the provided parameters.\n\n    Args:\n        longitude_column: Name of the column containing longitude values.\n        latitude_column: Name of the column containing latitude values.\n        time_column: Name of the column containing time values.\n        depth_column: Name of the column containing depth values.\n        columns: List of additional columns to include in the query.\n        bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).\n        depth_range: Optional range for depth defined as (min_depth, max_depth).\n        time_range: Optional range for time defined as (start_time, end_time).\n    Returns\n        A Query object that can be executed to retrieve the subset of data.\n    \"\"\"\n    query = self.query()\n    query.add_select_column(longitude_column)\n    query.add_select_column(latitude_column)\n    query.add_select_column(time_column)\n    query.add_select_column(depth_column)\n    for column in columns:\n        query.add_select_column(column)\n    if bbox:\n        query.add_filter(AndFilter([\n            RangeFilter(longitude_column, bbox[0], bbox[2]),\n            RangeFilter(latitude_column, bbox[1], bbox[3])\n        ]))\n    if depth_range:\n        query.add_filter(RangeFilter(depth_column, depth_range[0], depth_range[1]))\n    if time_range:\n        query.add_filter(RangeFilter(time_column, time_range[0].strftime(\"%Y-%m-%dT%H:%M:%S\"), time_range[1].strftime(\"%Y-%m-%dT%H:%M:%S\")))\n    return query\n</code></pre>"},{"location":"using/exploring/","title":"Exploring the Beacon Data Lake","text":"<p>In this section, we will explore the capabilities of the Beacon Data Lake by performing some common tasks.</p>"},{"location":"using/exploring/#connecting-to-a-beacon-data-lake","title":"Connecting to a Beacon Data Lake","text":"<p>First, we need to connect to a Beacon Data Lake using the <code>beacon_api</code> package:</p> <pre><code>from beacon_api import Client\nclient = Client(\"https://beacon.example.com\")\n</code></pre>"},{"location":"using/exploring/#listing-available-tables","title":"Listing Available Tables","text":"<p>First, let's list all available tables in the Beacon Data Lake:</p> <pre><code>tables = client.list_tables()\nprint(tables)\n</code></pre>"},{"location":"using/exploring/#viewing-table-schema","title":"Viewing Table Schema","text":"<p>Next, we can view the schema of a specific table to understand its structure and available columns:</p> <pre><code>schema = tables['default'].get_table_schema()\nprint(schema)\n</code></pre>"},{"location":"using/exploring/#querying-data","title":"Querying Data","text":"<p>Finally, we can perform a query on the data:</p> <pre><code>df = (\n    tables['default']\n    .query()\n    .add_select_column(\"LONGITUDE\")\n    .add_select_column(\"LATITUDE\")\n    .add_select_column(\"JULD\")\n    .add_range_filter(\"JULD\", \"2020-01-01T00:00:00\", \"2021-01-01T00:00:00\")\n    .to_pandas_dataframe()\n)\nprint(df)\n</code></pre>"},{"location":"using/querying/","title":"Querying the Beacon Data Lake","text":"<p>In this example, we will demonstrate how to create and execute a query on a specific table in the Beacon Data Lake using the <code>beacon_api</code> package.</p> <p>In this example, we will query the <code>vessels</code> table to retrieve information about vessels with a specific name.</p> <pre><code>from beacon_api import Client\n\nclient = Client(\"https://some-beacon-datalake.com\")\ntables = client.list_tables()\nvessels_table = tables['vessels']\nquery = (\n    vessels_table\n    .query()\n    .add_select_column(\"VESSEL_NAME\")\n    .add_select_column(\"IMO\")\n    .add_select_column(\"CALL_SIGN\")\n    .add_range_filter(\"LENGTH\", 100, 300)\n)\ndf = query.to_pandas_dataframe()\nprint(df)\n</code></pre>"},{"location":"using/querying/#selecting-columns","title":"Selecting Columns","text":"<p>You can select specific columns to include in the query results using the <code>add_select_column</code> method. This helps to limit the amount of data returned and focus on the relevant information. You can also apply an alias to the selected column using the <code>alias</code> parameter. This can be useful for renaming columns in the output.</p> <pre><code>query = (\n    vessels_table\n    .query()\n    .add_select_column(\"VESSEL_NAME\")\n    .add_select_column(\"IMO\", alias=\"IMO_Number\")\n    .add_select_column(\"CALL_SIGN\")\n)\n</code></pre> <p>Optionally, you can also coalesce columns. Coalescing columns allows you to combine multiple columns into one, taking the first non-null value from the specified columns. You can select a coalesced column using the <code>add_select_coalesced</code> method.</p> <pre><code>query = (\n    vessels_table\n    .query()\n    .add_select_coalesced([\"VESSEL_NAME\", \"VESSEL_ALIAS\"], \"OUTPUT_VESSEL_NAME\")\n)\n</code></pre> <p>Warning</p> <p>Coalescing columns can only be done on columns of the same data type. The resulting column will have the same data type as the input columns. If the input columns have different data types, an error will be raised when executing the query. Coalescing can sometimes prevent filters from being pushed down to the file index, which can lead to slower queries.</p>"},{"location":"using/querying/#applying-filters","title":"Applying Filters","text":"<p>You can apply various filters to narrow down the query results. Filters can only be applied to columns that are selected in the query (whenever an alias is used, that alias must be used in the filter). Here are some examples of different types of filters you can use:</p>"},{"location":"using/querying/#range-filters","title":"Range Filters","text":"<pre><code>query = (\n    vessels_table\n    .query()\n    .add_select_column(\"LENGTH\")\n    .add_select_column(\"TIME\", alias=\"TIMESTAMP\")\n    .add_range_filter(\"LENGTH\", 100, 300)\n    .add_range_filter(\"TIMESTAMP\", \"2022-01-01T00:00:00Z\", \"2022-12-31T23:59:59Z\")\n)\n</code></pre>"},{"location":"using/querying/#equalnot-equal-filters","title":"Equal/Not Equal Filters","text":"<pre><code>query = (\n    vessels_table\n    .query()\n    .add_select_column(\"VESSEL_NAME\")\n    .add_select_column(\"IMO\")\n    .add_equals_filter(\"VESSEL_NAME\", \"Some Vessel\")\n    .add_not_equals_filter(\"IMO\", \"1234567\")\n)\n</code></pre>"},{"location":"using/querying/#nullnot-null-filters","title":"Null/Not Null Filters","text":"<pre><code>query = (\n    vessels_table\n    .query()\n    .add_select_column(\"VESSEL_NAME\")\n    .add_select_column(\"IMO\")\n    .add_is_null_filter(\"VESSEL_NAME\")\n    .add_is_not_null_filter(\"IMO\")\n)\n</code></pre>"},{"location":"using/querying/#polygon-filters","title":"Polygon Filters","text":"<pre><code>query = (\n    vessels_table\n    .query()\n    .add_select_column(\"lon\", alias=\"longitude\")\n    .add_select_column(\"lat\", alias=\"latitude\")\n    .add_polygon_filter(\"longitude\", \"latitude\", [(-74.0, 40.7), (-73.9, 40.7), (-73.9, 40.8), (-74.0, 40.8), (-74.0, 40.7)])\n)\n</code></pre>"},{"location":"using/querying/#executing-the-query-output","title":"Executing the Query (Output)","text":"<p>Once you have built your query with the desired select columns and filters, you can execute it and retrieve the results using various 'to_*' methods. The most common method is <code>to_pandas_dataframe</code>, which returns the results as a Pandas DataFrame.</p>"},{"location":"using/querying/#to-pandas-dataframe","title":"To Pandas DataFrame","text":"<pre><code>df = (\n    query\n    .select_column(\"VESSEL_NAME\")\n    .select_column(\"IMO\")\n    .to_pandas_dataframe()\n)\nprint(df)\n</code></pre>"},{"location":"using/querying/#to-geopandas-dataframe","title":"To GeoPandas DataFrame","text":"<p>Note</p> <p>GeoPandas support requires the <code>geopandas</code> extra dependency. You can install it using: <pre><code>pip install beacon_api[geopandas]\n</code></pre></p> <pre><code>gdf = (\n    query\n    .select_column(\"VESSEL_NAME\")\n    .select_column(\"LONGITUDE\")\n    .select_column(\"LATITUDE\")\n    .to_geopandas_dataframe(\"LONGITUDE\", \"LATITUDE\")\n)\nprint(gdf)\n</code></pre>"},{"location":"using/querying/#to-parquet-file","title":"To Parquet File","text":"<pre><code>(\n    query\n    .select_column(\"VESSEL_NAME\")\n    .select_column(\"LONGITUDE\")\n    .select_column(\"LATITUDE\")\n    .to_parquet(\"vessels.parquet\")\n)\n</code></pre>"},{"location":"using/querying/#to-geoparquet-file","title":"To GeoParquet File","text":"<pre><code>(\n    query\n    .select_column(\"VESSEL_NAME\")\n    .select_column(\"LONGITUDE\")\n    .select_column(\"LATITUDE\")\n    .to_geoparquet(\"vessels.geoparquet\", \"LONGITUDE\", \"LATITUDE\")\n)\n</code></pre>"},{"location":"using/querying/#to-arrow-ipc-file","title":"To Arrow Ipc File","text":"<pre><code>(\n    query\n    .select_column(\"VESSEL_NAME\")\n    .select_column(\"LONGITUDE\")\n    .select_column(\"LATITUDE\")\n    .to_arrow(\"vessels.arrow\")\n)\n</code></pre>"},{"location":"using/querying/#to-netcdf-file","title":"To NetCDF File","text":"<pre><code>(\n    query\n    .select_column(\"VESSEL_NAME\")\n    .select_column(\"LONGITUDE\")\n    .select_column(\"LATITUDE\")\n    .to_netcdf(\"vessels.nc\")\n)\n</code></pre>"},{"location":"using/querying/#to-csv-file","title":"To CSV File","text":"<pre><code>(\n    query\n    .select_column(\"VESSEL_NAME\")\n    .select_column(\"LONGITUDE\")\n    .select_column(\"LATITUDE\")\n    .to_csv(\"vessels.csv\")\n)\n</code></pre>"},{"location":"using/querying/#to-zarr-store","title":"To Zarr Store","text":"<p>Note</p> <p>Zarr support requires the <code>zarr</code> extra dependency. You can install it using: <pre><code>pip install beacon_api[zarr]\n</code></pre></p> <pre><code>(\n    query\n    .select_column(\"VESSEL_NAME\")\n    .select_column(\"LONGITUDE\")\n    .select_column(\"LATITUDE\")\n    .to_zarr(\"vessels.zarr\")\n)\n</code></pre>"},{"location":"using/tables/","title":"Data Tables / Data Collections","text":"<p>In the Beacon Data Lake, data is organized into tables (also referred to as data collections). Each table contains a set of columns originating from the datasets stored inside the data lake. You can interact with these tables using the <code>beacon_api</code> package to perform queries and retrieve data. Each table is represented by a <code>Table</code> object in the <code>beacon_api</code> package. You can obtain a <code>Table</code> object by listing the available tables in the connected Beacon Data Lake using the <code>list_tables</code> method of the <code>Client</code> object.</p> <pre><code>tables = client.list_tables()\nprint(tables)  # This will print a dictionary of available tables\n</code></pre> <p>You can access a specific table by its name from the dictionary returned by <code>list_tables</code>. For example, to access a table named <code>vessels</code>, you can do the following:</p> <pre><code>vessels_table = tables['vessels']\nprint(vessels_table)  # This will print the Table object for the 'vessels' table\n</code></pre> <p>Once you have a <code>Table</code> object, you can view its schema (available columns) using the <code>get_table_schema</code> method:</p> <pre><code>schema = vessels_table.get_table_schema()\nprint(schema)  # This will print the schema of the 'vessels' table\n</code></pre> <p>You can also create and execute queries on the table using the <code>query</code> method of the <code>Table</code> object. This will return a <code>Query</code> object that you can use to build and execute your query.</p> <pre><code>query = vessels_table.query()\n# You can then build your query using the Query methods\n</code></pre>"}]}