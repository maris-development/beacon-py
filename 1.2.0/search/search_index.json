{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Beacon API Python SDK","text":"<p>The Beacon Python SDK is a client for Beacon Data Lakes. It discovers datasets, inspects schemas, and ships a composable query builder that can stream results into familiar scientific Python tools.</p>"},{"location":"#why-use-the-sdk","title":"Why use the SDK?","text":"<ul> <li>One client for every Beacon Node \u2013 the <code>Client</code> wires authentication headers, probes <code>/api/health</code>, and exposes helpers such as <code>list_tables()</code> and <code>list_datasets()</code>.</li> <li>Discoverability built-in \u2013 <code>DataTable</code> and <code>Dataset</code> helpers return Arrow schemas, table descriptions, and file metadata so you always know which columns exist before writing a query.</li> <li>Chainable JSON/SQL query builders \u2013 start from a table or dataset, add selects, filters, distinct clauses, or geospatial predicates, and export to Pandas, GeoPandas, Dask, xarray, or on-disk formats such as (Geo)Parquet, Arrow IPC, NetCDF, NdNetCDF, CSV, Zarr, or ODV.</li> <li>Typed, well-documented API surface \u2013 the docs you are reading mirror the public classes (<code>Client</code>, <code>DataTable</code>, <code>Dataset</code>, <code>JSONQuery</code>, <code>SQLQuery</code>, \u2026) so editors and notebooks surface the same guidance.</li> </ul> <p>Beacon Data Lake platform</p> <p>Looking for the platform documentation itself? Head over to the Beacon Data Lake docs.</p>"},{"location":"#quick-start","title":"Quick start","text":"<pre><code>from beacon_api import Client\n\nclient = Client(\n    \"https://beacon.example.com\",\n    jwt_token=\"&lt;optional bearer token&gt;\",\n)\n\nclient.check_status()  # probes /api/health and prints the Beacon version\n\ntables = client.list_tables()\nstations = tables[\"default\"]\n\ndf = (\n    stations\n    .query()\n    .add_select_columns([\n        (\"LONGITUDE\", None),\n        (\"LATITUDE\", None),\n        (\"JULD\", None),\n        (\"TEMP\", \"temperature_c\"),\n    ])\n    .add_range_filter(\"JULD\", \"2024-01-01T00:00:00\", \"2024-12-31T23:59:59\")\n    .to_pandas_dataframe()\n)\n</code></pre>"},{"location":"#core-concepts","title":"Core concepts","text":""},{"location":"#client","title":"Client","text":"<p>Manages the HTTP session, authentication headers, and compatibility checks. Use <code>get_server_info()</code> to inspect the Beacon version and extensions, <code>list_tables()</code> to enumerate logical collections, or <code>list_datasets()</code> (Beacon \u2265 1.4.0) to work with direct file paths.</p>"},{"location":"#tables-and-datasets","title":"Tables and datasets","text":"<p>Tables (instances of <code>DataTable</code>) represent logical collections backed by one or more datasets. They expose helpers such as <code>get_table_schema()</code> or <code>subset()</code> to quickly explore spatial/temporal windows. Datasets expose similar helpers but start from a known file/URI and can construct a query via <code>Dataset.query()</code>.</p>"},{"location":"#query-builders","title":"Query builders","text":"<p><code>JSONQuery</code> powers the fluent builder: select columns, coalesce values, add range/equality/geospatial filters, sorting, distinct clauses, or call <code>explain()</code> to see the server-side plan. Prefer <code>Client.sql_query()</code> when you already have SQL.</p>"},{"location":"#rich-outputs","title":"Rich outputs","text":"<p>Every query inherits the <code>BaseQuery</code> output helpers. Stream into <code>to_pandas_dataframe()</code>, <code>to_geo_pandas_dataframe()</code>, <code>to_dask_dataframe()</code>, <code>to_xarray_dataset()</code>, or write datasets using <code>to_parquet()</code>, <code>to_nd_netcdf()</code>, <code>to_zarr()</code>, <code>to_odv()</code> and more.</p>"},{"location":"#where-to-next","title":"Where to next?","text":"<ul> <li>Installation \u2013 supported Python versions and optional extras.</li> <li>Getting started \u2013 end-to-end walkthrough from connecting a client to exporting a DataFrame.</li> <li>Using Beacon \u2013 deep dives into tables, datasets, and the JSON query builder.</li> <li>Working with datasets \u2013 jump straight from file paths/URIs to a query builder.</li> <li>API reference \u2013 auto-generated API docs straight from the SDK source.</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"changelog/#120-2026-01-14","title":"[1.2.0] - 2026-01-14","text":""},{"location":"changelog/#breaking-changes","title":"Breaking changes","text":"<ul> <li>Query streaming now returns a <code>pyarrow.RecordBatchStreamReader</code> from <code>Query.execute_streaming()</code> instead of yielding individual <code>RecordBatch</code> objects. This allows users to manage the stream lifecycle directly and integrate with Arrow's native reading/writing utilities.</li> </ul>"},{"location":"changelog/#1100-2025-12-07","title":"1.10.0 - 2025-12-07","text":""},{"location":"changelog/#breaking-changes_1","title":"Breaking changes","text":"<ul> <li>Raised the minimum supported Python version from 3.8 to 3.10 and promoted several previously-optional dependencies (<code>fsspec</code>, <code>dask</code>, <code>zarr</code>, <code>networkx</code>, <code>matplotlib</code>, <code>numpy</code>, <code>geopandas</code>) to core requirements. Lightweight environments may need to be recreated before upgrading.</li> <li>Reworked the query entry points to be table/dataset-first. <code>Client.list_tables()</code> now returns <code>DataTable</code> helpers, <code>Client.list_datasets()</code> mirrors that experience for raw files, and the legacy <code>Client.query()</code>/<code>Client.available_columns*()</code> helpers have been deprecated in favor of the richer table and dataset APIs.</li> </ul>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Dataset-aware workflows. <code>Client.list_datasets()</code> (Beacon \u2265 1.4.0) now surfaces every server-side dataset as a typed <code>Dataset</code> helper that can: fetch a <code>pyarrow.Schema</code>, expose metadata (<code>get_file_name()</code>, <code>get_file_format()</code>), and produce a JSON query builder via <code>.query()</code>. CSV and Zarr datasets accept format-specific options such as custom delimiters or statistics columns directly on the query call.</li> <li>Beacon node management helpers. Administrative operations\u2014including <code>upload_dataset()</code>, <code>download_dataset()</code>, <code>delete_dataset()</code>, <code>create_logical_table()</code> and <code>delete_table()</code>\u2014were added to <code>Client</code>. Each helper enforces <code>BaseBeaconSession.is_admin()</code> and server version gates so automation scripts can manage Beacon nodes safely.</li> <li>Modular JSON query builder. The monolithic <code>beacon_api.query</code> module has been replaced by a node-based package (e.g. <code>_from</code>, <code>select</code>, <code>filter</code>, <code>distinct</code>, <code>sort</code>, <code>functions</code>). This unlocks fluent helpers such as <code>add_select_column</code>, <code>add_select_coalesced</code>, <code>add_polygon_filter</code>, <code>set_distinct</code>, <code>add_sort</code>, and new function nodes (<code>Functions.concat</code>, <code>Functions.coalesce</code>, <code>Functions.try_cast_to_type</code>, <code>Functions.map_pressure_to_depth</code>, etc.) for assembling complex projections.</li> <li>Geospatial and scientific outputs. <code>BaseQuery</code> can now stream Arrow record batches (<code>execute_streaming</code>) and materialize results as GeoParquet, GeoPandas, NdNetCDF, NetCDF, Arrow, CSV, Parquet, Zarr, Ocean Data View exports, or directly into an xarray dataset. The helpers write responses chunk-by-chunk to disk to avoid loading full payloads into memory.</li> <li>Documentation and site tooling. The MkDocs configuration now ships topical guides under \u201cUsing the Data Lake\u201d (Exploring, Querying, Tables, Datasets), API references powered by <code>mkdocstrings</code>, versioned docs via <code>mike</code>, and an example gallery (e.g. the World Ocean Database walkthrough) that mirrors the new SDK surface area.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li><code>BaseBeaconSession</code> now detects the Beacon server version on construction, exposes <code>version_at_least()</code>, and checks admin capabilities with <code>is_admin()</code>. Higher-level helpers automatically guard experimental endpoints (datasets, logical tables, streaming outputs) behind these checks.</li> <li><code>DataTable</code> introspection now fetches Arrow schemas through <code>/api/table-schema</code>, exposing precise field types for downstream tooling. The <code>subset()</code> helper applies the new dataclass-based filter nodes so you can reuse bounding-box, depth, and time filters elsewhere.</li> <li>Query materialization helpers such as <code>to_parquet</code>, <code>to_csv</code>, <code>to_arrow</code>, and <code>to_geoparquet</code> now stream response chunks to disk rather than buffering entire files in memory, improving stability on large exports.</li> <li>Documentation content was rewritten to align with the new APIs\u2014<code>docs/getting_started.md</code>, <code>docs/using/*.md</code>, and the reference pages now showcase dataset-first queries, polygon filters, geospatial exports, and SQL parity.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Eliminated runaway memory usage during large exports by switching every file writer to <code>response.iter_content()</code> streaming.</li> <li>Hardened dataset/table schema parsing: unsupported Beacon field types now trigger explicit exceptions, while timestamp formats are automatically mapped to the correct <code>pyarrow</code> timestamp resolution.</li> </ul>"},{"location":"getting_started/","title":"Getting started","text":"<p>This walkthrough mirrors the public API exposed by the SDK so you can go from zero to a working query in minutes.</p>"},{"location":"getting_started/#1-install-and-import","title":"1. Install and import","text":"<p>Beacon API supports Python 3.10+. Install it from PyPI and import the pieces you need:</p> <pre><code>pip install beacon-api\n</code></pre> <pre><code>from beacon_api import Client\n</code></pre> <p>Editor support</p> <p>The project ships typed stubs (<code>py.typed</code>) so VS Code, PyCharm, or notebooks provide signature help and inline documentation out of the box.</p>"},{"location":"getting_started/#2-create-a-client","title":"2. Create a client","text":"<p>Instantiate <code>Client</code> with the Beacon base URL and (optionally) headers for authentication. The constructor normalizes headers, sets JSON defaults, and validates connectivity by calling <code>/api/health</code>.</p> <pre><code>client = Client(\n    \"https://beacon.example.com\",\n    # jwt_token=\"&lt;optional bearer token&gt;\",\n    # proxy_headers={\"X-Forwarded-For\": \"&lt;ip&gt;\"},\n    # basic_auth=(\"user\", \"pass\") is also supported\n)\n</code></pre> <p>Use <code>client.check_status()</code> to verify connectivity and print the Beacon version, or <code>client.get_server_info()</code> to inspect the metadata returned by <code>/api/info</code>.</p>"},{"location":"getting_started/#3-discover-tables-and-datasets","title":"3. Discover tables and datasets","text":"<p><code>list_tables()</code> returns a mapping of table names to <code>DataTable</code> helpers that already know their description and type.</p> <pre><code>tables = client.list_tables()\nstations = tables[\"stations-collection\"]\n\nprint(stations.get_table_description())\nschema = stations.get_table_schema()        # Arrow schema with pyarrow fields\nschema_arrow = stations.get_table_schema_arrow()\n</code></pre> <p>If your Beacon Node is running v1.4.0 or later, use <code>list_datasets()</code> to enumerate file-backed resources and derive a query directly from a <code>Dataset</code>:</p> <pre><code>datasets = client.list_datasets(pattern=\"*.parquet\", limit=5)\nfile = datasets[\"wod/2024-01.parquet\"]\n\nprint(file.get_file_format(), file.get_file_name())\ndataset_schema = file.get_schema()\ndataset_query = file.query()\n</code></pre> <p>Deprecation notice</p> <p><code>Client.query()</code> and <code>Client.subset()</code> are still available for backwards compatibility but emit deprecation warnings. Prefer starting from a table (<code>tables[\"default\"].query()</code>) or dataset (<code>file.query()</code>).</p>"},{"location":"getting_started/#4-build-a-json-query","title":"4. Build a JSON query","text":"<p>All table and dataset helpers return a <code>JSONQuery</code>, a fluent builder with chainable selects and filters:</p> <pre><code>df = (\n    tables['argo'] # Select the 'default' table as our data source\n    .query() # Create a new query on the selected table\n    .add_select_column(\"LONGITUDE\") # Select the LONGITUDE column\n    .add_select_column(\"LATITUDE\") # Select the LATITUDE column\n    .add_select_column(\"JULD\")\n    .add_select_column(\"PRES\")\n    .add_select_column(\"TEMP\")\n    .add_select_column(\"PSAL\")\n    .add_select_column(\".featureType\") # Select the .featureType column\n    .add_select_column(\"DATA_TYPE\")\n    .add_range_filter(\"JULD\", \"2020-01-01T00:00:00\", \"2021-01-01T00:00:00\") # Filter for JULD between 2020 and 2021 for the column JULD\n    .add_range_filter(\"PRES\", 0, 10) # Filter for pressure between 0 and 10 dbar for the column PRES\n    .to_pandas_dataframe() # Execute the query and return the results as a Pandas DataFrame\n)\ndf\n</code></pre> <pre><code>from datetime import datetime\n\nquery = (\n    stations\n    .query()\n    .add_select_columns([\n        (\"LONGITUDE\", None),\n        (\"LATITUDE\", None),\n        (\"JULD\", None),\n        (\"TEMP\", \"temperature_c\"),\n        (\"PSAL\", \"salinity\"),\n    ])\n    .add_select_coalesced([\"SEA_NAME\", \"BASIN\"], alias=\"water_body\")\n    .add_range_filter(\"JULD\", datetime(2024, 1, 1), datetime(2024, 6, 1))\n    .add_range_filter(\"PRES\", 0, 10)\n    .add_polygon_filter(\n        longitude_column=\"LONGITUDE\",\n        latitude_column=\"LATITUDE\",\n        polygon=[(-5.2, 52.0), (-5.2, 52.5), (-4.2, 52.5), (-4.2, 52.0), (-5.2, 52.0)],\n    )\n)\n</code></pre> <p>Need a quick spatial/temporal subset without writing filters manually? <code>DataTable.subset()</code> wraps the same builder and automatically selects longitude/latitude/depth/time columns.</p> <pre><code>subset_query = stations.subset(\n    longitude_column=\"LONGITUDE\",\n    latitude_column=\"LATITUDE\",\n    time_column=\"JULD\",\n    depth_column=\"PRES\",\n    columns=[\"TEMP\", \"PSAL\"],\n    bbox=(-20, 40, -10, 50),\n    depth_range=(0, 50),\n)\n</code></pre>"},{"location":"getting_started/#5-execute-the-query","title":"5. Execute the query","text":"<p>Every query inherits rich output helpers from <code>BaseQuery</code>:</p> <pre><code>query = tables['argo'].query()\n\n... # build up the query as shown above\n\n# Serialize results into various formats\ndf = query.to_pandas_dataframe()\ngdf = query.to_geo_pandas_dataframe(\"LONGITUDE\", \"LATITUDE\")\n\nquery.to_parquet(\"subset.parquet\")\nquery.to_geoparquet(\"subset.geoparquet\", \"LONGITUDE\", \"LATITUDE\")\nquery.to_netcdf(\"subset.nc\")\nquery.to_nd_netcdf(\"subset_nd.nc\", dimension_columns=[\"LONGITUDE\", \"LATITUDE\", \"JULD\"])\nquery.to_zarr(\"subset.zarr\")\n</code></pre> <p>Beacon compatibility</p> <p><code>to_nd_netcdf</code> requires Beacon Node v1.5.0 or newer.</p> <p>Need lazy/out-of-core execution? Use <code>to_dask_dataframe()</code> or <code>to_xarray_dataset()</code> with chunking, or call <code>to_dask_dataframe().head()</code> for quick inspection.</p> <p>Profiling and explain</p> <p>Call <code>query.explain()</code> to retrieve the Beacon execution plan, or <code>query.execute()</code> to inspect the raw HTTP response.</p>"},{"location":"getting_started/#6-running-sql-directly","title":"6. Running SQL directly","text":"<p>When you already have SQL, skip the builder and call:</p> <pre><code>sql = client.sql_query(\"\"\"\n    SELECT lon, lat, juld, temperature\n    FROM &lt;some-collection-name&gt;\n    WHERE juld BETWEEN '2024-01-01T00:00:00' AND '2024-06-30T23:59:59'\n\"\"\")\n\ndf = sql.to_pandas_dataframe()\nprint(df)\n</code></pre>"},{"location":"getting_started/#next-steps","title":"Next steps","text":"<ul> <li>Deep dive into exploring datasets and tables.</li> <li>Learn the fluent APIs in Querying the Beacon Data Lake.</li> <li>Browse the auto-generated API reference.</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Install the published wheel straight from PyPI:</p> <pre><code>pip install beacon-api\n</code></pre> <p>Upgrade PyArrow if you are on 19.0.0</p> <p>PyArrow 19.0.0 shipped with a known regression in the Parquet reader. The SDK already pins <code>pyarrow&gt;=17.0.0,!=19.0.0</code>, but if you installed PyArrow separately make sure you are on \u226519.0.1.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or newer</li> <li>Internet access to reach your Beacon Node</li> </ul> <p>Check your Python version with:</p> <pre><code>python --version\n</code></pre>"},{"location":"installation/#what-gets-installed","title":"What gets installed?","text":"<p>The default installation already brings along: <code>pandas</code>, <code>pyarrow</code>, <code>xarray</code>, <code>dask</code>, <code>fsspec</code>, <code>geopandas</code>, <code>zarr</code>, and <code>netCDF4</code>. That means features such as <code>to_geo_pandas_dataframe</code>, <code>to_zarr</code>, or <code>to_nd_netcdf</code> work out of the box\u2014no optional extras required.</p>"},{"location":"installation/#upgrading","title":"Upgrading","text":"<pre><code>pip install --upgrade beacon-api\n</code></pre> <p>You can combine this with <code>pipx runpip</code> or environment managers (Conda, uv, Rye, \u2026) if you prefer isolation.</p>"},{"location":"examples/wod/","title":"World Ocean Database Example","text":"<p>The World Ocean Database (WOD) is a comprehensive collection of oceanographic data, including temperature, salinity, oxygen, and other parameters. This example demonstrates how to use the <code>beacon_api</code> package to query and retrieve data from a Beacon Data Lake that hosts the WOD dataset. The Beacon Data Lake contains around 20 million netCDF files stored into various Beacon Binary Format files (think of a zip containing multiple netcdf files), which are organized into tables for efficient querying.</p>"},{"location":"examples/wod/#connecting-to-the-beacon-wod-data-lake","title":"Connecting to the Beacon WOD Data Lake","text":"<pre><code>from beacon_api import Client\n\nclient = Client(\"https://beacon-wod.maris.nl\")\ntables = client.list_tables()\nwod_table = tables['default']\n</code></pre>"},{"location":"examples/wod/#viewing-table-schema","title":"Viewing Table Schema","text":"<pre><code>schema = wod_table.get_table_schema()\nprint(schema)\n</code></pre>"},{"location":"examples/wod/#querying-data","title":"Querying Data","text":"<pre><code>df = (\n    wod_table\n    .query()\n    .add_select_column(\"lon\", alias=\"longitude\")\n    .add_select_column(\"lat\", alias=\"latitude\")\n    .add_select_column(\"z\", alias=\"depth\")\n    .add_select_column(\"time\")\n    .add_select_column(\"Temperature\")\n    .add_select_column(\"Salinity\")\n    .add_range_filter(\"time\", \"2020-01-01T00:00:00\", \"2021-01-01T00:00:00\")\n    .to_pandas_dataframe()\n)\nprint(df)\n</code></pre>"},{"location":"reference/client/","title":"Client reference","text":"<p>A <code>Client</code> provides a connection to a Beacon Node, manages authentication headers and exposes helpers for discovering tables/datasets before building JSON or SQL queries.</p> Source code in <code>beacon_api/client.py</code> <pre><code>class Client:\n    \"\"\"\n    A ``Client`` provides a connection to a Beacon Node, manages authentication headers and exposes helpers for\n    discovering tables/datasets before building JSON or SQL queries.\n    \"\"\"\n\n    def __init__(self, url: str, proxy_headers: dict[str,str] | None = None, jwt_token: str | None = None, basic_auth: tuple[str, str] | None = None):\n        \"\"\"Create a Beacon API client.\n\n        Args:\n            url: Base Beacon Node URL, e.g. ``\"https://beacon-node.example.com\"``.\n            proxy_headers: Optional custom headers added to every request.\n            jwt_token: Optional bearer token used for ``Authorization`` header.\n            basic_auth: Optional ``(username, password)`` tuple for HTTP basic auth.\n\n        Raises:\n            ValueError: If ``basic_auth`` is not a 2-item tuple.\n            Exception: When the Beacon Node health endpoint cannot be reached.\n        \"\"\"\n        if proxy_headers is None:\n            proxy_headers = {}\n        # Set JSON headers\n        proxy_headers['Content-Type'] = 'application/json'\n        proxy_headers['Accept'] = 'application/json'\n        if jwt_token:\n            proxy_headers['Authorization'] = f'Bearer {jwt_token}'\n\n        if basic_auth:\n            if not isinstance(basic_auth, tuple) or len(basic_auth) != 2:\n                raise ValueError(\"Basic auth must be a tuple of (username, password)\")\n            proxy_headers['Authorization'] = f'{requests.auth._basic_auth_str(*basic_auth)}' # type: ignore\n\n        self.session = BaseBeaconSession(url, proxy_headers=proxy_headers)\n\n        if self.check_status():\n            raise Exception(\"Failed to connect to server\")\n\n    def check_status(self):\n        \"\"\"Verify that the Beacon Node responds to ``/api/health``.\n\n        Returns:\n            None. The method prints diagnostic information on success.\n\n        Raises:\n            Exception: If the Beacon Node returns a non-200 response.\n        \"\"\"\n        response = self.session.get(\"api/health\")\n        if response.status_code != 200:\n            raise Exception(f\"Failed to connect to server: {response.text}\")\n        else:\n            print(\"Connected to: {} server successfully\".format(self.session.base_url))\n            print(\"Beacon Version:\", self.get_server_info()['beacon_version'])\n\n    def get_server_info(self) -&gt; dict:\n        \"\"\"Return the server metadata exposed by ``/api/info``.\"\"\"\n        response = self.session.get(\"/api/info\")\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get server info: {response.text}\")\n        return response.json()\n\n    @deprecated(version=\"1.1.0\",reason=\"Use list_tables() to get available tables. From there you can find the available columns for each table. This method will be removed in future versions.\")\n    def available_columns(self) -&gt; list[str]:\n        \"\"\"Return column names for the default table (deprecated).\"\"\"\n        response = self.session.get(\"/api/query/available-columns\")\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get columns: {response.text}\")\n        columns = response.json()\n        return columns\n\n    @deprecated(version=\"1.1.0\",reason=\"Use list_tables() to get available tables. From there you can find the available columns and their data types for each table. This method will be removed in future versions.\")\n    def available_columns_with_data_type(self) -&gt; dict[str, type]:\n        tables = self.list_tables()\n        if 'default' not in tables:\n            raise Exception(\"No default table found\")\n        table = tables['default']\n        return table.get_table_schema()\n\n    def list_tables(self) -&gt; dict[str,DataTable]:\n        \"\"\"Retrieve all logical tables available on the Beacon Node.\n\n        Returns:\n            dict[str, DataTable]: Mapping of table name to :class:`DataTable` helper.\n        \"\"\"\n        response = self.session.get(\"/api/tables\")\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get tables: {response.text}\")\n        tables = response.json()\n\n        data_tables = {}\n        for table in tables:\n            data_tables[table] = DataTable(\n                http_session=self.session,\n                table_name=table,\n            )\n\n        return data_tables\n\n    def list_datasets(self, pattern: str | None = None, limit : int | None = None, offset: int | None = None, force=False) -&gt; dict[str, Dataset]:\n        \"\"\"Enumerate datasets registered with the Beacon node.\n\n        Args:\n            pattern: Optional glob-like filter applied by the server.\n            limit: Optional page size to cap the number of results.\n            offset: Optional offset for pagination.\n\n        Returns:\n            dict[str, Dataset]: Mapping from file path to :class:`Dataset` helper.\n\n        Raises:\n            Exception: If the Beacon Node version &lt; 1.4.0 or the HTTP call fails.\n        \"\"\"\n\n        if not force and not self.session.version_at_least(1,4,0):\n            raise Exception(\"Listing datasets requires Beacon server version 1.4.0 or higher\")\n\n        response = self.session.get(\"/api/list-datasets\", params={\n            \"pattern\": pattern,\n            \"limit\": limit,\n            \"offset\": offset\n        })\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get datasets: {response.text}\")\n        datasets = response.json()\n        dataset_objects = {}\n        for dataset in datasets:\n            file_path = dataset['file_path']\n            file_format = dataset['format']\n\n            dataset_objects[file_path] = Dataset(\n                http_session=self.session,\n                file_path=file_path,\n                file_format=file_format\n            )\n        return dataset_objects\n\n    def sql_query(self, sql: str) -&gt; SQLQuery:\n        \"\"\"Create a new :class:`SQLQuery` for direct SQL execution.\n\n        Args:\n            sql: Raw SQL string sent to the Beacon Node.\n\n        Returns:\n            SQLQuery: Builder that exposes ``to_dataframe``, ``to_csv`` etc.\n        \"\"\"\n        return SQLQuery(http_session=self.session, query=sql)\n\n    @deprecated(\"To query, use list_tables() or list_datasets() as a base to create a new query object. This method will be removed in future versions.\")\n    def query(self) -&gt; JSONQuery:\n        \"\"\"Create a new query object. \n        This is the starting point for building a query.\n        The query can then be built using the methods on the Query object.\n        You can also create a query from a specific table from the list_tables() method.\n\n        To materialize and run the query, use the .to_dataframe() or .to_csv() methods on the Query object.\n        Returns:\n            JSONQuery: A new query object.\n        \"\"\"\n\n        # Get the default table and use it as the from source\n        response = self.session.get(\"/api/default-table\")  # Ensure default table exists\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get default table: {response.text}\")\n        default_table = response.json()\n        return JSONQuery(http_session=self.session, _from=FromTable(default_table))\n\n    @deprecated(\"Use the subset method on the DataTable object obtained from list_tables(). This method will be removed in future versions.\")\n    def subset(self, longitude_column: str, latitude_column: str, time_column: str, depth_column: str, columns: list[str],\n                         bbox: Optional[tuple[float, float, float, float]] = None,\n                         depth_range: Optional[tuple[float, float]] = None,\n                         time_range: Optional[tuple[datetime.datetime, datetime.datetime]] = None) -&gt; JSONQuery:\n        \"\"\"\n        Create a query to subset the default collection based on the provided parameters.\n\n        Args:\n            longitude_column: Name of the column containing longitude values.\n            latitude_column: Name of the column containing latitude values.\n            time_column: Name of the column containing time values.\n            depth_column: Name of the column containing depth values.\n            columns: List of additional columns to include in the query.\n            bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).\n            depth_range: Optional range for depth defined as (min_depth, max_depth).\n            time_range: Optional range for time defined as (start_time, end_time).\n        Returns\n            JSONQuery: Query object that can be executed to retrieve the subset.\n        \"\"\"\n        response = self.session.get(\"/api/default-table\")  # Ensure default table exists\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get default table: {response.text}\")\n        default_table = response.json()\n\n        table = self.list_tables()[default_table]\n        return table.subset(\n            longitude_column=longitude_column,\n            latitude_column=latitude_column,\n            time_column=time_column,\n            depth_column=depth_column,\n            columns=columns,\n            bbox=bbox,\n            depth_range=depth_range,\n            time_range=time_range\n        )\n\n    def upload_dataset(self, file_path: str, destination_path: str, force=False) -&gt; None:\n        \"\"\"Upload a local dataset file to the Beacon Node.\n\n        Args:\n            file_path: Path to the local file to upload.\n            destination_path: Destination path on the Beacon Node.\n\n        Raises:\n            Exception: If the upload fails.\n        \"\"\"\n\n        # Require Beacon server version &gt;= 1.5.0\n        if not force and not self.session.version_at_least(1,5,0):\n            raise Exception(\"Uploading datasets requires Beacon server version 1.5.0 or higher\")\n\n        # Requires admin privileges\n        if not self.session.is_admin():\n            raise Exception(\"Uploading datasets requires admin privileges\")\n\n        # We use requests directly here to support multipart/form-data \n        auth_headers = {\n            \"Authorization\": self.session.headers.get(\"Authorization\", \"\")\n        }\n        url = f\"{self.session.base_url}/api/admin/upload-file\"\n\n        # Upload the file using a multipart/form-data POST request\n        # the first part should be the prefix of the destination path eg. /data/datasets/ of /data/datasets/myfile.parquet\n        prefix = '/'.join(destination_path.split('/')[:-1])\n        # File name is the last part\n        file_name = destination_path.split('/')[-1]\n        payload = {'prefix': prefix }\n        files=[\n            ('file',(file_name,open(file_path,'rb'),'application/octet-stream'))\n        ]\n        response = requests.request(\"POST\", url, headers=auth_headers, data=payload, files=files)\n        if response.status_code != 200:\n            raise Exception(f\"Failed to upload dataset: {response.text}\")        \n\n\n    def download_dataset(self, dataset_path: str, local_path: str, force=False) -&gt; None:\n        \"\"\"Download a dataset file from the Beacon Node to a local path.\n\n        Args:\n            dataset_path: Path to the dataset file on the Beacon Node.\n            local_path: Local path where the file will be saved.\n        Raises:\n            Exception: If the download fails.\n        \"\"\"\n        # Require Beacon server version &gt;= 1.5.0\n        if not force and not self.session.version_at_least(1,5,0):\n            raise Exception(\"Downloading datasets requires Beacon server version 1.5.0 or higher\")\n\n        # Requires admin privileges\n        if not self.session.is_admin():\n            raise Exception(\"Downloading datasets requires admin privileges\")\n\n        response = self.session.get(\"/api/admin/download-file\", params={\n            \"file_path\": dataset_path\n        }, stream=True)\n        if response.status_code != 200:\n            raise Exception(f\"Failed to download dataset: {response.text}\")\n\n        with open(local_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n\n    def delete_dataset(self, dataset_path: str, force=False) -&gt; None:\n        \"\"\"Delete a dataset file from the Beacon Node.\n\n        Args:\n            dataset_path: Path to the dataset file on the Beacon Node.\n\n        Raises:\n            Exception: If the deletion fails.\n        \"\"\"\n\n        # Require Beacon server version &gt;= 1.5.0\n        if not force and not self.session.version_at_least(1,5,0):\n            raise Exception(\"Deleting datasets requires Beacon server version 1.5.0 or higher\")\n\n        # Requires admin privileges\n        if not self.session.is_admin():\n            raise Exception(\"Deleting datasets requires admin privileges\")\n\n        response = self.session.delete(\"/api/admin/delete-file\", params={\n            \"file_path\": dataset_path\n        })\n        if response.status_code != 200:\n            raise Exception(f\"Failed to delete dataset: {response.text}\")\n\n    def create_logical_table(self, table_name: str, dataset_glob_paths: list[str], file_format: str, description: str | None = None, force=False, **kwargs) -&gt; None:\n        \"\"\"Create a new logical table on the Beacon Node.\n\n        Args:\n            table_name: Name of the new logical table.\n            dataset_glob_paths: List of dataset file paths (can include glob patterns).\n            file_format: Format of the dataset files (e.g. \"parquet\", \"csv\", \"zarr\").\n            description: Optional description of the logical table.\n            **kwargs: Additional parameters specific to the file format. (e.g. delimiter for CSV, Zarr statistics columns)\n\n        Raises:\n            Exception: If the creation fails.\n        \"\"\"\n\n        # Require Beacon server version &gt;= 1.4.0\n        if not force and not self.session.version_at_least(1,4,0):\n            raise Exception(\"Creating logical tables requires Beacon server version 1.4.0 or higher\")\n\n        # Requires admin privileges\n        if not self.session.is_admin():\n            raise Exception(\"Creating logical tables requires admin privileges\")\n\n        json_data = {\n            \"table_name\": table_name,\n            \"table_type\": {\n                \"logical\": {\n                    \"glob_paths\": dataset_glob_paths,\n                    \"file_format\": file_format,\n                    **kwargs\n                }\n            },\n            \"description\": description\n        }\n\n        response = self.session.post(\"/api/admin/create-table\", json=json_data)\n        if response.status_code != 200:\n            raise Exception(f\"Failed to create table: {response.text}\")\n\n    def delete_table(self, table_name: str, force=False) -&gt; None:\n        \"\"\"Delete a logical table from the Beacon Node.\n\n        Args:\n            table_name: Name of the logical table to delete.\n        Raises:\n            Exception: If the deletion fails.\n        \"\"\"\n\n        # Require Beacon server version &gt;= 1.4.0\n        if not force and not self.session.version_at_least(1,4,0):\n            raise Exception(\"Deleting logical tables requires Beacon server version 1.4.0 or higher\")\n\n        # Requires admin privileges\n        if not self.session.is_admin():\n            raise Exception(\"Deleting logical tables requires admin privileges\")\n\n        response = self.session.delete(\"/api/admin/delete-table\", params={\n            \"table_name\": table_name\n        })\n        if response.status_code != 200:\n            raise Exception(f\"Failed to delete table: {response.text}\")\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.__init__","title":"<code>__init__(url, proxy_headers=None, jwt_token=None, basic_auth=None)</code>","text":"<p>Create a Beacon API client.</p> <p>Args:     url: Base Beacon Node URL, e.g. <code>\"https://beacon-node.example.com\"</code>.     proxy_headers: Optional custom headers added to every request.     jwt_token: Optional bearer token used for <code>Authorization</code> header.     basic_auth: Optional <code>(username, password)</code> tuple for HTTP basic auth.</p> <p>Raises:     ValueError: If <code>basic_auth</code> is not a 2-item tuple.     Exception: When the Beacon Node health endpoint cannot be reached.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def __init__(self, url: str, proxy_headers: dict[str,str] | None = None, jwt_token: str | None = None, basic_auth: tuple[str, str] | None = None):\n    \"\"\"Create a Beacon API client.\n\n    Args:\n        url: Base Beacon Node URL, e.g. ``\"https://beacon-node.example.com\"``.\n        proxy_headers: Optional custom headers added to every request.\n        jwt_token: Optional bearer token used for ``Authorization`` header.\n        basic_auth: Optional ``(username, password)`` tuple for HTTP basic auth.\n\n    Raises:\n        ValueError: If ``basic_auth`` is not a 2-item tuple.\n        Exception: When the Beacon Node health endpoint cannot be reached.\n    \"\"\"\n    if proxy_headers is None:\n        proxy_headers = {}\n    # Set JSON headers\n    proxy_headers['Content-Type'] = 'application/json'\n    proxy_headers['Accept'] = 'application/json'\n    if jwt_token:\n        proxy_headers['Authorization'] = f'Bearer {jwt_token}'\n\n    if basic_auth:\n        if not isinstance(basic_auth, tuple) or len(basic_auth) != 2:\n            raise ValueError(\"Basic auth must be a tuple of (username, password)\")\n        proxy_headers['Authorization'] = f'{requests.auth._basic_auth_str(*basic_auth)}' # type: ignore\n\n    self.session = BaseBeaconSession(url, proxy_headers=proxy_headers)\n\n    if self.check_status():\n        raise Exception(\"Failed to connect to server\")\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.available_columns","title":"<code>available_columns()</code>","text":"<p>Return column names for the default table (deprecated).</p> Source code in <code>beacon_api/client.py</code> <pre><code>@deprecated(version=\"1.1.0\",reason=\"Use list_tables() to get available tables. From there you can find the available columns for each table. This method will be removed in future versions.\")\ndef available_columns(self) -&gt; list[str]:\n    \"\"\"Return column names for the default table (deprecated).\"\"\"\n    response = self.session.get(\"/api/query/available-columns\")\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get columns: {response.text}\")\n    columns = response.json()\n    return columns\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.check_status","title":"<code>check_status()</code>","text":"<p>Verify that the Beacon Node responds to <code>/api/health</code>.</p> <p>Returns:     None. The method prints diagnostic information on success.</p> <p>Raises:     Exception: If the Beacon Node returns a non-200 response.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def check_status(self):\n    \"\"\"Verify that the Beacon Node responds to ``/api/health``.\n\n    Returns:\n        None. The method prints diagnostic information on success.\n\n    Raises:\n        Exception: If the Beacon Node returns a non-200 response.\n    \"\"\"\n    response = self.session.get(\"api/health\")\n    if response.status_code != 200:\n        raise Exception(f\"Failed to connect to server: {response.text}\")\n    else:\n        print(\"Connected to: {} server successfully\".format(self.session.base_url))\n        print(\"Beacon Version:\", self.get_server_info()['beacon_version'])\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.create_logical_table","title":"<code>create_logical_table(table_name, dataset_glob_paths, file_format, description=None, force=False, **kwargs)</code>","text":"<p>Create a new logical table on the Beacon Node.</p> <p>Args:     table_name: Name of the new logical table.     dataset_glob_paths: List of dataset file paths (can include glob patterns).     file_format: Format of the dataset files (e.g. \"parquet\", \"csv\", \"zarr\").     description: Optional description of the logical table.     **kwargs: Additional parameters specific to the file format. (e.g. delimiter for CSV, Zarr statistics columns)</p> <p>Raises:     Exception: If the creation fails.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def create_logical_table(self, table_name: str, dataset_glob_paths: list[str], file_format: str, description: str | None = None, force=False, **kwargs) -&gt; None:\n    \"\"\"Create a new logical table on the Beacon Node.\n\n    Args:\n        table_name: Name of the new logical table.\n        dataset_glob_paths: List of dataset file paths (can include glob patterns).\n        file_format: Format of the dataset files (e.g. \"parquet\", \"csv\", \"zarr\").\n        description: Optional description of the logical table.\n        **kwargs: Additional parameters specific to the file format. (e.g. delimiter for CSV, Zarr statistics columns)\n\n    Raises:\n        Exception: If the creation fails.\n    \"\"\"\n\n    # Require Beacon server version &gt;= 1.4.0\n    if not force and not self.session.version_at_least(1,4,0):\n        raise Exception(\"Creating logical tables requires Beacon server version 1.4.0 or higher\")\n\n    # Requires admin privileges\n    if not self.session.is_admin():\n        raise Exception(\"Creating logical tables requires admin privileges\")\n\n    json_data = {\n        \"table_name\": table_name,\n        \"table_type\": {\n            \"logical\": {\n                \"glob_paths\": dataset_glob_paths,\n                \"file_format\": file_format,\n                **kwargs\n            }\n        },\n        \"description\": description\n    }\n\n    response = self.session.post(\"/api/admin/create-table\", json=json_data)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to create table: {response.text}\")\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.delete_dataset","title":"<code>delete_dataset(dataset_path, force=False)</code>","text":"<p>Delete a dataset file from the Beacon Node.</p> <p>Args:     dataset_path: Path to the dataset file on the Beacon Node.</p> <p>Raises:     Exception: If the deletion fails.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def delete_dataset(self, dataset_path: str, force=False) -&gt; None:\n    \"\"\"Delete a dataset file from the Beacon Node.\n\n    Args:\n        dataset_path: Path to the dataset file on the Beacon Node.\n\n    Raises:\n        Exception: If the deletion fails.\n    \"\"\"\n\n    # Require Beacon server version &gt;= 1.5.0\n    if not force and not self.session.version_at_least(1,5,0):\n        raise Exception(\"Deleting datasets requires Beacon server version 1.5.0 or higher\")\n\n    # Requires admin privileges\n    if not self.session.is_admin():\n        raise Exception(\"Deleting datasets requires admin privileges\")\n\n    response = self.session.delete(\"/api/admin/delete-file\", params={\n        \"file_path\": dataset_path\n    })\n    if response.status_code != 200:\n        raise Exception(f\"Failed to delete dataset: {response.text}\")\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.delete_table","title":"<code>delete_table(table_name, force=False)</code>","text":"<p>Delete a logical table from the Beacon Node.</p> <p>Args:     table_name: Name of the logical table to delete. Raises:     Exception: If the deletion fails.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def delete_table(self, table_name: str, force=False) -&gt; None:\n    \"\"\"Delete a logical table from the Beacon Node.\n\n    Args:\n        table_name: Name of the logical table to delete.\n    Raises:\n        Exception: If the deletion fails.\n    \"\"\"\n\n    # Require Beacon server version &gt;= 1.4.0\n    if not force and not self.session.version_at_least(1,4,0):\n        raise Exception(\"Deleting logical tables requires Beacon server version 1.4.0 or higher\")\n\n    # Requires admin privileges\n    if not self.session.is_admin():\n        raise Exception(\"Deleting logical tables requires admin privileges\")\n\n    response = self.session.delete(\"/api/admin/delete-table\", params={\n        \"table_name\": table_name\n    })\n    if response.status_code != 200:\n        raise Exception(f\"Failed to delete table: {response.text}\")\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.download_dataset","title":"<code>download_dataset(dataset_path, local_path, force=False)</code>","text":"<p>Download a dataset file from the Beacon Node to a local path.</p> <p>Args:     dataset_path: Path to the dataset file on the Beacon Node.     local_path: Local path where the file will be saved. Raises:     Exception: If the download fails.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def download_dataset(self, dataset_path: str, local_path: str, force=False) -&gt; None:\n    \"\"\"Download a dataset file from the Beacon Node to a local path.\n\n    Args:\n        dataset_path: Path to the dataset file on the Beacon Node.\n        local_path: Local path where the file will be saved.\n    Raises:\n        Exception: If the download fails.\n    \"\"\"\n    # Require Beacon server version &gt;= 1.5.0\n    if not force and not self.session.version_at_least(1,5,0):\n        raise Exception(\"Downloading datasets requires Beacon server version 1.5.0 or higher\")\n\n    # Requires admin privileges\n    if not self.session.is_admin():\n        raise Exception(\"Downloading datasets requires admin privileges\")\n\n    response = self.session.get(\"/api/admin/download-file\", params={\n        \"file_path\": dataset_path\n    }, stream=True)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to download dataset: {response.text}\")\n\n    with open(local_path, 'wb') as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.get_server_info","title":"<code>get_server_info()</code>","text":"<p>Return the server metadata exposed by <code>/api/info</code>.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def get_server_info(self) -&gt; dict:\n    \"\"\"Return the server metadata exposed by ``/api/info``.\"\"\"\n    response = self.session.get(\"/api/info\")\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get server info: {response.text}\")\n    return response.json()\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.list_datasets","title":"<code>list_datasets(pattern=None, limit=None, offset=None, force=False)</code>","text":"<p>Enumerate datasets registered with the Beacon node.</p> <p>Args:     pattern: Optional glob-like filter applied by the server.     limit: Optional page size to cap the number of results.     offset: Optional offset for pagination.</p> <p>Returns:     dict[str, Dataset]: Mapping from file path to :class:<code>Dataset</code> helper.</p> <p>Raises:     Exception: If the Beacon Node version &lt; 1.4.0 or the HTTP call fails.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def list_datasets(self, pattern: str | None = None, limit : int | None = None, offset: int | None = None, force=False) -&gt; dict[str, Dataset]:\n    \"\"\"Enumerate datasets registered with the Beacon node.\n\n    Args:\n        pattern: Optional glob-like filter applied by the server.\n        limit: Optional page size to cap the number of results.\n        offset: Optional offset for pagination.\n\n    Returns:\n        dict[str, Dataset]: Mapping from file path to :class:`Dataset` helper.\n\n    Raises:\n        Exception: If the Beacon Node version &lt; 1.4.0 or the HTTP call fails.\n    \"\"\"\n\n    if not force and not self.session.version_at_least(1,4,0):\n        raise Exception(\"Listing datasets requires Beacon server version 1.4.0 or higher\")\n\n    response = self.session.get(\"/api/list-datasets\", params={\n        \"pattern\": pattern,\n        \"limit\": limit,\n        \"offset\": offset\n    })\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get datasets: {response.text}\")\n    datasets = response.json()\n    dataset_objects = {}\n    for dataset in datasets:\n        file_path = dataset['file_path']\n        file_format = dataset['format']\n\n        dataset_objects[file_path] = Dataset(\n            http_session=self.session,\n            file_path=file_path,\n            file_format=file_format\n        )\n    return dataset_objects\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.list_tables","title":"<code>list_tables()</code>","text":"<p>Retrieve all logical tables available on the Beacon Node.</p> <p>Returns:     dict[str, DataTable]: Mapping of table name to :class:<code>DataTable</code> helper.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def list_tables(self) -&gt; dict[str,DataTable]:\n    \"\"\"Retrieve all logical tables available on the Beacon Node.\n\n    Returns:\n        dict[str, DataTable]: Mapping of table name to :class:`DataTable` helper.\n    \"\"\"\n    response = self.session.get(\"/api/tables\")\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get tables: {response.text}\")\n    tables = response.json()\n\n    data_tables = {}\n    for table in tables:\n        data_tables[table] = DataTable(\n            http_session=self.session,\n            table_name=table,\n        )\n\n    return data_tables\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.query","title":"<code>query()</code>","text":"<p>Create a new query object.  This is the starting point for building a query. The query can then be built using the methods on the Query object. You can also create a query from a specific table from the list_tables() method.</p> <p>To materialize and run the query, use the .to_dataframe() or .to_csv() methods on the Query object. Returns:     JSONQuery: A new query object.</p> Source code in <code>beacon_api/client.py</code> <pre><code>@deprecated(\"To query, use list_tables() or list_datasets() as a base to create a new query object. This method will be removed in future versions.\")\ndef query(self) -&gt; JSONQuery:\n    \"\"\"Create a new query object. \n    This is the starting point for building a query.\n    The query can then be built using the methods on the Query object.\n    You can also create a query from a specific table from the list_tables() method.\n\n    To materialize and run the query, use the .to_dataframe() or .to_csv() methods on the Query object.\n    Returns:\n        JSONQuery: A new query object.\n    \"\"\"\n\n    # Get the default table and use it as the from source\n    response = self.session.get(\"/api/default-table\")  # Ensure default table exists\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get default table: {response.text}\")\n    default_table = response.json()\n    return JSONQuery(http_session=self.session, _from=FromTable(default_table))\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.sql_query","title":"<code>sql_query(sql)</code>","text":"<p>Create a new :class:<code>SQLQuery</code> for direct SQL execution.</p> <p>Args:     sql: Raw SQL string sent to the Beacon Node.</p> <p>Returns:     SQLQuery: Builder that exposes <code>to_dataframe</code>, <code>to_csv</code> etc.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def sql_query(self, sql: str) -&gt; SQLQuery:\n    \"\"\"Create a new :class:`SQLQuery` for direct SQL execution.\n\n    Args:\n        sql: Raw SQL string sent to the Beacon Node.\n\n    Returns:\n        SQLQuery: Builder that exposes ``to_dataframe``, ``to_csv`` etc.\n    \"\"\"\n    return SQLQuery(http_session=self.session, query=sql)\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.subset","title":"<code>subset(longitude_column, latitude_column, time_column, depth_column, columns, bbox=None, depth_range=None, time_range=None)</code>","text":"<p>Create a query to subset the default collection based on the provided parameters.</p> <p>Args:     longitude_column: Name of the column containing longitude values.     latitude_column: Name of the column containing latitude values.     time_column: Name of the column containing time values.     depth_column: Name of the column containing depth values.     columns: List of additional columns to include in the query.     bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).     depth_range: Optional range for depth defined as (min_depth, max_depth).     time_range: Optional range for time defined as (start_time, end_time). Returns     JSONQuery: Query object that can be executed to retrieve the subset.</p> Source code in <code>beacon_api/client.py</code> <pre><code>@deprecated(\"Use the subset method on the DataTable object obtained from list_tables(). This method will be removed in future versions.\")\ndef subset(self, longitude_column: str, latitude_column: str, time_column: str, depth_column: str, columns: list[str],\n                     bbox: Optional[tuple[float, float, float, float]] = None,\n                     depth_range: Optional[tuple[float, float]] = None,\n                     time_range: Optional[tuple[datetime.datetime, datetime.datetime]] = None) -&gt; JSONQuery:\n    \"\"\"\n    Create a query to subset the default collection based on the provided parameters.\n\n    Args:\n        longitude_column: Name of the column containing longitude values.\n        latitude_column: Name of the column containing latitude values.\n        time_column: Name of the column containing time values.\n        depth_column: Name of the column containing depth values.\n        columns: List of additional columns to include in the query.\n        bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).\n        depth_range: Optional range for depth defined as (min_depth, max_depth).\n        time_range: Optional range for time defined as (start_time, end_time).\n    Returns\n        JSONQuery: Query object that can be executed to retrieve the subset.\n    \"\"\"\n    response = self.session.get(\"/api/default-table\")  # Ensure default table exists\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get default table: {response.text}\")\n    default_table = response.json()\n\n    table = self.list_tables()[default_table]\n    return table.subset(\n        longitude_column=longitude_column,\n        latitude_column=latitude_column,\n        time_column=time_column,\n        depth_column=depth_column,\n        columns=columns,\n        bbox=bbox,\n        depth_range=depth_range,\n        time_range=time_range\n    )\n</code></pre>"},{"location":"reference/client/#beacon_api.client.Client.upload_dataset","title":"<code>upload_dataset(file_path, destination_path, force=False)</code>","text":"<p>Upload a local dataset file to the Beacon Node.</p> <p>Args:     file_path: Path to the local file to upload.     destination_path: Destination path on the Beacon Node.</p> <p>Raises:     Exception: If the upload fails.</p> Source code in <code>beacon_api/client.py</code> <pre><code>def upload_dataset(self, file_path: str, destination_path: str, force=False) -&gt; None:\n    \"\"\"Upload a local dataset file to the Beacon Node.\n\n    Args:\n        file_path: Path to the local file to upload.\n        destination_path: Destination path on the Beacon Node.\n\n    Raises:\n        Exception: If the upload fails.\n    \"\"\"\n\n    # Require Beacon server version &gt;= 1.5.0\n    if not force and not self.session.version_at_least(1,5,0):\n        raise Exception(\"Uploading datasets requires Beacon server version 1.5.0 or higher\")\n\n    # Requires admin privileges\n    if not self.session.is_admin():\n        raise Exception(\"Uploading datasets requires admin privileges\")\n\n    # We use requests directly here to support multipart/form-data \n    auth_headers = {\n        \"Authorization\": self.session.headers.get(\"Authorization\", \"\")\n    }\n    url = f\"{self.session.base_url}/api/admin/upload-file\"\n\n    # Upload the file using a multipart/form-data POST request\n    # the first part should be the prefix of the destination path eg. /data/datasets/ of /data/datasets/myfile.parquet\n    prefix = '/'.join(destination_path.split('/')[:-1])\n    # File name is the last part\n    file_name = destination_path.split('/')[-1]\n    payload = {'prefix': prefix }\n    files=[\n        ('file',(file_name,open(file_path,'rb'),'application/octet-stream'))\n    ]\n    response = requests.request(\"POST\", url, headers=auth_headers, data=payload, files=files)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to upload dataset: {response.text}\")        \n</code></pre>"},{"location":"reference/dataset/","title":"Dataset reference","text":"<p>               Bases: <code>Generic[_FormatT]</code></p> <p>File or object-store resource that Beacon can scan directly.</p> <p>The class acts as a light-weight descriptor containing the user's original <code>file_path</code> plus convenience methods to inspect schema information and kick off JSON query builders.</p> Source code in <code>beacon_api/dataset.py</code> <pre><code>class Dataset(Generic[_FormatT]):\n    \"\"\"File or object-store resource that Beacon can scan directly.\n\n    The class acts as a light-weight descriptor containing the user's\n    original ``file_path`` plus convenience methods to inspect schema\n    information and kick off JSON query builders.\n    \"\"\"\n\n    def __init__(self, http_session: BaseBeaconSession, file_path: str, file_format: _FormatT):\n        \"\"\"Create a dataset descriptor.\n\n        Args:\n            http_session: Session that knows how to communicate with the Beacon Node.\n            file_path: Absolute/relative path or URI that Beacon can read.\n            file_format: File format string supported by Beacon (e.g. ``parquet``).\n        \"\"\"\n\n        self.session = http_session\n        self.file_path = file_path\n        self.file_format = file_format\n\n    def get_file_path(self) -&gt; str:\n        \"\"\"Return the original path/URI provided when constructing the dataset.\"\"\"\n\n        return self.file_path\n\n    def get_file_format(self) -&gt; str:\n        \"\"\"Return the declared file format string (case preserved).\"\"\"\n\n        return self.file_format\n\n    def get_file_name(self) -&gt; str:\n        \"\"\"Return the basename portion of the dataset path/URI.\"\"\"\n\n        return os.path.basename(self.file_path.rstrip(\"/\\\\\"))\n\n    def get_schema(self) -&gt; SchemaType:\n        \"\"\"Fetch the dataset schema by calling the Beacon Node.\n\n        Returns:\n            SchemaType: JSON-compatible schema description mirroring the\n                server's ``/api/dataset-schema`` payload.\n\n        Raises:\n            RuntimeError: If the HTTP request fails.\n            ValueError: When the response body is not valid JSON or the\n                decoded value is not a JSON object.\n            Exception: For unsupported field types surfaced by Beacon.\n        \"\"\"\n\n        response = self.session.get(\"/api/dataset-schema\", params={\"file\": self.file_path})\n        if response.status_code != 200:\n            raise RuntimeError(f\"Failed to get dataset schema: {response.text}\")\n\n        try:\n            schema_data = response.json()\n        except ValueError as exc:\n            raise ValueError(\"Dataset schema response was not valid JSON\") from exc\n\n        if not isinstance(schema_data, dict):\n            raise ValueError(\"Dataset schema response must be a JSON object\")\n\n        fields = []\n\n        for field in schema_data['fields']:\n            field_type = field['data_type']\n\n            if isinstance(field_type, str):\n                fields.append(pa.field(field['name'], field_type))\n\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Second\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('s')))\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Millisecond\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('ms')))\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Microsecond\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('us')))\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Nanosecond\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('ns')))\n\n            else:\n                raise Exception(f\"Unsupported data type for field {field['name']}: {field_type}\")\n\n        return pa.schema(fields)\n\n    def get_file_extension(self) -&gt; str:\n        \"\"\"Return the lowercase file extension without the leading dot.\"\"\"\n\n        file_name = self.get_file_name()\n        _, extension = os.path.splitext(file_name)\n        return extension.lstrip(\".\")\n\n    def __str__(self) -&gt; str:\n        return self.file_path\n\n    def __repr__(self) -&gt; str:\n        return f\"Dataset(file_path={self.file_path})\"\n\n    @overload\n    def query(self: \"Dataset[Literal['csv']]\", *, delimiter: str | None = None, **kwargs: Any) -&gt; JSONQuery:\n        ...\n\n    @overload\n    def query(self: \"Dataset[Literal['zarr']]\", *, statistics_columns: Sequence[str] | None = None, **kwargs: Any) -&gt; JSONQuery:\n        ...\n\n    @overload\n    def query(self, **kwargs: Any) -&gt; JSONQuery:\n        ...\n\n    def query(\n        self,\n        *,\n        delimiter: str | None = None,\n        statistics_columns: Sequence[str] | None = None,\n        **kwargs: Any,\n    ) -&gt; JSONQuery:\n        \"\"\"Build a :class:`~beacon_api.query.JSONQuery` starting from this dataset.\n\n        Args:\n            delimiter: Optional CSV delimiter override (only valid for CSV datasets).\n            statistics_columns: Optional Zarr statistics column names (only valid for Zarr datasets).\n            **kwargs: Additional format-specific options forwarded to the query builder.\n\n        Returns:\n            JSONQuery: Query builder tied to this dataset source.\n\n        Raises:\n            ValueError: If a format-specific option is passed to the wrong\n                dataset type or the format is not supported.\n        \"\"\"\n\n        file_format_str = self.get_file_format().lower()\n        builder = _DATASET_FROM_FACTORIES.get(file_format_str)\n\n        if builder is None:\n            supported = \", \".join(sorted(_DATASET_FROM_FACTORIES))\n            raise ValueError(f\"Unsupported dataset format '{file_format_str}'. Supported formats: {supported}\")\n\n        builder_options: dict[str, Any] = dict(kwargs)\n\n        if delimiter is not None:\n            if file_format_str != \"csv\":\n                raise ValueError(\"The 'delimiter' option is only supported for CSV datasets.\")\n            builder_options[\"delimiter\"] = delimiter\n\n        if statistics_columns is not None:\n            if file_format_str != \"zarr\":\n                raise ValueError(\"The 'statistics_columns' option is only supported for Zarr datasets.\")\n            builder_options[\"statistics_columns\"] = list(statistics_columns)\n\n        _from = builder(self.file_path, builder_options)\n        return JSONQuery(http_session=self.session, _from=_from)\n</code></pre>"},{"location":"reference/dataset/#beacon_api.dataset.Dataset.__init__","title":"<code>__init__(http_session, file_path, file_format)</code>","text":"<p>Create a dataset descriptor.</p> <p>Args:     http_session: Session that knows how to communicate with the Beacon Node.     file_path: Absolute/relative path or URI that Beacon can read.     file_format: File format string supported by Beacon (e.g. <code>parquet</code>).</p> Source code in <code>beacon_api/dataset.py</code> <pre><code>def __init__(self, http_session: BaseBeaconSession, file_path: str, file_format: _FormatT):\n    \"\"\"Create a dataset descriptor.\n\n    Args:\n        http_session: Session that knows how to communicate with the Beacon Node.\n        file_path: Absolute/relative path or URI that Beacon can read.\n        file_format: File format string supported by Beacon (e.g. ``parquet``).\n    \"\"\"\n\n    self.session = http_session\n    self.file_path = file_path\n    self.file_format = file_format\n</code></pre>"},{"location":"reference/dataset/#beacon_api.dataset.Dataset.get_file_extension","title":"<code>get_file_extension()</code>","text":"<p>Return the lowercase file extension without the leading dot.</p> Source code in <code>beacon_api/dataset.py</code> <pre><code>def get_file_extension(self) -&gt; str:\n    \"\"\"Return the lowercase file extension without the leading dot.\"\"\"\n\n    file_name = self.get_file_name()\n    _, extension = os.path.splitext(file_name)\n    return extension.lstrip(\".\")\n</code></pre>"},{"location":"reference/dataset/#beacon_api.dataset.Dataset.get_file_format","title":"<code>get_file_format()</code>","text":"<p>Return the declared file format string (case preserved).</p> Source code in <code>beacon_api/dataset.py</code> <pre><code>def get_file_format(self) -&gt; str:\n    \"\"\"Return the declared file format string (case preserved).\"\"\"\n\n    return self.file_format\n</code></pre>"},{"location":"reference/dataset/#beacon_api.dataset.Dataset.get_file_name","title":"<code>get_file_name()</code>","text":"<p>Return the basename portion of the dataset path/URI.</p> Source code in <code>beacon_api/dataset.py</code> <pre><code>def get_file_name(self) -&gt; str:\n    \"\"\"Return the basename portion of the dataset path/URI.\"\"\"\n\n    return os.path.basename(self.file_path.rstrip(\"/\\\\\"))\n</code></pre>"},{"location":"reference/dataset/#beacon_api.dataset.Dataset.get_file_path","title":"<code>get_file_path()</code>","text":"<p>Return the original path/URI provided when constructing the dataset.</p> Source code in <code>beacon_api/dataset.py</code> <pre><code>def get_file_path(self) -&gt; str:\n    \"\"\"Return the original path/URI provided when constructing the dataset.\"\"\"\n\n    return self.file_path\n</code></pre>"},{"location":"reference/dataset/#beacon_api.dataset.Dataset.get_schema","title":"<code>get_schema()</code>","text":"<p>Fetch the dataset schema by calling the Beacon Node.</p> <p>Returns:     SchemaType: JSON-compatible schema description mirroring the         server's <code>/api/dataset-schema</code> payload.</p> <p>Raises:     RuntimeError: If the HTTP request fails.     ValueError: When the response body is not valid JSON or the         decoded value is not a JSON object.     Exception: For unsupported field types surfaced by Beacon.</p> Source code in <code>beacon_api/dataset.py</code> <pre><code>def get_schema(self) -&gt; SchemaType:\n    \"\"\"Fetch the dataset schema by calling the Beacon Node.\n\n    Returns:\n        SchemaType: JSON-compatible schema description mirroring the\n            server's ``/api/dataset-schema`` payload.\n\n    Raises:\n        RuntimeError: If the HTTP request fails.\n        ValueError: When the response body is not valid JSON or the\n            decoded value is not a JSON object.\n        Exception: For unsupported field types surfaced by Beacon.\n    \"\"\"\n\n    response = self.session.get(\"/api/dataset-schema\", params={\"file\": self.file_path})\n    if response.status_code != 200:\n        raise RuntimeError(f\"Failed to get dataset schema: {response.text}\")\n\n    try:\n        schema_data = response.json()\n    except ValueError as exc:\n        raise ValueError(\"Dataset schema response was not valid JSON\") from exc\n\n    if not isinstance(schema_data, dict):\n        raise ValueError(\"Dataset schema response must be a JSON object\")\n\n    fields = []\n\n    for field in schema_data['fields']:\n        field_type = field['data_type']\n\n        if isinstance(field_type, str):\n            fields.append(pa.field(field['name'], field_type))\n\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Second\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('s')))\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Millisecond\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('ms')))\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Microsecond\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('us')))\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Nanosecond\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('ns')))\n\n        else:\n            raise Exception(f\"Unsupported data type for field {field['name']}: {field_type}\")\n\n    return pa.schema(fields)\n</code></pre>"},{"location":"reference/dataset/#beacon_api.dataset.Dataset.query","title":"<code>query(*, delimiter=None, statistics_columns=None, **kwargs)</code>","text":"<pre><code>query(*, delimiter: str | None = None, **kwargs: Any) -&gt; JSONQuery\n</code></pre><pre><code>query(*, statistics_columns: Sequence[str] | None = None, **kwargs: Any) -&gt; JSONQuery\n</code></pre><pre><code>query(**kwargs: Any) -&gt; JSONQuery\n</code></pre> <p>Build a :class:<code>~beacon_api.query.JSONQuery</code> starting from this dataset.</p> <p>Args:     delimiter: Optional CSV delimiter override (only valid for CSV datasets).     statistics_columns: Optional Zarr statistics column names (only valid for Zarr datasets).     **kwargs: Additional format-specific options forwarded to the query builder.</p> <p>Returns:     JSONQuery: Query builder tied to this dataset source.</p> <p>Raises:     ValueError: If a format-specific option is passed to the wrong         dataset type or the format is not supported.</p> Source code in <code>beacon_api/dataset.py</code> <pre><code>def query(\n    self,\n    *,\n    delimiter: str | None = None,\n    statistics_columns: Sequence[str] | None = None,\n    **kwargs: Any,\n) -&gt; JSONQuery:\n    \"\"\"Build a :class:`~beacon_api.query.JSONQuery` starting from this dataset.\n\n    Args:\n        delimiter: Optional CSV delimiter override (only valid for CSV datasets).\n        statistics_columns: Optional Zarr statistics column names (only valid for Zarr datasets).\n        **kwargs: Additional format-specific options forwarded to the query builder.\n\n    Returns:\n        JSONQuery: Query builder tied to this dataset source.\n\n    Raises:\n        ValueError: If a format-specific option is passed to the wrong\n            dataset type or the format is not supported.\n    \"\"\"\n\n    file_format_str = self.get_file_format().lower()\n    builder = _DATASET_FROM_FACTORIES.get(file_format_str)\n\n    if builder is None:\n        supported = \", \".join(sorted(_DATASET_FROM_FACTORIES))\n        raise ValueError(f\"Unsupported dataset format '{file_format_str}'. Supported formats: {supported}\")\n\n    builder_options: dict[str, Any] = dict(kwargs)\n\n    if delimiter is not None:\n        if file_format_str != \"csv\":\n            raise ValueError(\"The 'delimiter' option is only supported for CSV datasets.\")\n        builder_options[\"delimiter\"] = delimiter\n\n    if statistics_columns is not None:\n        if file_format_str != \"zarr\":\n            raise ValueError(\"The 'statistics_columns' option is only supported for Zarr datasets.\")\n        builder_options[\"statistics_columns\"] = list(statistics_columns)\n\n    _from = builder(self.file_path, builder_options)\n    return JSONQuery(http_session=self.session, _from=_from)\n</code></pre>"},{"location":"reference/functions/","title":"Functions Reference","text":"Source code in <code>beacon_api/query/functions.py</code> <pre><code>class Functions:\n    @staticmethod\n    def concat(args: List[Union[str, Select]], alias: str) -&gt; SelectFunction:\n        \"\"\"\n        Constructs a CONCAT function, concatenating the selected columns or arguments.\n        Args:\n            args (list[str  |  Select]): List of column names (str) or Select objects to concatenate.\n            alias (str): Alias name for the resulting select expression.\n        \"\"\"\n\n        select_args = []\n        for arg in args:\n            if isinstance(arg, str):\n                select_args.append(SelectColumn(column=arg))\n            elif isinstance(arg, Select):\n                select_args.append(arg)\n        return SelectFunction(\"concat\", args=select_args, alias=alias)\n\n    @staticmethod\n    def coalesce(args: List[Union[str, Select]], alias: str) -&gt; SelectFunction:\n        \"\"\"\n        Constructs a COALESCE function, returning the first non-null value from the selected columns or arguments.\n        Args:\n            args (list[str  |  Select]): List of column names (str) or Select objects to coalesce.\n            alias (str): Alias name for the resulting select expression.\n\n        Returns:\n            SelectFunction: SelectFunction representing the COALESCE operation.\n        \"\"\"\n        select_args = []\n        for arg in args:\n            if isinstance(arg, str):\n                select_args.append(SelectColumn(column=arg))\n            elif isinstance(arg, Select):\n                select_args.append(arg)\n        return SelectFunction(\"coalesce\", args=select_args, alias=alias)\n\n    @staticmethod\n    def try_cast_to_type(arg: Union[str, Select], to_type: DTypeLike, alias: str) -&gt; SelectFunction:\n            \"\"\"\n            Attempts to cast the input column or argument to the specified data type.\n            Args:\n                arg: Column name (str) or Select object to cast.\n                to_type: Target data type (compatible with numpy dtype). Eg. np.int64, np.float64, np.datetime64, np.str_\n                alias: Alias name for the resulting select expression.\n            Returns:\n                SelectFunction representing the cast operation.\n            \"\"\"\n            dtype = np.dtype(to_type)  # normalize everything into a np.dtype\n            arrow_type = None\n            if np.issubdtype(dtype, np.integer):\n                print(\"This is an integer dtype:\", dtype)\n                arrow_type = \"Int64\"\n            elif np.issubdtype(dtype, np.floating):\n                arrow_type = \"Float64\"\n            elif np.issubdtype(dtype, np.datetime64):\n                arrow_type = 'Timestamp(Nanosecond, None)'\n            elif np.issubdtype(dtype, np.str_):\n                arrow_type = 'Utf8'\n            else:\n                raise ValueError(f\"Unsupported type for cast_to_type: {to_type}\")\n\n            if isinstance(arg, str):\n                arg = SelectColumn(column=arg)\n                return SelectFunction(\"try_arrow_cast\", args=[arg, SelectLiteral(value=arrow_type)], alias=alias)\n            elif isinstance(arg, Select):\n                return SelectFunction(\"try_arrow_cast\", args=[arg, SelectLiteral(value=arrow_type)], alias=alias)\n\n    @staticmethod\n    def cast_byte_to_char(arg: Union[str, Select], alias: str) -&gt; SelectFunction:\n        \"\"\"Maps byte values to char.\n\n        Args:\n            arg (str | Select): column name (str) or Select object containing the byte value.\n            alias (str): Alias name for the resulting select expression/column.\n\n        Returns:\n            SelectFunction: SelectFunction representing the cast operation.\n        \"\"\"\n        if isinstance(arg, str):\n            arg = SelectColumn(column=arg)\n        return SelectFunction(\"cast_int8_as_char\", args=[arg], alias=alias)\n\n    @staticmethod\n    def map_wod_quality_flag_to_sdn_scheme(arg: Union[str, Select], alias: str) -&gt; SelectFunction:\n        \"\"\"Maps WOD quality flags to the SDN scheme.\n\n        Args:\n            arg (str | Select): column name (str) or Select object containing the WOD quality flag.\n            alias (str): Alias name for the resulting select expression/column.\n\n        Returns:\n            SelectFunction: SelectFunction representing the mapping operation.\n        \"\"\"\n        if isinstance(arg, str):\n            arg = SelectColumn(column=arg)\n        return SelectFunction(\"map_wod_quality_flag\", args=[arg], alias=alias)\n\n    @staticmethod\n    def map_pressure_to_depth(arg: Union[str, Select], latitude_column: Union[str, Select], alias: str) -&gt; SelectFunction:\n        \"\"\"Maps pressure values to depth based on latitude using teos-10.\n\n        Args:\n            arg (str | Select): column name (str) or Select object containing the pressure value.\n            latitude_column (str | Select): column name (str) or Select object containing the latitude value.\n            alias (str): Alias name for the resulting select expression/column.\n\n        Returns:\n            SelectFunction: SelectFunction representing the pressure-to-depth mapping operation.\n        \"\"\"\n        if isinstance(arg, str):\n            arg = SelectColumn(column=arg)\n        if isinstance(latitude_column, str):\n            latitude_column = SelectColumn(column=latitude_column)\n        return SelectFunction(\"pressure_to_depth_teos_10\", args=[arg, latitude_column], alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.cast_byte_to_char","title":"<code>cast_byte_to_char(arg, alias)</code>  <code>staticmethod</code>","text":"<p>Maps byte values to char.</p> <p>Args:     arg (str | Select): column name (str) or Select object containing the byte value.     alias (str): Alias name for the resulting select expression/column.</p> <p>Returns:     SelectFunction: SelectFunction representing the cast operation.</p> Source code in <code>beacon_api/query/functions.py</code> <pre><code>@staticmethod\ndef cast_byte_to_char(arg: Union[str, Select], alias: str) -&gt; SelectFunction:\n    \"\"\"Maps byte values to char.\n\n    Args:\n        arg (str | Select): column name (str) or Select object containing the byte value.\n        alias (str): Alias name for the resulting select expression/column.\n\n    Returns:\n        SelectFunction: SelectFunction representing the cast operation.\n    \"\"\"\n    if isinstance(arg, str):\n        arg = SelectColumn(column=arg)\n    return SelectFunction(\"cast_int8_as_char\", args=[arg], alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.coalesce","title":"<code>coalesce(args, alias)</code>  <code>staticmethod</code>","text":"<p>Constructs a COALESCE function, returning the first non-null value from the selected columns or arguments. Args:     args (list[str  |  Select]): List of column names (str) or Select objects to coalesce.     alias (str): Alias name for the resulting select expression.</p> <p>Returns:     SelectFunction: SelectFunction representing the COALESCE operation.</p> Source code in <code>beacon_api/query/functions.py</code> <pre><code>@staticmethod\ndef coalesce(args: List[Union[str, Select]], alias: str) -&gt; SelectFunction:\n    \"\"\"\n    Constructs a COALESCE function, returning the first non-null value from the selected columns or arguments.\n    Args:\n        args (list[str  |  Select]): List of column names (str) or Select objects to coalesce.\n        alias (str): Alias name for the resulting select expression.\n\n    Returns:\n        SelectFunction: SelectFunction representing the COALESCE operation.\n    \"\"\"\n    select_args = []\n    for arg in args:\n        if isinstance(arg, str):\n            select_args.append(SelectColumn(column=arg))\n        elif isinstance(arg, Select):\n            select_args.append(arg)\n    return SelectFunction(\"coalesce\", args=select_args, alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.concat","title":"<code>concat(args, alias)</code>  <code>staticmethod</code>","text":"<p>Constructs a CONCAT function, concatenating the selected columns or arguments. Args:     args (list[str  |  Select]): List of column names (str) or Select objects to concatenate.     alias (str): Alias name for the resulting select expression.</p> Source code in <code>beacon_api/query/functions.py</code> <pre><code>@staticmethod\ndef concat(args: List[Union[str, Select]], alias: str) -&gt; SelectFunction:\n    \"\"\"\n    Constructs a CONCAT function, concatenating the selected columns or arguments.\n    Args:\n        args (list[str  |  Select]): List of column names (str) or Select objects to concatenate.\n        alias (str): Alias name for the resulting select expression.\n    \"\"\"\n\n    select_args = []\n    for arg in args:\n        if isinstance(arg, str):\n            select_args.append(SelectColumn(column=arg))\n        elif isinstance(arg, Select):\n            select_args.append(arg)\n    return SelectFunction(\"concat\", args=select_args, alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.map_pressure_to_depth","title":"<code>map_pressure_to_depth(arg, latitude_column, alias)</code>  <code>staticmethod</code>","text":"<p>Maps pressure values to depth based on latitude using teos-10.</p> <p>Args:     arg (str | Select): column name (str) or Select object containing the pressure value.     latitude_column (str | Select): column name (str) or Select object containing the latitude value.     alias (str): Alias name for the resulting select expression/column.</p> <p>Returns:     SelectFunction: SelectFunction representing the pressure-to-depth mapping operation.</p> Source code in <code>beacon_api/query/functions.py</code> <pre><code>@staticmethod\ndef map_pressure_to_depth(arg: Union[str, Select], latitude_column: Union[str, Select], alias: str) -&gt; SelectFunction:\n    \"\"\"Maps pressure values to depth based on latitude using teos-10.\n\n    Args:\n        arg (str | Select): column name (str) or Select object containing the pressure value.\n        latitude_column (str | Select): column name (str) or Select object containing the latitude value.\n        alias (str): Alias name for the resulting select expression/column.\n\n    Returns:\n        SelectFunction: SelectFunction representing the pressure-to-depth mapping operation.\n    \"\"\"\n    if isinstance(arg, str):\n        arg = SelectColumn(column=arg)\n    if isinstance(latitude_column, str):\n        latitude_column = SelectColumn(column=latitude_column)\n    return SelectFunction(\"pressure_to_depth_teos_10\", args=[arg, latitude_column], alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.map_wod_quality_flag_to_sdn_scheme","title":"<code>map_wod_quality_flag_to_sdn_scheme(arg, alias)</code>  <code>staticmethod</code>","text":"<p>Maps WOD quality flags to the SDN scheme.</p> <p>Args:     arg (str | Select): column name (str) or Select object containing the WOD quality flag.     alias (str): Alias name for the resulting select expression/column.</p> <p>Returns:     SelectFunction: SelectFunction representing the mapping operation.</p> Source code in <code>beacon_api/query/functions.py</code> <pre><code>@staticmethod\ndef map_wod_quality_flag_to_sdn_scheme(arg: Union[str, Select], alias: str) -&gt; SelectFunction:\n    \"\"\"Maps WOD quality flags to the SDN scheme.\n\n    Args:\n        arg (str | Select): column name (str) or Select object containing the WOD quality flag.\n        alias (str): Alias name for the resulting select expression/column.\n\n    Returns:\n        SelectFunction: SelectFunction representing the mapping operation.\n    \"\"\"\n    if isinstance(arg, str):\n        arg = SelectColumn(column=arg)\n    return SelectFunction(\"map_wod_quality_flag\", args=[arg], alias=alias)\n</code></pre>"},{"location":"reference/functions/#beacon_api.query.Functions.try_cast_to_type","title":"<code>try_cast_to_type(arg, to_type, alias)</code>  <code>staticmethod</code>","text":"<p>Attempts to cast the input column or argument to the specified data type. Args:     arg: Column name (str) or Select object to cast.     to_type: Target data type (compatible with numpy dtype). Eg. np.int64, np.float64, np.datetime64, np.str_     alias: Alias name for the resulting select expression. Returns:     SelectFunction representing the cast operation.</p> Source code in <code>beacon_api/query/functions.py</code> <pre><code>@staticmethod\ndef try_cast_to_type(arg: Union[str, Select], to_type: DTypeLike, alias: str) -&gt; SelectFunction:\n        \"\"\"\n        Attempts to cast the input column or argument to the specified data type.\n        Args:\n            arg: Column name (str) or Select object to cast.\n            to_type: Target data type (compatible with numpy dtype). Eg. np.int64, np.float64, np.datetime64, np.str_\n            alias: Alias name for the resulting select expression.\n        Returns:\n            SelectFunction representing the cast operation.\n        \"\"\"\n        dtype = np.dtype(to_type)  # normalize everything into a np.dtype\n        arrow_type = None\n        if np.issubdtype(dtype, np.integer):\n            print(\"This is an integer dtype:\", dtype)\n            arrow_type = \"Int64\"\n        elif np.issubdtype(dtype, np.floating):\n            arrow_type = \"Float64\"\n        elif np.issubdtype(dtype, np.datetime64):\n            arrow_type = 'Timestamp(Nanosecond, None)'\n        elif np.issubdtype(dtype, np.str_):\n            arrow_type = 'Utf8'\n        else:\n            raise ValueError(f\"Unsupported type for cast_to_type: {to_type}\")\n\n        if isinstance(arg, str):\n            arg = SelectColumn(column=arg)\n            return SelectFunction(\"try_arrow_cast\", args=[arg, SelectLiteral(value=arrow_type)], alias=alias)\n        elif isinstance(arg, Select):\n            return SelectFunction(\"try_arrow_cast\", args=[arg, SelectLiteral(value=arrow_type)], alias=alias)\n</code></pre>"},{"location":"reference/query/","title":"Query reference","text":""},{"location":"reference/query/#base-query-helpers","title":"Base query helpers","text":"Source code in <code>beacon_api/query/__init__.py</code> <pre><code>class BaseQuery:\n    def __init__(self, http_session: BaseBeaconSession):\n        self.http_session = http_session\n        self.output_format = None\n\n    @abstractmethod\n    def compile(self) -&gt; dict:\n        ...\n\n    def set_output(self, output_format: Output) -&gt; None:\n        \"\"\"Set the output format for the query\"\"\"\n        self.output_format = output_format\n\n    def output(self) -&gt; dict:\n        \"\"\"Get the output format of the query, if specified\"\"\"\n        return {\n            \"output\": self.output_format.to_dict() if self.output_format else None\n        }\n\n    def compile_query(self) -&gt; str:\n        \"\"\"Compile the query to a JSON string\"\"\"\n        query_body_dict = self.output() | self.compile()\n\n        def datetime_converter(o):\n            if isinstance(o, datetime):\n                return o.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n            raise TypeError(f\"Type {type(o)} not serializable\")\n\n        query_body = json.dumps(query_body_dict, default=datetime_converter)\n        return query_body\n\n    def explain(self) -&gt; dict:\n        \"\"\"Get the query plan\"\"\"\n        query = self.compile_query()\n        response = self.http_session.post(\"/api/explain-query\", data=query)\n        if response.status_code != 200:\n            raise Exception(f\"Explain query failed: {response.text}\")\n        return response.json()\n\n    def execute(self, stream=False) -&gt; Response:\n        \"\"\"Run the query and return the response\"\"\"\n        query_body = self.compile_query()\n        print(f\"Running query: {query_body}\")\n        response = self.http_session.post(\"/api/query\", data=query_body, stream=stream)\n        if response.status_code != 200:\n            raise Exception(f\"Query failed: {response.text}\")\n        if len(response.content) == 0:\n            raise Exception(\"Query returned no content\")\n        return response\n\n    def execute_streaming(self, force=False) -&gt; pa.RecordBatchStreamReader:\n        \"\"\"Run the query and return the response as a streaming response\"\"\"\n        if not force and not self.http_session.version_at_least(1, 5, 0):\n            raise Exception(\"Streaming queries require the Beacon Node version to be atleast 1.5.0 or higher\")\n\n        query_body = self.compile_query()\n        print(f\"Running query: {query_body}\")\n        response = self.http_session.post(\"/api/query\", data=query_body, stream=True)\n\n        stream = ipc.open_stream(response.raw)\n\n        return stream\n\n    def to_xarray_dataset(self, dimension_columns: List[str], chunks: Union[dict, None] = None, auto_cleanup=True, force=False) -&gt; xr.Dataset:\n        \"\"\"Converts the query results to an xarray Dataset with n-dimensional structure.\n\n        Args:\n            dimension_columns (list[str]): The list of columns to use as dimensions in the xarray Dataset.\n\n        Returns:\n            xarray.Dataset: The query results as an xarray Dataset.\n        \"\"\"\n        if not force and not self.http_session.version_at_least(1, 5, 0):\n            raise Exception(\"xarray dataset output requires the Beacon Node version to be atleast 1.5.0 or higher\")\n\n        # create tempfile for the netcdf file\n        fd, path = tempfile.mkstemp(suffix=\".nc\")\n        self.to_nd_netcdf(file_path=path, dimension_columns=dimension_columns,force=force)\n\n        ds = xr.open_dataset(path, chunks=chunks)\n        # register for cleanup the tempfile\n        if auto_cleanup:\n            atexit.register(lambda: os.path.exists(path) and os.remove(path))\n\n        return ds\n\n    def to_pandas_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Execute the query and return the results as a pandas DataFrame\"\"\"\n        self.set_output(Parquet())\n        response = self.execute()\n        bytes_io = BytesIO(response.content)\n        return pd.read_parquet(bytes_io)\n\n    def to_geo_pandas_dataframe(self, longitude_column: str, latitude_column: str, crs: str = \"EPSG:4326\") -&gt; gpd.GeoDataFrame:\n        \"\"\"Converts the query results to a GeoPandas GeoDataFrame.\n\n        Args:\n            longitude_column (str): The name of the column representing longitude.\n            latitude_column (str): The name of the column representing latitude.\n            crs (str, optional): The coordinate reference system to use. Defaults to \"EPSG:4326\".\n\n        Returns:\n            gpd.GeoDataFrame: The query results as a GeoPandas GeoDataFrame.\n        \"\"\"\n\n        self.set_output(GeoParquet(longitude_column=longitude_column, latitude_column=latitude_column))\n        response = self.execute()\n        bytes_io = BytesIO(response.content)\n        # Read into parquet arrow table \n        table = pq.read_table(bytes_io)\n\n        gdf = gpd.GeoDataFrame.from_arrow(table)\n        gdf.set_crs(crs, inplace=True)\n        return gdf\n\n    def to_parquet(self, file_path: str, streaming_chunk_size: int = 1024*1024):\n        \"\"\"Execute the query and save the results as a Parquet file\"\"\"\n        self.set_output(Parquet())\n        response = self.execute()\n        with open(file_path, \"wb\") as f:\n            # Write the content of the response to a file\n            for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n                if chunk:  # skip keep-alive chunks\n                    f.write(chunk)\n\n    def to_geoparquet(self, file_path: str, longitude_column: str, latitude_column: str, streaming_chunk_size: int = 1024*1024):\n        \"\"\"Execute the query and save the results as a GeoParquet file\"\"\"\n        self.set_output(GeoParquet(longitude_column=longitude_column, latitude_column=latitude_column))\n        response = self.execute()\n        with open(file_path, \"wb\") as f:\n            # Write the content of the response to a file\n            for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n                if chunk:  # skip keep-alive chunks\n                    f.write(chunk)\n\n    def to_csv(self, file_path: str, streaming_chunk_size: int = 1024*1024):\n        \"\"\"Execute the query and save the results as a CSV file\"\"\"\n        self.set_output(CSV())\n        response = self.execute()\n        with open(file_path, \"wb\") as f:\n            # Write the content of the response to a file\n            for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n                if chunk:  # skip keep-alive chunks\n                    f.write(chunk)\n\n    def to_arrow(self, file_path: str, streaming_chunk_size: int = 1024*1024):\n        \"\"\"Execute the query and save the results as an Arrow file\"\"\"\n        self.set_output(Arrow())\n        response = self.execute()\n        with open(file_path, \"wb\") as f:\n            # Write the content of the response to a file\n            for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n                if chunk:  # skip keep-alive chunks\n                    f.write(chunk)\n\n    def to_netcdf(self, file_path: str, build_nc_local:bool = True, streaming_chunk_size: int = 1024*1024):\n        \"\"\"Execute the query and save the results as an NetCDF file\"\"\"\n        if build_nc_local:\n            df = self.to_pandas_dataframe()\n            xdf = df.to_xarray()\n            xdf.to_netcdf(file_path, mode=\"w\")\n        else:\n            self.set_output(NetCDF())  # Specify dimension columns as needed\n            response = self.execute()\n            with open(file_path, \"wb\") as f:\n                # Write the content of the response to a file\n                for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n                    if chunk:  # skip keep-alive chunks\n                        f.write(chunk)\n\n    def to_nd_netcdf(self, file_path: str, dimension_columns: list[str], streaming_chunk_size: int = 1024*1024, force: bool = False):\n        \"\"\"Execute the query and save the results as an NdNetCDF file\"\"\"\n        if not force and not self.http_session.version_at_least(1, 5, 0):\n            raise Exception(\"NdNetCDF output format requires the Beacon Node version to be atleast 1.5.0 or higher\")\n        self.set_output(NdNetCDF(dimension_columns=dimension_columns))\n        response = self.execute()\n        with open(file_path, \"wb\") as f:\n            # Write the content of the response to a file\n            for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n                if chunk:  # skip keep-alive chunks\n                    f.write(chunk)\n\n    def to_zarr(self, file_path: str):\n        # Read to pandas dataframe first\n        df = self.to_pandas_dataframe()\n        # Convert to Zarr format\n        xdf = df.to_xarray()\n        xdf.to_zarr(file_path, mode=\"w\")\n\n    def to_odv(self, odv_output: Odv, file_path: str):\n        \"\"\"Exports the query results to an ODV file.\n\n        Args:\n            odv_output (Odv): The ODV output format to use.\n            file_path (str): The path to the file where the ODV data will be saved.\n        \"\"\"\n        self.set_output(odv_output)\n        response = self.execute()\n        with open(file_path, \"wb\") as f:\n            # Write the content of the response to a file\n            f.write(response.content)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.compile_query","title":"<code>compile_query()</code>","text":"<p>Compile the query to a JSON string</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def compile_query(self) -&gt; str:\n    \"\"\"Compile the query to a JSON string\"\"\"\n    query_body_dict = self.output() | self.compile()\n\n    def datetime_converter(o):\n        if isinstance(o, datetime):\n            return o.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n        raise TypeError(f\"Type {type(o)} not serializable\")\n\n    query_body = json.dumps(query_body_dict, default=datetime_converter)\n    return query_body\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.execute","title":"<code>execute(stream=False)</code>","text":"<p>Run the query and return the response</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def execute(self, stream=False) -&gt; Response:\n    \"\"\"Run the query and return the response\"\"\"\n    query_body = self.compile_query()\n    print(f\"Running query: {query_body}\")\n    response = self.http_session.post(\"/api/query\", data=query_body, stream=stream)\n    if response.status_code != 200:\n        raise Exception(f\"Query failed: {response.text}\")\n    if len(response.content) == 0:\n        raise Exception(\"Query returned no content\")\n    return response\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.execute_streaming","title":"<code>execute_streaming(force=False)</code>","text":"<p>Run the query and return the response as a streaming response</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def execute_streaming(self, force=False) -&gt; pa.RecordBatchStreamReader:\n    \"\"\"Run the query and return the response as a streaming response\"\"\"\n    if not force and not self.http_session.version_at_least(1, 5, 0):\n        raise Exception(\"Streaming queries require the Beacon Node version to be atleast 1.5.0 or higher\")\n\n    query_body = self.compile_query()\n    print(f\"Running query: {query_body}\")\n    response = self.http_session.post(\"/api/query\", data=query_body, stream=True)\n\n    stream = ipc.open_stream(response.raw)\n\n    return stream\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.explain","title":"<code>explain()</code>","text":"<p>Get the query plan</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def explain(self) -&gt; dict:\n    \"\"\"Get the query plan\"\"\"\n    query = self.compile_query()\n    response = self.http_session.post(\"/api/explain-query\", data=query)\n    if response.status_code != 200:\n        raise Exception(f\"Explain query failed: {response.text}\")\n    return response.json()\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.output","title":"<code>output()</code>","text":"<p>Get the output format of the query, if specified</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def output(self) -&gt; dict:\n    \"\"\"Get the output format of the query, if specified\"\"\"\n    return {\n        \"output\": self.output_format.to_dict() if self.output_format else None\n    }\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.set_output","title":"<code>set_output(output_format)</code>","text":"<p>Set the output format for the query</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def set_output(self, output_format: Output) -&gt; None:\n    \"\"\"Set the output format for the query\"\"\"\n    self.output_format = output_format\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.to_arrow","title":"<code>to_arrow(file_path, streaming_chunk_size=1024 * 1024)</code>","text":"<p>Execute the query and save the results as an Arrow file</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def to_arrow(self, file_path: str, streaming_chunk_size: int = 1024*1024):\n    \"\"\"Execute the query and save the results as an Arrow file\"\"\"\n    self.set_output(Arrow())\n    response = self.execute()\n    with open(file_path, \"wb\") as f:\n        # Write the content of the response to a file\n        for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n            if chunk:  # skip keep-alive chunks\n                f.write(chunk)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.to_csv","title":"<code>to_csv(file_path, streaming_chunk_size=1024 * 1024)</code>","text":"<p>Execute the query and save the results as a CSV file</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def to_csv(self, file_path: str, streaming_chunk_size: int = 1024*1024):\n    \"\"\"Execute the query and save the results as a CSV file\"\"\"\n    self.set_output(CSV())\n    response = self.execute()\n    with open(file_path, \"wb\") as f:\n        # Write the content of the response to a file\n        for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n            if chunk:  # skip keep-alive chunks\n                f.write(chunk)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.to_geo_pandas_dataframe","title":"<code>to_geo_pandas_dataframe(longitude_column, latitude_column, crs='EPSG:4326')</code>","text":"<p>Converts the query results to a GeoPandas GeoDataFrame.</p> <p>Args:     longitude_column (str): The name of the column representing longitude.     latitude_column (str): The name of the column representing latitude.     crs (str, optional): The coordinate reference system to use. Defaults to \"EPSG:4326\".</p> <p>Returns:     gpd.GeoDataFrame: The query results as a GeoPandas GeoDataFrame.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def to_geo_pandas_dataframe(self, longitude_column: str, latitude_column: str, crs: str = \"EPSG:4326\") -&gt; gpd.GeoDataFrame:\n    \"\"\"Converts the query results to a GeoPandas GeoDataFrame.\n\n    Args:\n        longitude_column (str): The name of the column representing longitude.\n        latitude_column (str): The name of the column representing latitude.\n        crs (str, optional): The coordinate reference system to use. Defaults to \"EPSG:4326\".\n\n    Returns:\n        gpd.GeoDataFrame: The query results as a GeoPandas GeoDataFrame.\n    \"\"\"\n\n    self.set_output(GeoParquet(longitude_column=longitude_column, latitude_column=latitude_column))\n    response = self.execute()\n    bytes_io = BytesIO(response.content)\n    # Read into parquet arrow table \n    table = pq.read_table(bytes_io)\n\n    gdf = gpd.GeoDataFrame.from_arrow(table)\n    gdf.set_crs(crs, inplace=True)\n    return gdf\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.to_geoparquet","title":"<code>to_geoparquet(file_path, longitude_column, latitude_column, streaming_chunk_size=1024 * 1024)</code>","text":"<p>Execute the query and save the results as a GeoParquet file</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def to_geoparquet(self, file_path: str, longitude_column: str, latitude_column: str, streaming_chunk_size: int = 1024*1024):\n    \"\"\"Execute the query and save the results as a GeoParquet file\"\"\"\n    self.set_output(GeoParquet(longitude_column=longitude_column, latitude_column=latitude_column))\n    response = self.execute()\n    with open(file_path, \"wb\") as f:\n        # Write the content of the response to a file\n        for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n            if chunk:  # skip keep-alive chunks\n                f.write(chunk)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.to_nd_netcdf","title":"<code>to_nd_netcdf(file_path, dimension_columns, streaming_chunk_size=1024 * 1024, force=False)</code>","text":"<p>Execute the query and save the results as an NdNetCDF file</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def to_nd_netcdf(self, file_path: str, dimension_columns: list[str], streaming_chunk_size: int = 1024*1024, force: bool = False):\n    \"\"\"Execute the query and save the results as an NdNetCDF file\"\"\"\n    if not force and not self.http_session.version_at_least(1, 5, 0):\n        raise Exception(\"NdNetCDF output format requires the Beacon Node version to be atleast 1.5.0 or higher\")\n    self.set_output(NdNetCDF(dimension_columns=dimension_columns))\n    response = self.execute()\n    with open(file_path, \"wb\") as f:\n        # Write the content of the response to a file\n        for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n            if chunk:  # skip keep-alive chunks\n                f.write(chunk)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.to_netcdf","title":"<code>to_netcdf(file_path, build_nc_local=True, streaming_chunk_size=1024 * 1024)</code>","text":"<p>Execute the query and save the results as an NetCDF file</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def to_netcdf(self, file_path: str, build_nc_local:bool = True, streaming_chunk_size: int = 1024*1024):\n    \"\"\"Execute the query and save the results as an NetCDF file\"\"\"\n    if build_nc_local:\n        df = self.to_pandas_dataframe()\n        xdf = df.to_xarray()\n        xdf.to_netcdf(file_path, mode=\"w\")\n    else:\n        self.set_output(NetCDF())  # Specify dimension columns as needed\n        response = self.execute()\n        with open(file_path, \"wb\") as f:\n            # Write the content of the response to a file\n            for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n                if chunk:  # skip keep-alive chunks\n                    f.write(chunk)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.to_odv","title":"<code>to_odv(odv_output, file_path)</code>","text":"<p>Exports the query results to an ODV file.</p> <p>Args:     odv_output (Odv): The ODV output format to use.     file_path (str): The path to the file where the ODV data will be saved.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def to_odv(self, odv_output: Odv, file_path: str):\n    \"\"\"Exports the query results to an ODV file.\n\n    Args:\n        odv_output (Odv): The ODV output format to use.\n        file_path (str): The path to the file where the ODV data will be saved.\n    \"\"\"\n    self.set_output(odv_output)\n    response = self.execute()\n    with open(file_path, \"wb\") as f:\n        # Write the content of the response to a file\n        f.write(response.content)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.to_pandas_dataframe","title":"<code>to_pandas_dataframe()</code>","text":"<p>Execute the query and return the results as a pandas DataFrame</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def to_pandas_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Execute the query and return the results as a pandas DataFrame\"\"\"\n    self.set_output(Parquet())\n    response = self.execute()\n    bytes_io = BytesIO(response.content)\n    return pd.read_parquet(bytes_io)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.to_parquet","title":"<code>to_parquet(file_path, streaming_chunk_size=1024 * 1024)</code>","text":"<p>Execute the query and save the results as a Parquet file</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def to_parquet(self, file_path: str, streaming_chunk_size: int = 1024*1024):\n    \"\"\"Execute the query and save the results as a Parquet file\"\"\"\n    self.set_output(Parquet())\n    response = self.execute()\n    with open(file_path, \"wb\") as f:\n        # Write the content of the response to a file\n        for chunk in response.iter_content(chunk_size=streaming_chunk_size):\n            if chunk:  # skip keep-alive chunks\n                f.write(chunk)\n</code></pre>"},{"location":"reference/query/#beacon_api.query.BaseQuery.to_xarray_dataset","title":"<code>to_xarray_dataset(dimension_columns, chunks=None, auto_cleanup=True, force=False)</code>","text":"<p>Converts the query results to an xarray Dataset with n-dimensional structure.</p> <p>Args:     dimension_columns (list[str]): The list of columns to use as dimensions in the xarray Dataset.</p> <p>Returns:     xarray.Dataset: The query results as an xarray Dataset.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def to_xarray_dataset(self, dimension_columns: List[str], chunks: Union[dict, None] = None, auto_cleanup=True, force=False) -&gt; xr.Dataset:\n    \"\"\"Converts the query results to an xarray Dataset with n-dimensional structure.\n\n    Args:\n        dimension_columns (list[str]): The list of columns to use as dimensions in the xarray Dataset.\n\n    Returns:\n        xarray.Dataset: The query results as an xarray Dataset.\n    \"\"\"\n    if not force and not self.http_session.version_at_least(1, 5, 0):\n        raise Exception(\"xarray dataset output requires the Beacon Node version to be atleast 1.5.0 or higher\")\n\n    # create tempfile for the netcdf file\n    fd, path = tempfile.mkstemp(suffix=\".nc\")\n    self.to_nd_netcdf(file_path=path, dimension_columns=dimension_columns,force=force)\n\n    ds = xr.open_dataset(path, chunks=chunks)\n    # register for cleanup the tempfile\n    if auto_cleanup:\n        atexit.register(lambda: os.path.exists(path) and os.remove(path))\n\n    return ds\n</code></pre>"},{"location":"reference/query/#json-query-builder","title":"JSON query builder","text":"<p>               Bases: <code>BaseQuery</code></p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>class JSONQuery(BaseQuery):\n    def __init__(self, http_session: BaseBeaconSession, _from: From):\n        print(f\"Creating JSONQuery with from: {_from}\")\n        super().__init__(http_session)\n        self._from = _from\n        self.selects = []\n        self.filters = []\n        self.sorts = []\n        self.distinct = None\n        self.limit = None\n        self.offset = None\n\n    def compile(self) -&gt; dict:\n        return {\n            \"select\": [s.to_dict() for s in self.selects],\n            \"filters\": [f.to_dict() for f in self.filters] if self.filters else None,\n            \"distinct\": self.distinct.to_dict() if self.distinct else None,\n            \"sort_by\": [s.to_dict() for s in self.sorts] if self.sorts else None,\n            \"limit\": self.limit,\n            \"offset\": self.offset,\n            **self._from.to_dict(),\n        }\n\n    def select(self, selects: List[Select]) -&gt; Self:\n        self.selects = selects\n        return self\n\n    def add_select(self, select: Select) -&gt; Self:\n        self.selects.append(select)\n        return self\n\n    def add_selects(self, selects: List[Select]) -&gt; Self:\n        \"\"\"Adds multiple select statements to the query.\n\n        Args:\n            selects (list[Select]): The select statements to add.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.selects.extend(selects)\n        return self\n\n    def add_select_column(self, column: str, alias: Optional[str] = None) -&gt; Self:\n        \"\"\"Adds a select column to the query.\n\n        Args:\n            column (str): The name of the column to select.\n            alias (str | None, optional): An optional alias for the column. Defaults to None.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.selects.append(SelectColumn(column=column, alias=alias))\n        return self\n\n    def add_select_columns(self, columns: List[Tuple[str, Optional[str]]]) -&gt; Self:\n        \"\"\"Adds multiple select columns to the query.\n\n        Args:\n            columns (List[Tuple[str, Optional[str]]]): A list of tuples containing column names and their aliases.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        for column, alias in columns:\n            self.selects.append(SelectColumn(column=column, alias=alias))\n        return self\n\n    def add_select_coalesced(self, mergeable_columns: List[str], alias: str) -&gt; Self:\n        \"\"\"Adds a coalesced select to the query.\n\n        Args:\n            mergeable_columns (list[str]): The columns to merge.\n            alias (str): The alias for the merged column.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        function_call = SelectFunction(\"coalesce\", args=[SelectColumn(column=col) for col in mergeable_columns], alias=alias)\n        self.selects.append(function_call)\n        return self\n\n    def filter(self, filters: List[Filter]) -&gt; Self:\n        \"\"\"Adds filters to the query.\n\n        Args:\n            filters (list[Filter]): The filters to add.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.filters = filters\n        return self\n\n    def add_filter(self, filter: Filter) -&gt; Self:\n        \"\"\"Adds a filter to the query.\n\n        Args:\n            filter (Filter): The filter to add.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.filters.append(filter)\n        return self\n\n    def add_bbox_filter(\n        self,\n        longitude_column: str,\n        latitude_column: str,\n        bbox: Tuple[float, float, float, float],\n    ) -&gt; Self:\n        \"\"\"Adds a bounding box filter to the query.\n\n        Args:\n            longitude_column (str): The name of the column for longitude.\n            latitude_column (str): The name of the column for latitude.\n            bbox (tuple[float, float, float, float]): The bounding box coordinates (min_lon, min_lat, max_lon, max_lat).\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.filters.append(\n            AndFilter(\n                filters=[\n                    RangeFilter(column=longitude_column, gt_eq=bbox[0]),\n                    RangeFilter(column=longitude_column, lt_eq=bbox[2]),\n                    RangeFilter(column=latitude_column, gt_eq=bbox[1]),\n                    RangeFilter(column=latitude_column, lt_eq=bbox[3]),\n                ]\n            )\n        )\n        return self\n\n    def add_polygon_filter(self, longitude_column: str, latitude_column: str, polygon: List[Tuple[float, float]]) -&gt; Self:\n        \"\"\"Adds a POLYGON filter to the query.\n\n        Args:\n            longitude_column (str): The name of the column for longitude.\n            latitude_column (str): The name of the column for latitude.\n            polygon (list[tuple[float, float]]): A list of (longitude, latitude) tuples defining the polygon.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.filters.append(PolygonFilter(longitude_column=longitude_column, latitude_column=latitude_column, polygon=polygon))\n        return self\n\n    def add_range_filter(\n        self,\n        column: str,\n        gt_eq: Union[str, int, float, datetime, None] = None,\n        lt_eq: Union[str, int, float, datetime, None] = None,\n        gt: Union[str, int, float, datetime, None] = None,\n        lt: Union[str, int, float, datetime, None] = None\n    ) -&gt; Self:\n        \"\"\"Adds a RANGE filter to the query.\n\n        Args:\n            column (str): The name of the column to filter.\n            gt_eq (str | int | float | datetime | None, optional): The lower bound for the range filter. Defaults to None.\n            lt_eq (str | int | float | datetime | None, optional): The upper bound for the range filter. Defaults to None.\n            gt (str | int | float | datetime | None, optional): The exclusive lower bound for the range filter. Defaults to None.\n            lt (str | int | float | datetime | None, optional): The exclusive upper bound for the range filter. Defaults to None.\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        if gt_eq is not None or lt_eq is not None:\n            self.filters.append(RangeFilter(column=column, gt_eq=gt_eq, lt_eq=lt_eq))\n\n        if gt is not None or lt is not None:\n            self.filters.append(ExclusiveRangeFilter(column=column, gt=gt, lt=lt))\n\n        return self\n\n    def add_equals_filter(\n        self, column: str, eq: Union[str, int, float, bool, datetime]\n    ) -&gt; Self:\n        \"\"\"Adds an EQUALS filter to the query.\n\n        Args:\n            column (str): The name of the column to filter.\n            eq (str | int | float | bool | datetime): The value to compare against.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.filters.append(EqualsFilter(column=column, eq=eq))\n        return self\n\n    def add_not_equals_filter(\n        self, column: str, neq: Union[str, int, float, bool, datetime]\n    ) -&gt; Self:\n        \"\"\"Adds a NOT EQUALS filter to the query.\n\n        Args:\n            column (str): The name of the column to filter.\n            neq (str | int | float | bool | datetime): The value to compare against.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n\n        self.filters.append(NotEqualsFilter(column=column, neq=neq))\n        return self\n\n    def add_is_null_filter(self, column: str) -&gt; Self:\n        \"\"\"Adds an IS NULL filter to the query.\n\n        Args:\n            column (str): The name of the column to filter.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.filters.append(FilterIsNull(column=column))\n        return self\n\n    def add_is_not_null_filter(self, column: str) -&gt; Self:\n        \"\"\"Adds an IS NOT NULL filter to the query.\n\n        Args:\n            column (str): The name of the column to filter.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.filters.append(IsNotNullFilter(column=column))\n        return self\n\n    def set_distinct(self, columns: list[str]) -&gt; Self:\n        \"\"\"Adds a DISTINCT clause to the query.\n\n        Args:\n            columns (list[str]): The list of columns to apply DISTINCT on.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.distinct = Distinct(columns=columns)\n        return self\n\n    def add_sort(self, column: str, ascending: bool = True) -&gt; Self:\n        \"\"\"Adds a SORT clause to the query.\n\n        Args:\n            column (str): The name of the column to sort by.\n            ascending (bool, optional): Whether to sort in ascending order. Defaults to True.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.sorts.append(SortColumn(column=column, ascending=ascending))\n        return self\n\n    def set_limit(self, limit: int) -&gt; Self:\n        \"\"\"Adds a LIMIT clause to the query.\n\n        Args:\n            limit (int): The maximum number of records to return.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.limit = limit\n        return self\n\n    def set_offset(self, offset: int) -&gt; Self:\n        \"\"\"Adds an OFFSET clause to the query.\n\n        Args:\n            offset (int): The number of records to skip.\n\n        Returns:\n            Self: The query builder instance.\n        \"\"\"\n        self.offset = offset\n        return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_bbox_filter","title":"<code>add_bbox_filter(longitude_column, latitude_column, bbox)</code>","text":"<p>Adds a bounding box filter to the query.</p> <p>Args:     longitude_column (str): The name of the column for longitude.     latitude_column (str): The name of the column for latitude.     bbox (tuple[float, float, float, float]): The bounding box coordinates (min_lon, min_lat, max_lon, max_lat).</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_bbox_filter(\n    self,\n    longitude_column: str,\n    latitude_column: str,\n    bbox: Tuple[float, float, float, float],\n) -&gt; Self:\n    \"\"\"Adds a bounding box filter to the query.\n\n    Args:\n        longitude_column (str): The name of the column for longitude.\n        latitude_column (str): The name of the column for latitude.\n        bbox (tuple[float, float, float, float]): The bounding box coordinates (min_lon, min_lat, max_lon, max_lat).\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.filters.append(\n        AndFilter(\n            filters=[\n                RangeFilter(column=longitude_column, gt_eq=bbox[0]),\n                RangeFilter(column=longitude_column, lt_eq=bbox[2]),\n                RangeFilter(column=latitude_column, gt_eq=bbox[1]),\n                RangeFilter(column=latitude_column, lt_eq=bbox[3]),\n            ]\n        )\n    )\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_equals_filter","title":"<code>add_equals_filter(column, eq)</code>","text":"<p>Adds an EQUALS filter to the query.</p> <p>Args:     column (str): The name of the column to filter.     eq (str | int | float | bool | datetime): The value to compare against.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_equals_filter(\n    self, column: str, eq: Union[str, int, float, bool, datetime]\n) -&gt; Self:\n    \"\"\"Adds an EQUALS filter to the query.\n\n    Args:\n        column (str): The name of the column to filter.\n        eq (str | int | float | bool | datetime): The value to compare against.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.filters.append(EqualsFilter(column=column, eq=eq))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_filter","title":"<code>add_filter(filter)</code>","text":"<p>Adds a filter to the query.</p> <p>Args:     filter (Filter): The filter to add.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_filter(self, filter: Filter) -&gt; Self:\n    \"\"\"Adds a filter to the query.\n\n    Args:\n        filter (Filter): The filter to add.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.filters.append(filter)\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_is_not_null_filter","title":"<code>add_is_not_null_filter(column)</code>","text":"<p>Adds an IS NOT NULL filter to the query.</p> <p>Args:     column (str): The name of the column to filter.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_is_not_null_filter(self, column: str) -&gt; Self:\n    \"\"\"Adds an IS NOT NULL filter to the query.\n\n    Args:\n        column (str): The name of the column to filter.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.filters.append(IsNotNullFilter(column=column))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_is_null_filter","title":"<code>add_is_null_filter(column)</code>","text":"<p>Adds an IS NULL filter to the query.</p> <p>Args:     column (str): The name of the column to filter.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_is_null_filter(self, column: str) -&gt; Self:\n    \"\"\"Adds an IS NULL filter to the query.\n\n    Args:\n        column (str): The name of the column to filter.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.filters.append(FilterIsNull(column=column))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_not_equals_filter","title":"<code>add_not_equals_filter(column, neq)</code>","text":"<p>Adds a NOT EQUALS filter to the query.</p> <p>Args:     column (str): The name of the column to filter.     neq (str | int | float | bool | datetime): The value to compare against.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_not_equals_filter(\n    self, column: str, neq: Union[str, int, float, bool, datetime]\n) -&gt; Self:\n    \"\"\"Adds a NOT EQUALS filter to the query.\n\n    Args:\n        column (str): The name of the column to filter.\n        neq (str | int | float | bool | datetime): The value to compare against.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n\n    self.filters.append(NotEqualsFilter(column=column, neq=neq))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_polygon_filter","title":"<code>add_polygon_filter(longitude_column, latitude_column, polygon)</code>","text":"<p>Adds a POLYGON filter to the query.</p> <p>Args:     longitude_column (str): The name of the column for longitude.     latitude_column (str): The name of the column for latitude.     polygon (list[tuple[float, float]]): A list of (longitude, latitude) tuples defining the polygon.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_polygon_filter(self, longitude_column: str, latitude_column: str, polygon: List[Tuple[float, float]]) -&gt; Self:\n    \"\"\"Adds a POLYGON filter to the query.\n\n    Args:\n        longitude_column (str): The name of the column for longitude.\n        latitude_column (str): The name of the column for latitude.\n        polygon (list[tuple[float, float]]): A list of (longitude, latitude) tuples defining the polygon.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.filters.append(PolygonFilter(longitude_column=longitude_column, latitude_column=latitude_column, polygon=polygon))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_range_filter","title":"<code>add_range_filter(column, gt_eq=None, lt_eq=None, gt=None, lt=None)</code>","text":"<p>Adds a RANGE filter to the query.</p> <p>Args:     column (str): The name of the column to filter.     gt_eq (str | int | float | datetime | None, optional): The lower bound for the range filter. Defaults to None.     lt_eq (str | int | float | datetime | None, optional): The upper bound for the range filter. Defaults to None.     gt (str | int | float | datetime | None, optional): The exclusive lower bound for the range filter. Defaults to None.     lt (str | int | float | datetime | None, optional): The exclusive upper bound for the range filter. Defaults to None. Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_range_filter(\n    self,\n    column: str,\n    gt_eq: Union[str, int, float, datetime, None] = None,\n    lt_eq: Union[str, int, float, datetime, None] = None,\n    gt: Union[str, int, float, datetime, None] = None,\n    lt: Union[str, int, float, datetime, None] = None\n) -&gt; Self:\n    \"\"\"Adds a RANGE filter to the query.\n\n    Args:\n        column (str): The name of the column to filter.\n        gt_eq (str | int | float | datetime | None, optional): The lower bound for the range filter. Defaults to None.\n        lt_eq (str | int | float | datetime | None, optional): The upper bound for the range filter. Defaults to None.\n        gt (str | int | float | datetime | None, optional): The exclusive lower bound for the range filter. Defaults to None.\n        lt (str | int | float | datetime | None, optional): The exclusive upper bound for the range filter. Defaults to None.\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    if gt_eq is not None or lt_eq is not None:\n        self.filters.append(RangeFilter(column=column, gt_eq=gt_eq, lt_eq=lt_eq))\n\n    if gt is not None or lt is not None:\n        self.filters.append(ExclusiveRangeFilter(column=column, gt=gt, lt=lt))\n\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_select_coalesced","title":"<code>add_select_coalesced(mergeable_columns, alias)</code>","text":"<p>Adds a coalesced select to the query.</p> <p>Args:     mergeable_columns (list[str]): The columns to merge.     alias (str): The alias for the merged column.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_select_coalesced(self, mergeable_columns: List[str], alias: str) -&gt; Self:\n    \"\"\"Adds a coalesced select to the query.\n\n    Args:\n        mergeable_columns (list[str]): The columns to merge.\n        alias (str): The alias for the merged column.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    function_call = SelectFunction(\"coalesce\", args=[SelectColumn(column=col) for col in mergeable_columns], alias=alias)\n    self.selects.append(function_call)\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_select_column","title":"<code>add_select_column(column, alias=None)</code>","text":"<p>Adds a select column to the query.</p> <p>Args:     column (str): The name of the column to select.     alias (str | None, optional): An optional alias for the column. Defaults to None.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_select_column(self, column: str, alias: Optional[str] = None) -&gt; Self:\n    \"\"\"Adds a select column to the query.\n\n    Args:\n        column (str): The name of the column to select.\n        alias (str | None, optional): An optional alias for the column. Defaults to None.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.selects.append(SelectColumn(column=column, alias=alias))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_select_columns","title":"<code>add_select_columns(columns)</code>","text":"<p>Adds multiple select columns to the query.</p> <p>Args:     columns (List[Tuple[str, Optional[str]]]): A list of tuples containing column names and their aliases.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_select_columns(self, columns: List[Tuple[str, Optional[str]]]) -&gt; Self:\n    \"\"\"Adds multiple select columns to the query.\n\n    Args:\n        columns (List[Tuple[str, Optional[str]]]): A list of tuples containing column names and their aliases.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    for column, alias in columns:\n        self.selects.append(SelectColumn(column=column, alias=alias))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_selects","title":"<code>add_selects(selects)</code>","text":"<p>Adds multiple select statements to the query.</p> <p>Args:     selects (list[Select]): The select statements to add.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_selects(self, selects: List[Select]) -&gt; Self:\n    \"\"\"Adds multiple select statements to the query.\n\n    Args:\n        selects (list[Select]): The select statements to add.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.selects.extend(selects)\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.add_sort","title":"<code>add_sort(column, ascending=True)</code>","text":"<p>Adds a SORT clause to the query.</p> <p>Args:     column (str): The name of the column to sort by.     ascending (bool, optional): Whether to sort in ascending order. Defaults to True.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def add_sort(self, column: str, ascending: bool = True) -&gt; Self:\n    \"\"\"Adds a SORT clause to the query.\n\n    Args:\n        column (str): The name of the column to sort by.\n        ascending (bool, optional): Whether to sort in ascending order. Defaults to True.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.sorts.append(SortColumn(column=column, ascending=ascending))\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.filter","title":"<code>filter(filters)</code>","text":"<p>Adds filters to the query.</p> <p>Args:     filters (list[Filter]): The filters to add.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def filter(self, filters: List[Filter]) -&gt; Self:\n    \"\"\"Adds filters to the query.\n\n    Args:\n        filters (list[Filter]): The filters to add.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.filters = filters\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.set_distinct","title":"<code>set_distinct(columns)</code>","text":"<p>Adds a DISTINCT clause to the query.</p> <p>Args:     columns (list[str]): The list of columns to apply DISTINCT on.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def set_distinct(self, columns: list[str]) -&gt; Self:\n    \"\"\"Adds a DISTINCT clause to the query.\n\n    Args:\n        columns (list[str]): The list of columns to apply DISTINCT on.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.distinct = Distinct(columns=columns)\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.set_limit","title":"<code>set_limit(limit)</code>","text":"<p>Adds a LIMIT clause to the query.</p> <p>Args:     limit (int): The maximum number of records to return.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def set_limit(self, limit: int) -&gt; Self:\n    \"\"\"Adds a LIMIT clause to the query.\n\n    Args:\n        limit (int): The maximum number of records to return.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.limit = limit\n    return self\n</code></pre>"},{"location":"reference/query/#beacon_api.query.JSONQuery.set_offset","title":"<code>set_offset(offset)</code>","text":"<p>Adds an OFFSET clause to the query.</p> <p>Args:     offset (int): The number of records to skip.</p> <p>Returns:     Self: The query builder instance.</p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>def set_offset(self, offset: int) -&gt; Self:\n    \"\"\"Adds an OFFSET clause to the query.\n\n    Args:\n        offset (int): The number of records to skip.\n\n    Returns:\n        Self: The query builder instance.\n    \"\"\"\n    self.offset = offset\n    return self\n</code></pre>"},{"location":"reference/query/#sql-query-wrapper","title":"SQL query wrapper","text":"<p>               Bases: <code>BaseQuery</code></p> Source code in <code>beacon_api/query/__init__.py</code> <pre><code>class SQLQuery(BaseQuery):\n    def __init__(self, http_session: BaseBeaconSession, query: str):\n        super().__init__(http_session)\n        self.query = query\n\n    def compile(self) -&gt; dict:\n        return {\"sql\": self.query}\n</code></pre>"},{"location":"reference/table/","title":"Table reference","text":"<p>Represents a data table available on the Beacon Node.</p> Source code in <code>beacon_api/table.py</code> <pre><code>class DataTable:\n    \"\"\"Represents a data table available on the Beacon Node.\"\"\"\n\n    # Constructor for DataTable\n    def __init__(self, http_session: BaseBeaconSession, table_name: str):\n        self.http_session = http_session\n        self.table_name = table_name\n\n        # Now query the server for the table type and description\n        # api/table-config?table_name={table_name}\n        response = self.http_session.get(\"/api/table-config\", params={\"table_name\": table_name})\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get table config: {response.text}\")\n        table_config = response.json()\n        self.table_type = table_config.get(\"table_type\", \"unknown\")\n        self.description = table_config.get(\"description\", None)\n\n    def get_table_description(self) -&gt; str:\n        \"\"\"Get the description of the table\"\"\"\n        return self.description if self.description else \"No description available\"    \n\n    def get_table_schema(self) -&gt; dict[str, type]:\n        \"\"\"Get the schema of the table\"\"\"\n        pa_schema = self.get_table_schema_arrow()\n        if pa_schema:\n            schema_dict = pa_schema\n            return schema_dict\n        else:\n            raise Exception(\"Failed to retrieve table schema\")\n\n    def get_table_schema_arrow(self) -&gt; pa.Schema:\n        \"\"\"Get the schema of the table in Arrow format\"\"\"\n        response = self.http_session.get(\"/api/table-schema\", params={\"table_name\": self.table_name})\n\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get table schema: {response.text}\")\n\n        schema_data = response.json()\n        fields = []\n\n        for field in schema_data['fields']:\n            field_type = field['data_type']\n\n            if isinstance(field_type, str):\n                fields.append(pa.field(field['name'], field_type))\n\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Second\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('s')))\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Millisecond\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('ms')))\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Microsecond\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('us')))\n            elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Nanosecond\", None]:\n                fields.append(pa.field(field['name'], pa.timestamp('ns')))\n\n            else:\n                raise Exception(f\"Unsupported data type for field {field['name']}: {field_type}\")\n\n        return pa.schema(fields)\n\n    def get_table_type(self) -&gt; Union[dict, str]:\n        \"\"\"Get the type of the table\"\"\"\n        return self.table_type\n\n\n    def subset(self, longitude_column: str, latitude_column: str, time_column: str, depth_column: str, columns: list[str],\n                         bbox: Optional[tuple[float, float, float, float]] = None,\n                         depth_range: Optional[tuple[float, float]] = None,\n                         time_range: Optional[tuple[datetime.datetime, datetime.datetime]] = None) -&gt; JSONQuery:\n        \"\"\"\n        Create a query to subset the table based on the provided parameters.\n\n        Args:\n            longitude_column: Name of the column containing longitude values.\n            latitude_column: Name of the column containing latitude values.\n            time_column: Name of the column containing time values.\n            depth_column: Name of the column containing depth values.\n            columns: List of additional columns to include in the query.\n            bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).\n            depth_range: Optional range for depth defined as (min_depth, max_depth).\n            time_range: Optional range for time defined as (start_time, end_time).\n        Returns\n            A Query object that can be executed to retrieve the subset of data.\n        \"\"\"\n        query = self.query()\n        query.add_select_column(longitude_column)\n        query.add_select_column(latitude_column)\n        query.add_select_column(time_column)\n        query.add_select_column(depth_column)\n        for column in columns:\n            query.add_select_column(column)\n        if bbox:\n            query.add_filter(AndFilter([\n                RangeFilter(longitude_column, bbox[0], bbox[2]),\n                RangeFilter(latitude_column, bbox[1], bbox[3])\n            ]))\n        if depth_range:\n            query.add_filter(RangeFilter(depth_column, depth_range[0], depth_range[1]))\n        if time_range:\n            query.add_filter(RangeFilter(time_column, time_range[0].strftime(\"%Y-%m-%dT%H:%M:%S\"), time_range[1].strftime(\"%Y-%m-%dT%H:%M:%S\")))\n        return query\n\n    def query(self) -&gt; JSONQuery:\n        \"\"\"Create a new query for the selected table.\n        The query can then be built using the Query methods.\n        Returns:\n            JSONQuery: A new query object.\n        \"\"\"\n        return JSONQuery(self.http_session, _from=FromTable(self.table_name))\n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.get_table_description","title":"<code>get_table_description()</code>","text":"<p>Get the description of the table</p> Source code in <code>beacon_api/table.py</code> <pre><code>def get_table_description(self) -&gt; str:\n    \"\"\"Get the description of the table\"\"\"\n    return self.description if self.description else \"No description available\"    \n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.get_table_schema","title":"<code>get_table_schema()</code>","text":"<p>Get the schema of the table</p> Source code in <code>beacon_api/table.py</code> <pre><code>def get_table_schema(self) -&gt; dict[str, type]:\n    \"\"\"Get the schema of the table\"\"\"\n    pa_schema = self.get_table_schema_arrow()\n    if pa_schema:\n        schema_dict = pa_schema\n        return schema_dict\n    else:\n        raise Exception(\"Failed to retrieve table schema\")\n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.get_table_schema_arrow","title":"<code>get_table_schema_arrow()</code>","text":"<p>Get the schema of the table in Arrow format</p> Source code in <code>beacon_api/table.py</code> <pre><code>def get_table_schema_arrow(self) -&gt; pa.Schema:\n    \"\"\"Get the schema of the table in Arrow format\"\"\"\n    response = self.http_session.get(\"/api/table-schema\", params={\"table_name\": self.table_name})\n\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get table schema: {response.text}\")\n\n    schema_data = response.json()\n    fields = []\n\n    for field in schema_data['fields']:\n        field_type = field['data_type']\n\n        if isinstance(field_type, str):\n            fields.append(pa.field(field['name'], field_type))\n\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Second\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('s')))\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Millisecond\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('ms')))\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Microsecond\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('us')))\n        elif isinstance(field_type, dict) and field_type.get(\"Timestamp\") == [\"Nanosecond\", None]:\n            fields.append(pa.field(field['name'], pa.timestamp('ns')))\n\n        else:\n            raise Exception(f\"Unsupported data type for field {field['name']}: {field_type}\")\n\n    return pa.schema(fields)\n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.get_table_type","title":"<code>get_table_type()</code>","text":"<p>Get the type of the table</p> Source code in <code>beacon_api/table.py</code> <pre><code>def get_table_type(self) -&gt; Union[dict, str]:\n    \"\"\"Get the type of the table\"\"\"\n    return self.table_type\n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.query","title":"<code>query()</code>","text":"<p>Create a new query for the selected table. The query can then be built using the Query methods. Returns:     JSONQuery: A new query object.</p> Source code in <code>beacon_api/table.py</code> <pre><code>def query(self) -&gt; JSONQuery:\n    \"\"\"Create a new query for the selected table.\n    The query can then be built using the Query methods.\n    Returns:\n        JSONQuery: A new query object.\n    \"\"\"\n    return JSONQuery(self.http_session, _from=FromTable(self.table_name))\n</code></pre>"},{"location":"reference/table/#beacon_api.table.DataTable.subset","title":"<code>subset(longitude_column, latitude_column, time_column, depth_column, columns, bbox=None, depth_range=None, time_range=None)</code>","text":"<p>Create a query to subset the table based on the provided parameters.</p> <p>Args:     longitude_column: Name of the column containing longitude values.     latitude_column: Name of the column containing latitude values.     time_column: Name of the column containing time values.     depth_column: Name of the column containing depth values.     columns: List of additional columns to include in the query.     bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).     depth_range: Optional range for depth defined as (min_depth, max_depth).     time_range: Optional range for time defined as (start_time, end_time). Returns     A Query object that can be executed to retrieve the subset of data.</p> Source code in <code>beacon_api/table.py</code> <pre><code>def subset(self, longitude_column: str, latitude_column: str, time_column: str, depth_column: str, columns: list[str],\n                     bbox: Optional[tuple[float, float, float, float]] = None,\n                     depth_range: Optional[tuple[float, float]] = None,\n                     time_range: Optional[tuple[datetime.datetime, datetime.datetime]] = None) -&gt; JSONQuery:\n    \"\"\"\n    Create a query to subset the table based on the provided parameters.\n\n    Args:\n        longitude_column: Name of the column containing longitude values.\n        latitude_column: Name of the column containing latitude values.\n        time_column: Name of the column containing time values.\n        depth_column: Name of the column containing depth values.\n        columns: List of additional columns to include in the query.\n        bbox: Optional bounding box defined as (min_longitude, min_latitude, max_longitude, max_latitude).\n        depth_range: Optional range for depth defined as (min_depth, max_depth).\n        time_range: Optional range for time defined as (start_time, end_time).\n    Returns\n        A Query object that can be executed to retrieve the subset of data.\n    \"\"\"\n    query = self.query()\n    query.add_select_column(longitude_column)\n    query.add_select_column(latitude_column)\n    query.add_select_column(time_column)\n    query.add_select_column(depth_column)\n    for column in columns:\n        query.add_select_column(column)\n    if bbox:\n        query.add_filter(AndFilter([\n            RangeFilter(longitude_column, bbox[0], bbox[2]),\n            RangeFilter(latitude_column, bbox[1], bbox[3])\n        ]))\n    if depth_range:\n        query.add_filter(RangeFilter(depth_column, depth_range[0], depth_range[1]))\n    if time_range:\n        query.add_filter(RangeFilter(time_column, time_range[0].strftime(\"%Y-%m-%dT%H:%M:%S\"), time_range[1].strftime(\"%Y-%m-%dT%H:%M:%S\")))\n    return query\n</code></pre>"},{"location":"using/datasets/","title":"Working with datasets","text":"<p>Sometimes you already know the precise file or object store path you want to analyze. Instead of going through a logical table, you can treat that file as a <code>Dataset</code> and issue a JSON query directly against it.</p>"},{"location":"using/datasets/#enumerate-datasets-beacon-140","title":"Enumerate datasets (Beacon \u2265 1.4.0)","text":"<p><code>Client.list_datasets()</code> calls <code>/api/list-datasets</code> and returns a dictionary of <code>Dataset</code> helpers keyed by their <code>file_path</code>:</p> <pre><code>from beacon_api import Client\n\nclient = Client(\"https://beacon.example.com\")\nraw_datasets = client.list_datasets(pattern=\"**/*.parquet\", limit=20)\nprint(f\"Discovered {len(raw_datasets)} eligible files\")\n</code></pre> <p>Each value in the dictionary is a <code>Dataset</code> instance.</p> <pre><code>first = next(iter(raw_datasets.values()))\nprint(first.get_file_format(), first.get_file_name())\nprint(first.get_file_path())\n</code></pre>"},{"location":"using/datasets/#inspect-schema","title":"Inspect schema","text":"<p><code>Dataset.get_schema()</code> makes the same <code>pyarrow.Schema</code> request that tables use, but it is scoped to the exact file you selected:</p> <pre><code>schema = first.get_schema()\nfor field in schema:\n    print(field.name, field.type)\n</code></pre> <p>Use this information to decide which columns to select before running a heavy query.</p>"},{"location":"using/datasets/#build-a-query-from-a-dataset","title":"Build a query from a dataset","text":"<p>Every <code>Dataset</code> exposes <code>.query()</code> which returns the familiar <code>JSONQuery</code> builder:</p> <pre><code>dataset_query = (\n    first\n    .query()\n    .add_select_column(\"lon\", alias=\"longitude\")\n    .add_select_column(\"lat\", alias=\"latitude\")\n    .add_range_filter(\"time\", \"2024-01-01T00:00:00\", \"2024-06-30T23:59:59\")\n)\n\nsubset = dataset_query.to_pandas_dataframe()\n</code></pre> <p>The method understands a couple of format-specific options:</p> <ul> <li>CSV datasets accept <code>delimiter=\",\"</code> (or any other single-character delimiter).</li> <li>Zarr datasets accept <code>statistics_columns=[\"col_a\", \"col_b\"]</code> so you can hint which axes include statistics.</li> </ul> <p>Pass these keyword arguments directly to <code>.query()</code>:</p> <pre><code>csv_file = client.list_datasets(pattern=\"*.csv\")[\"raw.csv\"]\nquery = csv_file.query(delimiter=\";\")\n</code></pre> <pre><code>zarr_file = client.list_datasets(pattern=\"*/zarr.json\")[\"profiles.zarr/zarr.json\"]\nquery = zarr_file.query(statistics_columns=[\"time\", \"depth\"])\n</code></pre> <p>Everything else\u2014selects, filters, sorts, exports\u2014matches the <code>DataTable</code> workflow outlined in the query guide. Datasets simply skip the logical abstraction layer when you want to target files directly.</p>"},{"location":"using/exploring/","title":"Exploring the Beacon Data Lake","text":"<p>This guide shows how to inspect/query a Beacon Node, discover the available data, and subset slices for local analysis. All snippets are built directly on top of the public SDK classes so you can paste them into scripts or notebooks as-is.</p>"},{"location":"using/exploring/#connect-and-verify","title":"Connect and verify","text":"<pre><code>from beacon_api import Client\n\nclient = Client(\"https://beacon.example.com\")\nclient.check_status()  # probes /api/health and prints the version\n\ninfo = client.get_server_info()\nprint(info[\"beacon_version\"], info.get(\"extensions\"))\n</code></pre> <p>Troubleshooting connectivity</p> <p>The client automatically prefixes relative URLs with the base URL. If you see <code>Failed to connect to server</code>, double-check the base URL and whether your token grants access to <code>/api/health</code>.</p>"},{"location":"using/exploring/#discover-tables","title":"Discover tables","text":"<p><code>list_tables()</code> returns a mapping of table names to <code>DataTable</code> helpers with cached metadata. Iterate to learn what each collection represents:</p> <pre><code>tables = client.list_tables()\nfor name, table in tables.items():\n    print(f\"{name} \u2192 {table.get_table_type()} :: {table.get_table_description()}\")\n</code></pre> <p>Grab one table\u2014<code>default</code> exists on every Beacon deployment\u2014and inspect its schema:</p> <pre><code>default_table = tables[\"default\"]\n\nschema_arrow = default_table.get_table_schema_arrow()\nfor field in schema_arrow:\n    print(f\"{field.name}: {field.type}\")\n</code></pre> <p>Use <code>get_table_schema()</code> when you want a PyArrow <code>Schema</code> object you can re-use (for instance, to validate a DataFrame before upload).</p>"},{"location":"using/exploring/#sample-data-quickly","title":"Sample data quickly","text":"<p>Need a glance at the data without hand-writing longitude/latitude filters? <code>DataTable.subset()</code> configures a <code>JSONQuery</code> with a bounding box, depth range, and time window:</p> <pre><code>sample = default_table.subset(\n    longitude_column=\"LONGITUDE\",\n    latitude_column=\"LATITUDE\",\n    time_column=\"JULD\",\n    depth_column=\"PRES\",\n    columns=[\"TEMP\", \"PSAL\"],\n    bbox=(-20, 40, -10, 50),\n    depth_range=(0, 25),\n)\n\npreview = sample.to_pandas_dataframe().head()\n</code></pre> <p>Because <code>subset()</code> returns a normal <code>JSONQuery</code>, you can tack on additional selects/filters before executing it.</p>"},{"location":"using/exploring/#browse-individual-datasets-beacon-14","title":"Browse individual datasets (Beacon \u2265 1.4)","text":"<p>When the Beacon Node exposes <code>/api/list-datasets</code>, you can work with files directly instead of logical tables:</p> <pre><code>datasets = client.list_datasets(pattern=\"**/*.parquet\", limit=10)\nfirst_file = next(iter(datasets.values()))\n\nprint(first_file.get_file_path(), first_file.get_file_format())\ndataset_schema = first_file.get_schema()\n</code></pre> <p>Once you have a <code>Dataset</code>, spin up a query exactly the same way as with tables:</p> <pre><code>dataset_query = first_file.query()\ndataset_df = (\n    dataset_query\n    .add_select_column(\"lon\", alias=\"longitude\")\n    .add_select_column(\"lat\", alias=\"latitude\")\n    .add_range_filter(\"time\", \"2023-01-01T00:00:00\", \"2023-12-31T23:59:59\")\n    .to_pandas_dataframe()\n)\n</code></pre>"},{"location":"using/exploring/#go-deeper-with-explain-and-export-helpers","title":"Go deeper with explain and export helpers","text":"<ul> <li><code>query.explain()</code> calls <code>/api/explain-query</code> so you can see exactly how the Beacon Node plans to execute the request.</li> <li><code>query.to_geo_pandas_dataframe(longitude, latitude)</code> materializes a <code>GeoDataFrame</code> complete with CRS.</li> <li><code>query.to_xarray_dataset([\"JULD\", \"PRES\"])</code> yields an n-dimensional dataset perfect for scientific workflows.</li> <li><code>query.to_parquet(\"subset.parquet\")</code>, <code>query.to_nd_netcdf(\"subset_nd.nc\", [\"JULD\", \"DEPTH\"])</code>, or <code>query.to_zarr(\"subset.zarr\")</code> stream results directly to disk.</li> </ul> <p>With these building blocks you can confidently explore any Beacon Data Lake, whether you prefer notebooks, scripts, or dashboards.</p>"},{"location":"using/querying/","title":"Querying the Beacon Data Lake","text":"<p>The SDK exposes two complementary query builders:</p> <ol> <li><code>JSONQuery</code> \u2013 a fluent, strongly-typed builder generated from a table or dataset via <code>.query()</code>.</li> <li><code>SQLQuery</code> \u2013 created through <code>Client.sql_query(\"SELECT ...\")</code> when you already have raw SQL.</li> </ol> <p>This page highlights the JSON builder because it reflects the method names living in <code>beacon_api.query.JSONQuery</code>.</p>"},{"location":"using/querying/#create-a-json-query","title":"Create a JSON query","text":"<p>Start from a table (or dataset) and chain builder calls. You can add selects first, then filters, then any optional clauses such as sort or distinct.</p> <pre><code>from beacon_api import Client\n\nclient = Client(\"https://beacon.example.com\")\nstations = client.list_tables()[\"default\"]\n\nquery = (\n    stations\n    .query()\n    .add_select_column(\"LONGITUDE\")\n    .add_select_column(\"LATITUDE\")\n    .add_select_column(\"JULD\")\n    .add_select_column(\"TEMP\", alias=\"temperature_c\")\n    .add_range_filter(\"JULD\", \"2024-01-01T00:00:00\", \"2024-06-30T23:59:59\")\n)\n</code></pre> <p>Datasets behave the same</p> <p>Every <code>Dataset</code> helper exposes <code>.query()</code> too. Whether you start from <code>tables[\"default\"]</code> or <code>client.list_datasets()[\"/data/foo.parquet\"]</code>, the returned object is the same <code>JSONQuery</code> class.</p>"},{"location":"using/querying/#selecting-columns-and-expressions","title":"Selecting columns and expressions","text":"<ul> <li><code>add_select_column(column, alias=None)</code> \u2013 add one column at a time.</li> <li><code>add_select_column(column, alias=None)</code> \u2013 add one column at a time (call repeatedly to build your projection).</li> <li><code>add_select_coalesced([\"col_a\", \"col_b\"], alias=\"preferred\")</code> \u2013 build a COALESCE expression server-side.</li> <li><code>add_selects([...])</code> \u2013 append fully-specified <code>Select</code> nodes when you need lower-level control.</li> </ul> <p>You can also use helpers from <code>beacon_api.query.Functions</code> to derive columns. For example, concatenate voyage identifiers or cast a numeric field:</p> <pre><code>from beacon_api.query import Functions\n\nquery = (\n    query\n    .add_select(Functions.concat([\"CRUISE\", \"STATION\"], alias=\"cast_id\"))\n    .add_select(Functions.try_cast_to_type(\"TEMP\", to_type=\"float64\", alias=\"temp_float\"))\n)\n</code></pre> <p>Warning</p> <p>Make sure the columns you reference in filters are also present in the select list. When you rename a column via <code>alias</code>, use that alias in your filters.</p>"},{"location":"using/querying/#adding-filters","title":"Adding filters","text":"<p>JSON queries support the same filter primitives as the Beacon API:</p> <pre><code>filtered = (\n    query\n    .add_equals_filter(\"DATA_TYPE\", \"CTD\")\n    .add_not_equals_filter(\"VESSEL\", \"TEST\")\n    .add_range_filter(\"PRES\", 0, 10)\n    .add_is_not_null_filter(\"TEMP\")\n    .add_bbox_filter(\"LONGITUDE\", \"LATITUDE\", bbox=(-20, 40, -10, 55))\n)\n</code></pre> <p>For custom boolean logic you can compose <code>AndFilter</code>/<code>OrFilter</code> nodes manually and pass them to <code>add_filter()</code>:</p> <pre><code>from beacon_api.query import AndFilter, RangeFilter\n\nfiltered = filtered.add_filter(\n    AndFilter([\n        RangeFilter(\"TEMP\", gt_eq=-2, lt_eq=35),\n        RangeFilter(\"PSAL\", gt_eq=30, lt_eq=40),\n    ])\n)\n</code></pre> <p>Geospatial workflows are covered via <code>add_polygon_filter(longitude_column, latitude_column, polygon)</code> which accepts any closed polygon expressed as a list of <code>(lon, lat)</code> tuples.</p>"},{"location":"using/querying/#distinct-and-sorting","title":"Distinct and sorting","text":"<p>Use <code>set_distinct([\"COLUMN\"])</code> to deduplicate rows before export. Sorting is handled per column:</p> <pre><code>query = (\n    query\n    .set_distinct([\"CRUISE\", \"STATION\"])\n    .add_sort(\"JULD\", ascending=True)\n    .add_sort(\"DEPTH\", ascending=False)\n)\n</code></pre>"},{"location":"using/querying/#inspect-the-plan","title":"Inspect the plan","text":"<p>Call <code>query.explain()</code> to inspect the Beacon execution plan before spending time/materializing the results. For ad-hoc debugging you can also call <code>query.execute()</code> to get the raw <code>requests.Response</code> object and inspect headers or bytes.</p>"},{"location":"using/querying/#materialize-results","title":"Materialize results","text":"<p>Every builder inherits from <code>BaseQuery</code>, so all outputs are available regardless of whether you built JSON or SQL:</p> Method Description <code>to_pandas_dataframe()</code> Executes the query and returns a Pandas <code>DataFrame</code>. <code>to_geo_pandas_dataframe(lon_col, lat_col, crs=\"EPSG:4326\")</code> Builds a <code>GeoDataFrame</code> and sets the CRS for you. <code>to_dask_dataframe(temp_name=\"temp.parquet\")</code> Streams results into an in-memory Parquet file and returns a lazy <code>dask.dataframe</code>. <code>to_xarray_dataset(dimension_columns, chunks=None)</code> Converts the results into an xarray <code>Dataset</code>; handy for multidimensional grids. <code>to_parquet(path)</code> / <code>to_geoparquet(path, lon, lat)</code> / <code>to_arrow(path)</code> / <code>to_csv(path)</code> Writes the streamed response directly to disk in the requested format. <code>to_netcdf(path)</code> Builds a local NetCDF file via Pandas \u2192 xarray. <code>to_nd_netcdf(path, dimension_columns)</code> Requests the Beacon server to emit NdNetCDF directly (requires Beacon \u2265 1.5.0). <code>to_zarr(path)</code> Converts the results to xarray and persists them as a Zarr store. <code>to_odv(Odv(...), path)</code> Emits an Ocean Data View export when the server supports it."},{"location":"using/querying/#example-gallery","title":"Example gallery","text":""},{"location":"using/querying/#dataset-powered-dask-pipelines","title":"Dataset-powered Dask pipelines","text":"<p>When you already know the file/URI you want, start from <code>Dataset</code> helpers and stream lazily with Dask.</p> <pre><code>datasets = client.list_datasets(pattern=\"**/*.parquet\", limit=1)\ndataset = next(iter(datasets.values()))\n\ndask_query = (\n    dataset\n    .query()\n    .add_select_column(\"lon\", alias=\"longitude\")\n    .add_select_column(\"lat\", alias=\"latitude\")\n    .add_select_column(\"time\")\n    .add_select_column(\"temperature\")\n    .add_range_filter(\"time\", \"2023-01-01T00:00:00\", \"2023-12-31T23:59:59\")\n)\n\ndask_df = dask_query.to_dask_dataframe()\n\nprint(dask_df.head())\n</code></pre>"},{"location":"using/querying/#sql-equivalent","title":"SQL equivalent","text":"<p>Prefer SQL? Build once in SQL, then call the same output helpers.</p> <pre><code>sql = client.sql_query(\n    \"\"\"\n    SELECT LONGITUDE, LATITUDE, JULD, TEMP AS temperature_c\n    FROM argo\n    WHERE DATA_TYPE = 'CTD'\n      AND JULD BETWEEN '2024-01-01 00:00:00'\n                    AND '2024-06-30 23:59:59'\n      AND PRES BETWEEN 0 AND 50\n    ORDER BY JULD ASC\n    \"\"\"\n)\n\nsql.to_parquet(\"ctd_slice.parquet\")\n</code></pre>"},{"location":"using/querying/#pandas-first-examples","title":"Pandas-first examples","text":"<p>The snippets below all end with <code>to_pandas_dataframe()</code> so you can copy them straight into notebooks.</p>"},{"location":"using/querying/#custom-column-selection-range-filters","title":"Custom column selection + range filters","text":"<pre><code>tables = client.list_tables()\ncollection = tables[\"default\"]\n\ndf = (\n    collection\n    .query()\n    .add_select_column(\"CRUISE\")\n    .add_select_column(\"STATION\")\n    .add_select_column(\"JULD\")\n    .add_select_column(\"TEMP\", alias=\"temperature_c\")\n    .add_range_filter(\"JULD\", \"2024-01-01T00:00:00\", \"2024-03-01T00:00:00\")\n    .add_range_filter(\"PRES\", 0, 20)\n    .to_pandas_dataframe()\n)\n</code></pre>"},{"location":"using/querying/#distinct-voyages-with-equality-filters","title":"Distinct voyages with equality filters","text":"<pre><code>voyages = (\n    collection\n    .query()\n    .add_select_column(\"CRUISE\")\n    .add_select_column(\"STATION\")\n    .add_select_column(\"DATA_TYPE\")\n    .add_equals_filter(\"DATA_TYPE\", \"PROFILER\")\n    .set_distinct([\"CRUISE\", \"STATION\", \"DATA_TYPE\"])\n    .to_pandas_dataframe()\n)\n</code></pre>"},{"location":"using/querying/#sorted-subset-with-boolean-combinations","title":"Sorted subset with boolean combinations","text":"<pre><code>from beacon_api.query import OrFilter, AndFilter, RangeFilter\n\nsorted_subset = (\n    collection\n    .query()\n    .add_select_column(\"LONGITUDE\")\n    .add_select_column(\"LATITUDE\")\n    .add_select_column(\"JULD\")\n    .add_select_column(\"TEMP\")\n    .add_select_column(\"PSAL\")\n    .add_filter(\n        OrFilter([\n            AndFilter([\n                RangeFilter(\"JULD\", gt_eq=\"2024-01-01T00:00:00\", lt_eq=\"2024-02-01T00:00:00\"),\n                RangeFilter(\"PRES\", lt_eq=10),\n            ]),\n            AndFilter([\n                RangeFilter(\"JULD\", gt_eq=\"2024-05-01T00:00:00\", lt_eq=\"2024-06-01T00:00:00\"),\n                RangeFilter(\"PRES\", lt_eq=5),\n            ]),\n        ])\n    )\n    .add_is_not_null_filter(\"TEMP\")\n    .add_sort(\"JULD\", ascending=True)\n    .add_sort(\"PRES\", ascending=True)\n    .to_pandas_dataframe()\n)\n</code></pre>"},{"location":"using/querying/#combining-coalesced-columns-with-polygon-filters","title":"Combining coalesced columns with polygon filters","text":"<pre><code>from beacon_api.query import Functions\n\nregions = (\n    collection\n    .query()\n    .add_select_column(\"LONGITUDE\")\n    .add_select_column(\"LATITUDE\")\n    .add_select_column(\"JULD\")\n    .add_select(Functions.coalesce([\"SEA_NAME\", \"BASIN\"], alias=\"region\"))\n    .add_polygon_filter(\n        longitude_column=\"LONGITUDE\",\n        latitude_column=\"LATITUDE\",\n        polygon=[(-5.5, 51.5), (-4.0, 51.5), (-4.0, 52.5), (-5.5, 52.5), (-5.5, 51.5)],\n    )\n    .set_distinct([\"region\"])\n    .to_pandas_dataframe()\n)\n</code></pre> <p>Need SQL instead? Construct an <code>SQLQuery</code> via <code>client.sql_query(\"SELECT ...\")</code> and call the exact same output helpers\u2014<code>to_pandas_dataframe()</code>, <code>to_parquet()</code> and friends live on the shared <code>BaseQuery</code> class.</p> <p>With these building blocks you can express everything from quick lookups to production-ready pipelines without leaving Python.</p>"},{"location":"using/tables/","title":"Working with tables","text":"<p>Beacon tables (also called data collections) are exposed as instances of <code>beacon_api.table.DataTable</code>. They wrap server metadata so you can quickly describe the content of a table before running costly queries.</p>"},{"location":"using/tables/#fetch-a-table","title":"Fetch a table","text":"<pre><code>tables = client.list_tables()\nstations = tables[\"stations-collection\"]\n\nprint(stations.table_name)\n</code></pre> <p><code>list_tables()</code> returns a dictionary, so simply index into it using the table name you are interested in.</p>"},{"location":"using/tables/#metadata-helpers","title":"Metadata helpers","text":"<p>Every <code>DataTable</code> fetches <code>/api/table-config?table_name=...</code> during initialization. Access that metadata via:</p> <pre><code>stations.get_table_description()\nstations.get_table_type()\n</code></pre> <p>Use <code>get_table_schema_arrow()</code> or <code>get_table_schema()</code> to inspect the schema before building a query:</p> <pre><code>schema = stations.get_table_schema_arrow()\nprint(schema)\n\n# convert to a dict of {column_name: python_type}\nschema_dict = stations.get_table_schema()\n</code></pre> <p>The schema result is a PyArrow <code>Schema</code>, meaning you can introspect field metadata, dtypes, or reuse it when constructing downstream DataFrames.</p>"},{"location":"using/tables/#create-a-query-from-a-table","title":"Create a query from a table","text":"<p>Once you know which columns you need, call <code>stations.query()</code> to obtain a <code>JSONQuery</code> builder:</p> <pre><code>query = (\n    stations\n    .query()\n    .add_select_column(\"LONGITUDE\")\n    .add_select_column(\"LATITUDE\")\n    .add_select_column(\"JULD\")\n    .add_range_filter(\"JULD\", \"2024-01-01T00:00:00\", \"2024-03-01T00:00:00\")\n)\n\ndf = query.to_pandas_dataframe()\n</code></pre> <p>Because <code>JSONQuery</code> is fluent, you can keep chaining selects, filters, sorting, or distinct clauses before materializing the results. Refer to Querying the Beacon Data Lake for every available builder method.</p>"},{"location":"using/tables/#subset-convenience-helper","title":"Subset convenience helper","text":"<p>For spatial/temporal slices there is <code>DataTable.subset()</code>. It adds the longitude, latitude, depth, and time selections plus the appropriate filters for you:</p> <pre><code>from datetime import datetime\n\nsubset = stations.subset(\n    longitude_column=\"LONGITUDE\",\n    latitude_column=\"LATITUDE\",\n    time_column=\"JULD\",\n    depth_column=\"PRES\",\n    columns=[\"TEMP\", \"PSAL\", \".featureType\"],\n    bbox=(-20, 40, -10, 55),\n    depth_range=(0, 50),\n    time_range=(datetime(2024, 1, 1), datetime(2024, 6, 1)),\n)\n\nsubset_df = subset.to_pandas_dataframe()\n</code></pre> <p>Because <code>subset()</code> simply returns a <code>JSONQuery</code>, you can continue chaining methods\u2014for example, add equals filters or change the output format with <code>subset.to_geoparquet(...)</code>.</p>"}]}